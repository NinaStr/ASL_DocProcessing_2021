{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef154ef6",
   "metadata": {},
   "source": [
    "# Object Detection - Online Prediction - Kubeflow Pipeline Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9fa5af95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ai]\n",
      "region = us-central1\n",
      "[compute]\n",
      "region = us-central1\n",
      "[core]\n",
      "account = 136021895401-compute@developer.gserviceaccount.com\n",
      "disable_usage_reporting = True\n",
      "project = qwiklabs-gcp-00-373ac55d0e0a\n",
      "\n",
      "Your active configuration is: [default]\n"
     ]
    }
   ],
   "source": [
    "!gcloud config list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0cdab8",
   "metadata": {},
   "source": [
    "**It is necessary to give some permissions to the service account first:**\n",
    "\n",
    "*Failed to create pipeline job. Error: Service account `136021895401-compute@developer.gserviceaccount.com` does not have `[storage.objects.get, storage.objects.create]` IAM permission(s) to the bucket \"qwiklabs-gcp-00-373ac55d0e0a\". Please either copy the files to the Google Cloud Storage bucket owned by your project, or grant the required IAM permission(s) to the service account..*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dcff9775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kubeflow pipelines version: 1.7.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "#!pip install --upgrade kfp\n",
    "import kfp\n",
    "from google.cloud import aiplatform\n",
    "from kfp.v2 import compiler\n",
    "from kfp.v2.dsl import component\n",
    "from kfp.v2.google import experimental\n",
    "from kfp.v2.google.client import AIPlatformClient\n",
    "\n",
    "print('Kubeflow pipelines version: {}'.format(kfp.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29cff534",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT='qwiklabs-gcp-00-373ac55d0e0a'\n",
    "REGION = 'us-central1'\n",
    "BUCKET='qwiklabs-gcp-00-373ac55d0e0a'\n",
    "BQ_DATASET='demo_dataset'\n",
    "OBJDET_TABLE='objdet'\n",
    "PIPELINE_ROOT = f\"gs://{BUCKET}/labeled_patents/pipeline_root\"\n",
    "\n",
    "\n",
    "MODEL_DISPLAY_NAME=f\"labpat_model\"\n",
    "MACHINE_TYPE=\"n1-standard-16\"\n",
    "REPLICA_COUNT=1\n",
    "DOCKER_IMAGE_URI_CREATE_BQDATASET=\"us-docker.pkg.dev/vertex-ai/training/tf-cpu.2-3:latest\"\n",
    "# Pre-built containers:\n",
    "# https://cloud.google.com/vertex-ai/docs/training/pre-built-containers\n",
    "\n",
    "\n",
    "# Output directory and job_name\n",
    "#OUTDIR=f\"gs://{BUCKET}/taxifare/trained_model_{TIMESTAMP}\"\n",
    "#MODEL_DISPLAY_NAME=f\"taxifare_{TIMESTAMP}\"\n",
    "\n",
    "#PYTHON_PACKAGE_URIS=f\"gs://{BUCKET}/taxifare/taxifare_trainer-0.1.tar.gz\"\n",
    "#SERVING_CONTAINER_IMAGE_URI=\"us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-3:latest\"\n",
    "#PYTHON_MODULE=\"trainer.task\"\n",
    "\n",
    "# Model and training hyperparameters\n",
    "#BATCH_SIZE=500\n",
    "#NUM_EXAMPLES_TO_TRAIN_ON=10000\n",
    "#NUM_EVALS=1000\n",
    "#NBUCKETS=10\n",
    "#LR=0.001\n",
    "#NNSIZE=\"32 8\"\n",
    "\n",
    "# GCS paths\n",
    "#GCS_PROJECT_PATH=f\"gs://{BUCKET}/taxifare\"\n",
    "#DATA_PATH=f\"{GCS_PROJECT_PATH}/data\"\n",
    "#TRAIN_DATA_PATH=f\"{DATA_PATH}/taxi-train*\"\n",
    "#EVAL_DATA_PATH=f\"{DATA_PATH}/taxi-valid*\"\n",
    "\n",
    "os.environ[\"PROJECT\"] = PROJECT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929ddd7e",
   "metadata": {},
   "source": [
    "### Component 1: Create the BQ dataset and OBJ results table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2ea7c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(packages_to_install=['google-cloud-bigquery'])\n",
    "def create_bigquery_demo_dataset(project: str,\n",
    "                                 dataset_name: str,\n",
    "                                 table_name: str):\n",
    "  \n",
    "    from google.cloud import bigquery   \n",
    "\n",
    "\n",
    "    client = bigquery.Client()\n",
    "\n",
    "    # Create dataset\n",
    "    dataset_id = f'{project}.{dataset_name}'\n",
    "    dataset = bigquery.Dataset(dataset_id)\n",
    "    dataset.location = \"US\"\n",
    "    dataset = client.create_dataset(dataset, timeout=30)  # Make an API request.\n",
    "    #print(\"Created dataset {}.{}\".format(client.project, dataset.dataset_id))\n",
    "\n",
    "    # Create table\n",
    "    OBJDET_SCHEMA = [\n",
    "        bigquery.SchemaField('file_name',   'STRING', mode='NULLABLE'),\n",
    "        bigquery.SchemaField('objdet_pred', 'STRING', mode='NULLABLE'),\n",
    "        bigquery.SchemaField('objdet_confidence', 'STRING', mode='NULLABLE'),\n",
    "        bigquery.SchemaField('objdet_xmin', 'FLOAT', mode='NULLABLE'),\n",
    "        bigquery.SchemaField('objdet_xmax', 'FLOAT', mode='NULLABLE'),\n",
    "        bigquery.SchemaField('objdet_ymin', 'FLOAT', mode='NULLABLE'),\n",
    "        bigquery.SchemaField('objdet_ymax', 'FLOAT', mode='NULLABLE')]\n",
    "\n",
    "    table_id = f'{project}.{dataset_name}.{table_name}'\n",
    "\n",
    "    schema = OBJDET_SCHEMA\n",
    "\n",
    "    table = bigquery.Table(table_id, schema=schema)\n",
    "    table = client.create_table(table)  # Make an API request.\n",
    "    #print(\"Created table {}.{}.{}\".format(table.project, table.dataset_id, table.table_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb30aeaa",
   "metadata": {},
   "source": [
    "### Component 2: Performing object detection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af4eeea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(packages_to_install=['google-cloud-bigquery', 'google-cloud-storage',  'google-cloud-aiplatform'])\n",
    "def perform_object_detection(project: str,\n",
    "                             region: str,\n",
    "                             bucket: str,\n",
    "                             img_blob: str,\n",
    "                             objdet_endpoint: str,\n",
    "                             dataset_name: str,\n",
    "                             table_name: str):\n",
    "    \n",
    "    # IMPORTS     \n",
    "    import os\n",
    "    import tempfile\n",
    "    import logging\n",
    "    \n",
    "    import traceback as tb\n",
    "\n",
    "    from google.cloud import bigquery\n",
    "    from google.cloud import storage\n",
    "    from fnmatch import fnmatch\n",
    "\n",
    "    import base64\n",
    "\n",
    "    from google.cloud import aiplatform\n",
    "    from google.cloud.aiplatform.gapic.schema import predict\n",
    "    \n",
    "    \n",
    "    #_____________________________________ AUXILIARY FUNCTIONS ______________________________\n",
    "    def get_bucket_file_list(bucket_name, fname_template='*'):\n",
    "        '''!@brief Function that returns the list of files in a bucket.\n",
    "        @param bucket (string) Bucket name.\n",
    "        @param fname_template (string) Template for filtering blob names \n",
    "        that supports Unix shell-style wildcards. For more info: \n",
    "        https://docs.python.org/3/library/fnmatch.html\n",
    "\n",
    "        @return (list of srtings) List of blob names in a bucket which \n",
    "        fullfills template structure.\n",
    "        '''\n",
    "        # Instantiating client:\n",
    "        storage_client = storage.Client()\n",
    "\n",
    "        # Note: Client.list_blobs requires at least package version 1.17.0.\n",
    "        blobs = storage_client.list_blobs(bucket_name)\n",
    "\n",
    "        # Listing all the blobs in a bucket:\n",
    "        blob_lst = [blob.name for blob in blobs]\n",
    "\n",
    "        # Filtering blob names with the template format given:  \n",
    "        file_lst = [fname for fname in blob_lst if fnmatch(fname, fname_template)]\n",
    "\n",
    "        return file_lst\n",
    "    \n",
    "    \n",
    "    def predict_image_classification_sample(\n",
    "        project: str,\n",
    "        endpoint_id: str,\n",
    "        filename: str,\n",
    "        location: str = \"us-central1\",\n",
    "        api_endpoint: str = \"us-central1-aiplatform.googleapis.com\"):\n",
    "        \n",
    "        # The AI Platform services require regional API endpoints.\n",
    "        client_options = {\"api_endpoint\": api_endpoint}\n",
    "        # Initialize client that will be used to create and send requests.\n",
    "        # This client only needs to be created once, and can be reused for multiple requests.\n",
    "        client = aiplatform.gapic.PredictionServiceClient(client_options=client_options)\n",
    "        with open(filename, \"rb\") as f:\n",
    "            file_content = f.read()\n",
    "\n",
    "        # The format of each instance should conform to the deployed model's prediction input schema.\n",
    "        encoded_content = base64.b64encode(file_content).decode(\"utf-8\")\n",
    "        instance = predict.instance.ImageObjectDetectionPredictionInstance(\n",
    "            content=encoded_content,\n",
    "        ).to_value()\n",
    "        instances = [instance]\n",
    "        parameters = predict.params.ImageObjectDetectionPredictionParams(\n",
    "            confidence_threshold=0.5, max_predictions=5,\n",
    "        ).to_value()\n",
    "        endpoint = client.endpoint_path(\n",
    "            project=project, location=location, endpoint=endpoint_id\n",
    "        )\n",
    "        response = client.predict(\n",
    "            endpoint=endpoint, instances=instances, parameters=parameters\n",
    "        )\n",
    "        predictions = response.predictions\n",
    "        return [dict(prediction) for prediction in predictions]\n",
    "    #___________________________________________ MAIN _______________________________________\n",
    "\n",
    "    # Instantiating BQ client:\n",
    "    client = bigquery.Client()\n",
    "    \n",
    "    files = get_bucket_file_list(bucket_name=f'{bucket}',\n",
    "                                 fname_template=img_blob)\n",
    "    \n",
    "    for file in files:             \n",
    "        # Downloading the file as a temporal file:\n",
    "        storage_client = storage.Client()\n",
    "        bucket = storage_client.bucket(project)\n",
    "        blob = bucket.blob(file)\n",
    "        _, path = tempfile.mkstemp()\n",
    "        blob.download_to_filename(path + '.png')    \n",
    "\n",
    "        # Obtaining online prediction:\n",
    "        preds = predict_image_classification_sample(\n",
    "            project=project,\n",
    "            endpoint_id=objdet_endpoint,\n",
    "            filename=f'{path}.png',\n",
    "            location=region,\n",
    "            api_endpoint='us-central1-aiplatform.googleapis.com')\n",
    "\n",
    "        # Parsing prediction:\n",
    "        objdet_pred = preds[0]['displayNames'][0]\n",
    "        objdet_confidence = preds[0]['confidences'][0]\n",
    "        objdet_xmin, objdet_xmax = preds[0]['bboxes'][0][0], preds[0]['bboxes'][0][1]\n",
    "        objdet_ymin, objdet_ymax = preds[0]['bboxes'][0][2], preds[0]['bboxes'][0][3]\n",
    "        \n",
    "        # Storing prediction into the BQ table:\n",
    "        rows_to_insert = [\n",
    "            {'file_name': f'{file}'.split('/')[-1],\n",
    "             'objdet_pred': f'{objdet_pred}',\n",
    "             'objdet_confidence': f'{objdet_confidence}',\n",
    "             'objdet_xmin': f'{objdet_xmin}',\n",
    "             'objdet_xmax': f'{objdet_xmax}',\n",
    "             'objdet_ymin': f'{objdet_ymin}',\n",
    "             'objdet_ymax': f'{objdet_ymax}'}\n",
    "        ]\n",
    "\n",
    "        table_id = f'{project}.{dataset_name}.{table_name}'\n",
    "\n",
    "        errors = client.insert_rows_json(table_id, rows_to_insert)  # Make an API request.\n",
    "        if errors == []:\n",
    "            print(\"New rows have been added.\")\n",
    "        else:\n",
    "            print(\"Encountered errors while inserting rows: {}\".format(errors))\n",
    "        os.remove(f'{path}.png')\n",
    "        os.remove(path)            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e93cbf4",
   "metadata": {},
   "source": [
    "### Creating the Pipeline: Component 1 + Component 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85455bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "@kfp.dsl.pipeline(name=\"labpat-pipeline-test2\", \n",
    "                  description='An example pipeline that performs addition calculations.',\n",
    "                  pipeline_root=PIPELINE_ROOT)\n",
    "\n",
    "def pipeline():\n",
    "\n",
    "    create_bqdataset_task = create_bigquery_demo_dataset(project='qwiklabs-gcp-00-373ac55d0e0a',\n",
    "                                                         dataset_name='demo_dataset_pipeline',\n",
    "                                                         table_name='objdet_results')\n",
    "    \n",
    "    \n",
    "    object_detection_task = perform_object_detection(project='qwiklabs-gcp-00-373ac55d0e0a',\n",
    "                                                     region='us-central1',\n",
    "                                                     bucket='qwiklabs-gcp-00-373ac55d0e0a',\n",
    "                                                     img_blob='labeled_patents/subsample_images/*',\n",
    "                                                     objdet_endpoint='2074030773706424320',\n",
    "                                                     dataset_name='demo_dataset_pipeline',\n",
    "                                                     table_name='objdet_results')\n",
    "\n",
    "    \n",
    "    object_detection_task.after(create_bqdataset_task)\n",
    "    \n",
    "    \n",
    "#     experimental.run_as_aiplatform_custom_job(\n",
    "#         create_bqdataset_task,\n",
    "#         display_name=f\"labpat_pipeline-create_bq_dataset\",\n",
    "#         worker_pool_specs=[\n",
    "#             {   \"containerSpec\": {\n",
    "#                     \"imageUri\": f\"{DOCKER_IMAGE_URI_CREATE_BQDATASET}\",\n",
    "#                 },\n",
    "#                 \"replica_count\": f\"{REPLICA_COUNT}\",\n",
    "#                 \"machineSpec\": {\n",
    "#                     \"machineType\": f\"{MACHINE_TYPE}\",\n",
    "#                 },                \n",
    "#             }\n",
    "#         ],\n",
    "#     )\n",
    "    \n",
    "# To define a custom job, worker_pool_spec is mandator. To define a worker_pool_spec \n",
    "# is also needed either a containerSpec or a pythonPackageSpec.\n",
    "# https://cloud.google.com/vertex-ai/docs/reference/rest/v1/CustomJobSpec    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9a2457",
   "metadata": {},
   "source": [
    "### Compiling the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd5d6d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.v2 import compiler\n",
    "\n",
    "if not os.path.isdir(\"vertex_pipelines\"):\n",
    "    os.mkdir(\"vertex_pipelines\")\n",
    "\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=pipeline,\n",
    "    package_path=\"./vertex_pipelines/labeled_patents_pipeline2.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b97ded",
   "metadata": {},
   "source": [
    "### Launching the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b00bf99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/kfp/v2/google/client/client.py:175: FutureWarning: AIPlatformClient will be deprecated in v1.9. Please use PipelineJob https://googleapis.dev/python/aiplatform/latest/_modules/google/cloud/aiplatform/pipeline_jobs.html in Vertex SDK. Install the SDK using \"pip install google-cloud-aiplatform\"\n",
      "  category=FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "# Instantiating an API client object:\n",
    "# TODO: use the new Vertex AI.\n",
    "api_client = AIPlatformClient(\n",
    "    project_id=PROJECT,\n",
    "    region=REGION,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c6674ad8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "See the Pipeline job <a href=\"https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/labpat-pipeline-test2-20210818182018?project=qwiklabs-gcp-00-373ac55d0e0a\" target=\"_blank\" >here</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = api_client.create_run_from_job_spec(\n",
    "    './vertex_pipelines/labeled_patents_pipeline2.json',\n",
    "    pipeline_root=f\"{PIPELINE_ROOT}\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-5.m76",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-5:m76"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
