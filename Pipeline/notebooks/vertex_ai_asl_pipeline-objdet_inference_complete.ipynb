{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6955ae0",
   "metadata": {},
   "source": [
    "# Labeled Patents - Vertex AI Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d2bc71",
   "metadata": {},
   "source": [
    "## Importing Auxiliary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c95f264e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kubeflow pipelines version: 1.7.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "from typing import NamedTuple\n",
    "\n",
    "#!pip install --upgrade kfp\n",
    "import kfp\n",
    "from google.cloud import aiplatform\n",
    "from kfp.v2 import compiler\n",
    "from kfp.v2.dsl import component, Artifact, Input\n",
    "from kfp.v2.google import experimental\n",
    "from kfp.v2.google.client import AIPlatformClient\n",
    "\n",
    "print('Kubeflow pipelines version: {}'.format(kfp.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf1ca83",
   "metadata": {},
   "source": [
    "## Setting Notebook Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9b331c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "UUID = datetime.now().strftime('%y%m%d_%H%M%S') #str\n",
    "PROJECT = 'qwiklabs-gcp-00-373ac55d0e0a'\n",
    "REGION = 'us-central1'\n",
    "\n",
    "BUCKET = 'doc_processing_patents'\n",
    "PDF_BUCKET_PATH = 'pdf'\n",
    "PNG_BUCKET_PATH = 'png'\n",
    "TXT_BUCKET_PATH = 'txt'\n",
    "\n",
    "RES_DATASET_NAME = 'docprocessing_' + UUID\n",
    "RES_DATASET_ID = f'{PROJECT}.{RES_DATASET_NAME}'\n",
    "\n",
    "TCN_MODEL_NAME = '2393478483993952256'\n",
    "TCN_RESTABLE_NAME = f'{RES_DATASET_ID}.tcn'\n",
    "TCN_RESTABLE_SCHEMA = \"\"\"\n",
    "[\n",
    " {\"name\": \"file\", \"field_type\": \"STRING\", \"mode\": \"REQUIRED\", \"description\": \"File path.\"},\n",
    " {\"name\": \"subject\", \"field_type\": \"STRING\", \"mode\": \"REQUIRED\", \"description\": \"Predicted class.\"},\n",
    " {\"name\": \"score\", \"field_type\": \"STRING\", \"mode\": \"REQUIRED\", \"description\": \"Confidence of the prediction.\"}\n",
    "]\n",
    "\"\"\"\n",
    "\n",
    "ICN_MODEL_NAME = '8925034949820547072'\n",
    "ICN_ENDPT_NAME = ''\n",
    "ICN_RESTABLE_NAME = f'{RES_DATASET_ID}.icn'\n",
    "ICN_RESTABLE_SCHEMA = \"\"\"\n",
    "[\n",
    " {\"name\":  \"file\", \"field_type\": \"STRING\", \"mode\": \"REQUIRED\", \"description\": \"File path.\"},\n",
    " {\"name\": \"label\", \"field_type\": \"STRING\", \"mode\": \"REQUIRED\", \"description\": \"Predicted class.\"},\n",
    " {\"name\": \"score\", \"field_type\": \"STRING\", \"mode\": \"REQUIRED\", \"description\": \"Confidence of the prediction.\"}\n",
    "]\n",
    "\"\"\"\n",
    "\n",
    "ODM_MODEL_NAME = '3409814256151953408'\n",
    "ODM_ENDPT_NAME = '2074030773706424320'\n",
    "ODM_RESTABLE_NAME = f'{RES_DATASET_ID}.odm'\n",
    "ODM_RESTABLE_SCHEMA = \"\"\"\n",
    "[\n",
    " {\"name\": \"file\",  \"field_type\": \"STRING\", \"mode\": \"REQUIRED\", \"description\": \"File path.\"},\n",
    " {\"name\": \"label\", \"field_type\": \"STRING\", \"mode\": \"REQUIRED\", \"description\": \"Predicted class.\"},\n",
    " {\"name\": \"score\", \"field_type\":  \"FLOAT\", \"mode\": \"REQUIRED\", \"description\": \"Confidence of the prediction.\"},\n",
    " {\"name\": \"xmin\",  \"field_type\":  \"FLOAT\", \"mode\": \"REQUIRED\", \"description\": \"X coordinate of the top left corner.\"},\n",
    " {\"name\": \"xmax\",  \"field_type\":  \"FLOAT\", \"mode\": \"REQUIRED\", \"description\": \"Y coordinate of the top left corner.\"},\n",
    " {\"name\": \"ymin\",  \"field_type\":  \"FLOAT\", \"mode\": \"REQUIRED\", \"description\": \"X coordinate of the bottom right corner.\"},\n",
    " {\"name\": \"ymax\",  \"field_type\":  \"FLOAT\", \"mode\": \"REQUIRED\", \"description\": \"Y coordinate of the bottom right corner.\"}\n",
    "]\n",
    "\"\"\"\n",
    "\n",
    "#src_path = \"gs://2021_08_16_tcn_dev\"\n",
    "#dst_path = \"gs://2021_08_16_tcn_dev\"\n",
    "\n",
    "PIPELINE_NAME = 'processing-of-patents'\n",
    "PIPELINE_ROOT = f\"gs://{BUCKET}/pipeline_root\"\n",
    "LOCAL_PIPELINE_PATH = './vertex_pipelines'\n",
    "LOCAL_PIPELINE_JSON = os.path.join(LOCAL_PIPELINE_PATH, 'doc_processing_pipeline.json')\n",
    "\n",
    "#RESULTS_BQ_DATASET='demo_dataset'\n",
    "#RESULTS_OBJDET_TABLE='objdet'\n",
    "\n",
    "#MODEL_DISPLAY_NAME=f\"labpat_model\"\n",
    "#MACHINE_TYPE=\"n1-standard-16\"\n",
    "#REPLICA_COUNT=1\n",
    "#DOCKER_IMAGE_URI_CREATE_BQDATASET=\"us-docker.pkg.dev/vertex-ai/training/tf-cpu.2-3:latest\"\n",
    "\n",
    "os.environ['PROJECT'] = PROJECT\n",
    "os.environ['REGION'] = REGION\n",
    "os.environ['BUCKET'] = BUCKET \n",
    "os.environ['PDF_BUCKET_PATH'] = PDF_BUCKET_PATH\n",
    "os.environ['PNG_BUCKET_PATH'] = PNG_BUCKET_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5097336",
   "metadata": {},
   "source": [
    "**Copying some demo files into the Bucket**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385fc730",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil -m cp gs://2021_08_16_tcn_dev/*.pdf gs://$BUCKET/$PDF_BUCKET_PATH\n",
    "!gsutil -m cp gs://$PROJECT/labeled_patents/subsample_images/* gs://$BUCKET/$PNG_BUCKET_PATH\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8ceee6",
   "metadata": {},
   "source": [
    "## Defining Vertex AI Components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9ac1fb",
   "metadata": {},
   "source": [
    "### Component 1: Performing OCR on PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d54b0403",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(packages_to_install=['google-cloud-storage',  'google-cloud-vision'])\n",
    "def perform_ocr_on_pdfs(src_path: str, \n",
    "                        dst_path: str,\n",
    "                        uuid: str,\n",
    "                        project: str):\n",
    "    \n",
    "    # IMPORTS:\n",
    "    import os\n",
    "    import logging\n",
    "    import traceback as tb\n",
    "    import time\n",
    "    from pathlib import Path\n",
    "    from google.cloud import storage\n",
    "    from google.cloud import vision\n",
    "    # from google.cloud import aiplatform\n",
    "\n",
    "    \n",
    "    # AUXILIARY FUNCTIONS:\n",
    "    def to_trace_str(e):\n",
    "        return ''.join(tb.format_exception(None, e, e.__traceback__))   \n",
    "    \n",
    "    \n",
    "    def dismantle_path(gcs_path):\n",
    "        parts = Path(gcs_path).parts\n",
    "        bucket_idx = 1 if parts[0].startswith(\"gs\") else 0\n",
    "        filename_idx = -1 if \".\" in parts[-1] else None\n",
    "\n",
    "        bucket_name = parts[bucket_idx]\n",
    "        filename = parts[filename_idx] if filename_idx else \"\"\n",
    "        directory = \"/\".join(parts[bucket_idx:filename_idx] if filename_idx else parts[bucket_idx+1:])\n",
    "        return bucket_name, directory, filename\n",
    "    \n",
    "    \n",
    "    def ocr(src_path, dst_path, project):\n",
    "        \"\"\"Perform optical character recognition in pdf files.\n",
    "        \n",
    "        Args\n",
    "            src_path\n",
    "            dst_path\n",
    "        \n",
    "        Returns\n",
    "            google.api_core.operation.Operation\n",
    "            To check if done use method .done()\n",
    "            \n",
    "        Link to documentation:  \n",
    "            https://googleapis.dev/python/vision/latest/vision_v1/types.html#google.cloud.vision_v1.types.OutputConfig\n",
    "            https://cloud.google.com/vision/docs/pdf\n",
    "        \n",
    "        \"\"\"\n",
    "        try:\n",
    "            logging.info(\"started optical character recognition\")\n",
    "            \n",
    "            src_bucket_name, src_directory, _ = dismantle_path(src_path)\n",
    "            dst_bucket_name, dst_directory, _ = dismantle_path(dst_path)\n",
    "            \n",
    "            storage_client = storage.Client(project=project)\n",
    "            src_bucket = storage_client.bucket(src_bucket_name)\n",
    "            dst_bucket = storage_client.bucket(dst_bucket_name)\n",
    "            \n",
    "            logging.info(f\"src_bucket_name {src_bucket_name}, src_directory {src_directory}\")\n",
    "\n",
    "            blob_list = [blob for blob in list(src_bucket.list_blobs()) if \\\n",
    "                         os.path.basename(src_directory) in blob.name and \\\n",
    "                         blob.name.endswith(\".pdf\")]\n",
    "            \n",
    "            logging.info(f\"found {len(blob_list)} pdf files in bucket {src_bucket_name}\")\n",
    "\n",
    "            client = vision.ImageAnnotatorClient()\n",
    "            feature = vision.Feature(type_=vision.Feature.Type.DOCUMENT_TEXT_DETECTION)\n",
    "            \n",
    "            operations = []\n",
    "            async_requests = []\n",
    "            \n",
    "            for b_idx, blob in enumerate(blob_list):\n",
    "                gcs_source_uri = os.path.join(src_path, blob.name)\n",
    "                gcs_destination_uri = os.path.join(dst_path, blob.name)\n",
    "\n",
    "                # source\n",
    "                gcs_source = vision.GcsSource(uri=gcs_source_uri)\n",
    "                input_config = vision.InputConfig(gcs_source=gcs_source, mime_type='application/pdf')\n",
    "\n",
    "                # destination\n",
    "                gcs_destination = vision.GcsDestination(uri=gcs_destination_uri)\n",
    "                output_config = vision.OutputConfig(gcs_destination=gcs_destination, batch_size=1)\n",
    "\n",
    "                logging.info(f\"started ocr for {b_idx} of {len(blob_list)} files\")\n",
    "                async_request = vision.AsyncAnnotateFileRequest(\n",
    "                    features=[feature], \n",
    "                    input_config=input_config,\n",
    "                    output_config=output_config\n",
    "                )\n",
    "                async_requests.append(async_request)\n",
    "\n",
    "            operation = client.async_batch_annotate_files(requests=async_requests)\n",
    "            return operation\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in method ocr: {to_trace_str(e)}\")\n",
    "            \n",
    "            \n",
    "    def create_text_files(gcs_path, project):\n",
    "        try:\n",
    "            # init bucket\n",
    "            bucket_name, directory, _ = dismantle_path(gcs_path)\n",
    "            storage_client = storage.Client(project=project)\n",
    "            bucket = storage_client.bucket(bucket_name)\n",
    "            blob_list = [blob for blob in list(bucket.list_blobs()) if \\\n",
    "                             os.path.basename(directory) in blob.name and \\\n",
    "                             blob.name.endswith(\"output-1-to-1.json\")]\n",
    "            \n",
    "            for b_idx, blob in enumerate(blob_list):\n",
    "                logging.info(f\"creating {b_idx+1} of {len(blob_list)} text files\")\n",
    "                json_string = blob.download_as_string()\n",
    "                response = json.loads(json_string)\n",
    "                text = response['responses'][0]['fullTextAnnotation']['text'] \n",
    "                txt_path = blob.name.replace(\"output-1-to-1.json\", \".txt\")\n",
    "                text_blob = bucket.blob(txt_path)\n",
    "                text_blob.upload_from_string(text)\n",
    "                \n",
    "            logging.info(\"finished creating text files\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in method create_text_files: {to_trace_str(e)}\") \n",
    "            \n",
    "    def get_extension(mime_type):\n",
    "        if mime_type == \"text/plain\":\n",
    "            return \".txt\"\n",
    "        elif mime_type == \"image/png\":\n",
    "            return \".png\"\n",
    "        else:\n",
    "            return \".txt\"\n",
    "    \n",
    "    def create_jsonl(gcs_path, mime_type, filename,project):\n",
    "        \"\"\"create jsonl out of files in bucket\n",
    "        \n",
    "        Args\n",
    "            gcs_path (str): bucket or dir where files are located\n",
    "            mime_type (str): the files mimetype \n",
    "            filename (str): the jsonl filename\n",
    "        \n",
    "        Returns\n",
    "            full path of jsonl\n",
    "        \"\"\"\n",
    "        try:\n",
    "            bucket_name, directory, _ = dismantle_path(gcs_path)\n",
    "            storage_client = storage.Client(project=project)\n",
    "            bucket = storage_client.bucket(bucket_name)\n",
    "            extension = get_extension(mime_type)\n",
    "\n",
    "            blob_list = [blob for blob in list(bucket.list_blobs()) if \\\n",
    "                             os.path.basename(directory) in blob.name and \\\n",
    "                             blob.name.endswith(extension)]\n",
    "\n",
    "            jsonl_content = \"\"\n",
    "\n",
    "            for b_idx, blob in enumerate(blob_list):\n",
    "                full_path = os.path.join(gcs_path,blob.name)\n",
    "\n",
    "                d = json.dumps(\n",
    "                    {\n",
    "                    \"content\": full_path,\n",
    "                    \"mimeType\": mime_type\n",
    "                    }\n",
    "                )+\"\\n\"\n",
    "\n",
    "                jsonl_content = jsonl_content+d\n",
    "\n",
    "            bucket.blob(filename).upload_from_string(jsonl_content)\n",
    "            logging.info(f\"uploaded jsonl {filename} to bucket {bucket_name}\")\n",
    "\n",
    "            return os.path.join(gcs_path,filename)\n",
    "        \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in jsonl creation: {to_trace_str(e)}\")\n",
    "    \n",
    "    \n",
    "    def preprocess_ocr(src_path, dst_path, jsonl_filename, project):\n",
    "        ocr_operation = ocr(src_path, dst_path, project)\n",
    "        \n",
    "        while not ocr_operation.done():\n",
    "            logging.info(\"wait for ocr to finish\")\n",
    "            time.sleep(5)\n",
    "            \n",
    "        create_text_files(dst_path, project)\n",
    "        return create_jsonl(gcs_path=dst_path, mime_type=\"text/plain\", filename=jsonl_filename, project=project)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # PIPELINE COMPONENT MAIN CODE:\n",
    "    pass\n",
    "#    logging.basicConfig(level=logging.INFO)\n",
    "#    logger = logging.getLogger(__name__)\n",
    "#     logging.info(f\"Starting the processing of pdfs with the OCR functionality of Google Vision API.\")\n",
    "#     \n",
    "#         \n",
    "#     # save everything in the same bucket\n",
    "#     jsonl_filename_tcn = f\"tcn_{uuid}.jsonl\"\n",
    "#         \n",
    "#     # create ocr\n",
    "#     jsonl_path_tcn = preprocess_ocr(src_path, dst_path, jsonl_filename_tcn, project)\n",
    "#     \n",
    "#     # return path where jsonl with .txt files is saved\n",
    "#     return jsonl_path_tcn "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673accd5",
   "metadata": {},
   "source": [
    "### Component 2: PDF to PNG conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7168ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component()\n",
    "def transform_pdfs_into_png():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003c89d1",
   "metadata": {},
   "source": [
    "### Component 3: Creating a BigQuery dataset to save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b8a7be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(packages_to_install=['google-cloud-bigquery'])\n",
    "def create_bq_results_dataset(project: str, \n",
    "                              dataset_id: str):\n",
    "    \"\"\"loads csv data in storage to BQ\"\"\"\n",
    "    # Send the dataset to the API for creation, with an explicit timeout.\n",
    "    # Raises google.api_core.exceptions.Conflict if the Dataset already\n",
    "    # exists within the project.\n",
    "    import logging\n",
    "    from google.cloud import bigquery\n",
    "\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logging.info(f\"Starting the creation of a BigQuery dataset to store analyses results.\")\n",
    "\n",
    "    bq = bigquery.Client(project=project)\n",
    "    try:\n",
    "        dataset = bigquery.Dataset(dataset_id)\n",
    "        dataset.location = \"US\"\n",
    "        bq.get_dataset(dataset_id)  # Make an API request.\n",
    "        logging.info(\"Dataset {} already exists\".format(dataset_id))\n",
    "    except Exception as e:\n",
    "        logging.info(\"Dataset {} is not found\".format(dataset_id))\n",
    "        dataset = bq.create_dataset(dataset, timeout=30)  # Make an API request.\n",
    "        dataset.location = \"US\"\n",
    "        logging.info(\"Created dataset {}.{}\".format(bq.project, dataset.dataset_id))\n",
    "    finally:\n",
    "        logging.info(f\"Finished creating or loading dataset {dataset_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59240e81",
   "metadata": {},
   "source": [
    "### Component 4.1: Creating text classification results table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "adf34072",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(packages_to_install=['google-cloud-bigquery'])\n",
    "def create_text_class_results_table(project:str, \n",
    "                                    dataset_id:str, \n",
    "                                    table_id:str, \n",
    "                                    schema:str):\n",
    "    \n",
    "    import ast\n",
    "    from google.cloud import bigquery\n",
    "    import logging\n",
    "    \n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logging.info(f\"Starting the creation of a BQ table to store text classification results.\")\n",
    "    \n",
    "    bq = bigquery.Client(project=project)\n",
    "    \n",
    "    try:\n",
    "        dataset = bigquery.Dataset(dataset_id)\n",
    "        dataset.location = \"US\"\n",
    "        bq.get_dataset(dataset_id)  # Make an API request.\n",
    "        logging.info(\"Dataset {} already exists\".format(dataset_id))\n",
    "    except Exception as e:\n",
    "        logging.info(\"Dataset {} is not found\".format(dataset_id))\n",
    "        dataset = bq.create_dataset(dataset, timeout=30)  # Make an API request.\n",
    "        dataset.location = \"US\"\n",
    "        logging.info(\"Created dataset {}.{}\".format(bq.project, dataset.dataset_id))\n",
    "    finally:\n",
    "        # create table\n",
    "        schema = [bigquery.SchemaField(**dct) for dct in ast.literal_eval(schema)]\n",
    "        table = bigquery.Table(table_id, schema=schema)\n",
    "        table = bq.create_table(table)\n",
    "        logging.info(f\"Created table {table_id}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2911eba3",
   "metadata": {},
   "source": [
    "### Component 4.2: Performing text classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b656c35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component()\n",
    "def text_class_predict():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6325c637",
   "metadata": {},
   "source": [
    "### Component 4.3: Storing text classification results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cea027f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component()\n",
    "def store_text_class_results():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e025f55",
   "metadata": {},
   "source": [
    "### Component 5.1: Creating image classification results table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "510ed04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(packages_to_install=['google-cloud-bigquery'])\n",
    "def create_img_class_results_table(project:str, \n",
    "                                    dataset_id:str, \n",
    "                                    table_id:str, \n",
    "                                    schema:str):\n",
    "    \n",
    "    import ast\n",
    "    from google.cloud import bigquery\n",
    "    import logging\n",
    "    \n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logging.info(f\"Starting the creation of a BQ table to store image classification results.\")\n",
    "    \n",
    "    bq = bigquery.Client(project=project)\n",
    "    \n",
    "    try:\n",
    "        dataset = bigquery.Dataset(dataset_id)\n",
    "        dataset.location = \"US\"\n",
    "        bq.get_dataset(dataset_id)  # Make an API request.\n",
    "        logging.info(\"Dataset {} already exists\".format(dataset_id))\n",
    "    except Exception as e:\n",
    "        logging.info(\"Dataset {} is not found\".format(dataset_id))\n",
    "        dataset = bq.create_dataset(dataset, timeout=30)  # Make an API request.\n",
    "        dataset.location = \"US\"\n",
    "        logging.info(\"Created dataset {}.{}\".format(bq.project, dataset.dataset_id))\n",
    "    finally:\n",
    "        # create table\n",
    "        schema = [bigquery.SchemaField(**dct) for dct in ast.literal_eval(schema)]\n",
    "        table = bigquery.Table(table_id, schema=schema)\n",
    "        table = bq.create_table(table)\n",
    "        logging.info(f\"Created table {table_id}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a761b66",
   "metadata": {},
   "source": [
    "### Component 5.2: Performing image classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e540d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component()\n",
    "def img_class_predict():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27947048",
   "metadata": {},
   "source": [
    "### Component 5.3: Storing image classification results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c25eced",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component()\n",
    "def store_img_class_results():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a6e37b",
   "metadata": {},
   "source": [
    "### Component 6.1: Creating object detection results table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bad8e7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(packages_to_install=['google-cloud-bigquery'])\n",
    "def create_obj_detection_results_table(project:str, \n",
    "                                       dataset_id:str, \n",
    "                                       table_id:str, \n",
    "                                       schema:str):\n",
    "    \n",
    "    import ast\n",
    "    from google.cloud import bigquery\n",
    "    import logging\n",
    "    \n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logging.info(f\"Starting the creation of a BQ table t store object detection results.\")\n",
    "    \n",
    "    \n",
    "    bq = bigquery.Client(project=project)\n",
    "    \n",
    "    try:\n",
    "        dataset = bigquery.Dataset(dataset_id)\n",
    "        dataset.location = \"US\"\n",
    "        bq.get_dataset(dataset_id)  # Make an API request.\n",
    "        logging.info(\"Dataset {} already exists\".format(dataset_id))\n",
    "    except Exception as e:\n",
    "        logging.info(\"Dataset {} is not found\".format(dataset_id))\n",
    "        dataset = bq.create_dataset(dataset, timeout=30)  # Make an API request.\n",
    "        dataset.location = \"US\"\n",
    "        logging.info(\"Created dataset {}.{}\".format(bq.project, dataset.dataset_id))\n",
    "    finally:\n",
    "        # create table\n",
    "        schema = [bigquery.SchemaField(**dct) for dct in ast.literal_eval(schema)]\n",
    "        table = bigquery.Table(table_id, schema=schema)\n",
    "        table = bq.create_table(table)\n",
    "        logging.info(f\"Created table {table_id}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e3eec8",
   "metadata": {},
   "source": [
    "### Component 6.2: Performing object detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0cd6249f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(packages_to_install=['google-cloud-bigquery', 'google-cloud-storage',  'google-cloud-aiplatform'])\n",
    "def obj_detection_predict(project: str,\n",
    "                          region: str,\n",
    "                          bucket_name: str,\n",
    "                          img_blob: str,\n",
    "                          objdet_endpoint: str) -> NamedTuple(\"Outputs\", [(\"predictions\", Artifact),]):\n",
    "    \n",
    "    # IMPORTS     \n",
    "    import os\n",
    "    import tempfile\n",
    "    import logging\n",
    "    import traceback as tb\n",
    "    from collections import namedtuple\n",
    "    from google.cloud import bigquery\n",
    "    from google.cloud import storage\n",
    "    from google.cloud import aiplatform\n",
    "    from fnmatch import fnmatch\n",
    "    import base64\n",
    "    from google.cloud.aiplatform.gapic.schema import predict\n",
    "    \n",
    "    \n",
    "    # AUXILIARY LIBRARIES\n",
    "    def get_bucket_file_list(bucket_name, fname_template='*'):\n",
    "        '''!@brief Function that returns the list of files in a bucket.\n",
    "        @param bucket (string) Bucket name.\n",
    "        @param fname_template (string) Template for filtering blob names \n",
    "        that supports Unix shell-style wildcards. For more info: \n",
    "        https://docs.python.org/3/library/fnmatch.html\n",
    "\n",
    "        @return (list of srtings) List of blob names in a bucket which \n",
    "        fullfills template structure.\n",
    "        '''\n",
    "        storage_client = storage.Client()\n",
    "        blobs = storage_client.list_blobs(bucket_name)\n",
    "        blob_lst = [blob.name for blob in blobs]  \n",
    "        file_lst = [fname for fname in blob_lst if fnmatch(fname, fname_template)]\n",
    "\n",
    "        return file_lst\n",
    "    \n",
    "    \n",
    "    def predict_image_classification_sample(\n",
    "        project: str,\n",
    "        endpoint_id: str,\n",
    "        filename: str,\n",
    "        location: str = \"us-central1\",\n",
    "        api_endpoint: str = \"us-central1-aiplatform.googleapis.com\"):\n",
    "        \n",
    "        # The AI Platform services require regional API endpoints.\n",
    "        client_options = {\"api_endpoint\": api_endpoint}\n",
    "        client = aiplatform.gapic.PredictionServiceClient(client_options=client_options)\n",
    "        with open(filename, \"rb\") as f:\n",
    "            file_content = f.read()\n",
    "            print('file: '+ str(file_content))\n",
    "\n",
    "        # The format of each instance should conform to the deployed model's prediction input schema.\n",
    "        encoded_content = base64.b64encode(file_content).decode(\"utf-8\")\n",
    "        print('img encoded: '+ str(encoded_content))\n",
    "        instance = predict.instance.ImageObjectDetectionPredictionInstance(content=encoded_content).to_value()\n",
    "        instances = [instance]\n",
    "        parameters = predict.params.ImageObjectDetectionPredictionParams(confidence_threshold=0.5, max_predictions=5).to_value()\n",
    "        endpoint = client.endpoint_path(project=project, location=location, endpoint=endpoint_id)\n",
    "        response = client.predict(endpoint=endpoint, instances=instances, parameters=parameters)\n",
    "        predictions = response.predictions\n",
    "        return [dict(prediction) for prediction in predictions]\n",
    "    \n",
    "\n",
    "    # MAIN BODY:    \n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logging.info(f\"Starting the object detection task.\")\n",
    "    \n",
    "    client = bigquery.Client()\n",
    "    files = get_bucket_file_list(bucket_name=f'{bucket_name}',\n",
    "                                 fname_template=img_blob+'*')\n",
    "    logging.info(str(files))\n",
    "    predictions = []\n",
    "    for file in files:             \n",
    "        # Downloading the file as a temporal file:\n",
    "        storage_client = storage.Client()\n",
    "        bucket = storage_client.bucket(bucket_name)\n",
    "        blob = bucket.blob(file)\n",
    "        _, path = tempfile.mkstemp()\n",
    "        blob.download_to_filename(path + '.png')    \n",
    "        \n",
    "        #print(str(file))\n",
    "        \n",
    "        # Obtaining online prediction:\n",
    "        preds = predict_image_classification_sample(project=project,\n",
    "                                                    endpoint_id=objdet_endpoint,\n",
    "                                                    filename=f'{path}.png',\n",
    "                                                    location=region,\n",
    "                                                    api_endpoint='us-central1-aiplatform.googleapis.com')\n",
    "    \n",
    "        print(str(preds))\n",
    "        \n",
    "        # Parsing prediction:\n",
    "        objdet_pred = preds[0]['displayNames'][0]\n",
    "        objdet_confidence = preds[0]['confidences'][0]\n",
    "        objdet_xmin, objdet_xmax = preds[0]['bboxes'][0][0], preds[0]['bboxes'][0][1]\n",
    "        objdet_ymin, objdet_ymax = preds[0]['bboxes'][0][2], preds[0]['bboxes'][0][3]\n",
    "        \n",
    "        # Storing prediction into the BQ table:\n",
    "        predictions.append(\n",
    "            {'file': f'{file}'.split('/')[-1],\n",
    "             'label': f'{objdet_pred}',\n",
    "             'score': f'{objdet_confidence}',\n",
    "             'xmin': f'{objdet_xmin}',\n",
    "             'xmax': f'{objdet_xmax}',\n",
    "             'ymin': f'{objdet_ymin}',\n",
    "             'ymax': f'{objdet_ymax}'}\n",
    "        )\n",
    "\n",
    "        logging.info(str(predictions))\n",
    "        \n",
    "    logging.info(f\"The object detection task has finished successfully .\")    \n",
    "    \n",
    "    # Creating the named tuple with the results:\n",
    "    outputs = namedtuple('Outputs',\n",
    "                         ['predictions'])\n",
    "    \n",
    "    return outputs(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9516cab",
   "metadata": {},
   "source": [
    "### Component 6.3: Storing object detection results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ff287982",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(packages_to_install=['google-cloud-bigquery'])\n",
    "def store_obj_detection_results(table_id: str,\n",
    "                                preds: Input[Artifact]):\n",
    "    \n",
    "    import logging\n",
    "    import ast\n",
    "    from google.cloud import bigquery\n",
    "    \n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logging.info(f\"Starting the storage of the object detection results.\")\n",
    "    \n",
    "    client = bigquery.Client()\n",
    "    \n",
    "    # Parsing the artifact:\n",
    "    with open(preds.path, \"r\") as preds_file:\n",
    "        contents = preds_file.read()\n",
    "        print(f\"generic contents: {contents}\")\n",
    "        print(type(contents))\n",
    "        \n",
    "        predictions = ast.literal_eval(contents)\n",
    "\n",
    "        for prediction in predictions:\n",
    "            errors = client.insert_rows_json(table_id, [prediction])\n",
    "            if errors == []:\n",
    "                logging.info(\"New row have been added.\")\n",
    "            else:\n",
    "                logging.info(\"Encountered errors while inserting rows: {}\".format(errors))\n",
    "\n",
    "            logging.info(f\"The object detection results have been stored successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b0605b",
   "metadata": {},
   "source": [
    "## Creating and Compiling the Vertex AI Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "18a99334",
   "metadata": {},
   "outputs": [],
   "source": [
    "@kfp.dsl.pipeline(name=PIPELINE_NAME, \n",
    "                  description='Pipeline that process patents pdf files.',\n",
    "                  pipeline_root=PIPELINE_ROOT)\n",
    "\n",
    "def pipeline():\n",
    "    # Preprocessing pipeline:\n",
    "    perform_ocr_on_pdfs_task = perform_ocr_on_pdfs(src_path=f\"gs://{BUCKET}/{PDF_BUCKET_PATH}\", \n",
    "                                                   dst_path=f\"gs://{BUCKET}/{TXT_BUCKET_PATH}\",\n",
    "                                                   uuid=UUID,\n",
    "                                                   project=PROJECT)\n",
    "    \n",
    "    transform_pdfs_into_png_task = transform_pdfs_into_png()\n",
    "    transform_pdfs_into_png_task.after(perform_ocr_on_pdfs_task)\n",
    "\n",
    "    create_bq_results_dataset_task = create_bq_results_dataset(project=PROJECT,\n",
    "                                                               dataset_id=RES_DATASET_ID)\n",
    "    create_bq_results_dataset_task.after(transform_pdfs_into_png_task)\n",
    "    \n",
    "    # Text classification pipeline:\n",
    "    create_text_class_results_table_task = create_text_class_results_table(project=PROJECT, \n",
    "                                                                           dataset_id=RES_DATASET_ID, \n",
    "                                                                           table_id=TCN_RESTABLE_NAME, \n",
    "                                                                           schema=TCN_RESTABLE_SCHEMA)\n",
    "    create_text_class_results_table_task.after(create_bq_results_dataset_task)\n",
    "    \n",
    "    text_class_predict_task = text_class_predict()\n",
    "    text_class_predict_task.after(create_text_class_results_table_task)\n",
    "    \n",
    "    store_text_class_results_task = store_text_class_results()\n",
    "    store_text_class_results_task.after(text_class_predict_task)\n",
    "    \n",
    "    # Image classification pipeline:\n",
    "    create_img_class_results_table_task = create_img_class_results_table(project=PROJECT, \n",
    "                                                                         dataset_id=RES_DATASET_ID, \n",
    "                                                                         table_id=ICN_RESTABLE_NAME, \n",
    "                                                                         schema=ICN_RESTABLE_SCHEMA)\n",
    "    create_img_class_results_table_task.after(create_bq_results_dataset_task)\n",
    "    \n",
    "    img_class_predict_task = img_class_predict()\n",
    "    img_class_predict_task.after(create_img_class_results_table_task)\n",
    "    \n",
    "    store_img_class_results_task = store_img_class_results()\n",
    "    store_img_class_results_task.after(img_class_predict_task)\n",
    "        \n",
    "    # Object detection pipeline:\n",
    "    create_obj_detection_results_table_task = create_obj_detection_results_table(project=PROJECT, \n",
    "                                                                                 dataset_id=RES_DATASET_ID, \n",
    "                                                                                 table_id=ODM_RESTABLE_NAME, \n",
    "                                                                                 schema=ODM_RESTABLE_SCHEMA)\n",
    "    create_obj_detection_results_table_task.after(create_bq_results_dataset_task)\n",
    "    \n",
    "    obj_detection_predict_task = obj_detection_predict(project=PROJECT,\n",
    "                                                       region=REGION,\n",
    "                                                       bucket_name=BUCKET,\n",
    "                                                       img_blob=PNG_BUCKET_PATH,\n",
    "                                                       objdet_endpoint=ODM_ENDPT_NAME)\n",
    "    obj_detection_predict_task.after(create_obj_detection_results_table_task)\n",
    "    \n",
    "    store_obj_detection_results_task = store_obj_detection_results(table_id=ODM_RESTABLE_NAME,\n",
    "                                                                   preds=obj_detection_predict_task.outputs['predictions'])   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "76ead83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(LOCAL_PIPELINE_PATH):\n",
    "    os.mkdir(LOCAL_PIPELINE_PATH)\n",
    "\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=pipeline,\n",
    "    package_path=LOCAL_PIPELINE_JSON\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6071f2d8",
   "metadata": {},
   "source": [
    "## Launching the Vertex AI Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ff9c8adc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/kfp/v2/google/client/client.py:175: FutureWarning: AIPlatformClient will be deprecated in v1.9. Please use PipelineJob https://googleapis.dev/python/aiplatform/latest/_modules/google/cloud/aiplatform/pipeline_jobs.html in Vertex SDK. Install the SDK using \"pip install google-cloud-aiplatform\"\n",
      "  category=FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "# Instantiating an API client object:\n",
    "# TODO: use the new Vertex AI.\n",
    "api_client = AIPlatformClient(\n",
    "    project_id=PROJECT,\n",
    "    region=REGION,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ea4e8beb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "See the Pipeline job <a href=\"https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/processing-of-patents-20210820061204?project=qwiklabs-gcp-00-373ac55d0e0a\" target=\"_blank\" >here</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = api_client.create_run_from_job_spec(\n",
    "    LOCAL_PIPELINE_JSON,\n",
    "    pipeline_root=f\"{PIPELINE_ROOT}\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-5.m76",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-5:m76"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
