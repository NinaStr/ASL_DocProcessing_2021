{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a26a42c",
   "metadata": {},
   "source": [
    "# Text Classification\n",
    "input: prepared pdfs\n",
    "output: created BQ tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "c8791039",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT = !gcloud config get-value project # returns SList\n",
    "PROJECT = PROJECT[0] # gets first element in list -> str\n",
    "REGION = \"us-central1\"  \n",
    "MODEL_RESOURCE_NAME = \"\" # TODO\n",
    "import os\n",
    "os.environ[\"PROJECT\"] = PROJECT\n",
    "os.environ[\"REGION\"] = REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c336faa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "bq = bigquery.Client(project=PROJECT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "960c9844",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0fbda1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "from google.cloud import vision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "15a10cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "import json\n",
    "import pandas as pd\n",
    "import tempfile "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "00c60a3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2021-08-16_143228'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a3663d6d",
   "metadata": {},
   "source": [
    "## TO-DO List:\n",
    "1. Receive input: Bucket with PDF files (str) DONE\n",
    "2. Preprocess/Transform PDF for task ONGOING\n",
    "    1. OCR --> Text DONE\n",
    "    2. maybe: save as JSONL or text file, documentation unclear: https://cloud.google.com/vertex-ai/docs/predictions/batch-predictions#text_1\n",
    "3. Batch Predict ONGOING\n",
    "4. Save result in storage DONE (csv)\n",
    "5. Load into BigQuery DONE (csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1523d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "gsutil cp gs://qwiklabs-gcp-00-373ac55d0e0a/labeled_patents/pdf/us_076.pdf gs://2021_08_16_tcn_dev/2021-08-16/us_076.pdf\n",
    "gsutil cp gs://qwiklabs-gcp-00-373ac55d0e0a/labeled_patents/pdf/med_tech_8.pdf gs://2021_08_16_tcn_dev/2021-08-16/med_tech_8.pdf\n",
    "gsutil cp gs://qwiklabs-gcp-00-373ac55d0e0a/labeled_patents/pdf/computer_vision_1.pdf gs://2021_08_16_tcn_dev/2021-08-16/computer_vision_1.pdf\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "76c28be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: \"main function\" Step 1:\n",
    "def predict(bucket_name, folder=None):\n",
    "    \"\"\"Runs AutoML Text classifier on a GCS folder and pushes results to BigQuery.\"\"\"\n",
    "    logger.info(\"Starting text classification.\")\n",
    "    \n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    \n",
    "    # Step 2: Preprocess/Transform PDF for task\n",
    "    logger.info(\"preprocessing files\")\n",
    "    text_list = preprocess(bucket, bucket_name, folder)\n",
    "    \n",
    "        \n",
    "    # Step 3:predict with automl\n",
    "    logger.info(\"predict with AutoML\")\n",
    "    str_time = datetime.now().strftime(\"%Y-%m-%d_%H%M%S\")\n",
    "    if folder:\n",
    "        gcs_destination = f\"gs://{bucket_name}/{folder}/automl-tcn-{str_time}\"\n",
    "    else:\n",
    "        gcs_destination = f\"gs://{bucket_name}/automl-tcn-{str_time}\"\n",
    "    \n",
    "    results = run_automl_text_batch(bucket, bucket_name, folder, text_list, gcs_destination)\n",
    "    \n",
    "#     # Step 4: save result in storage\n",
    "#     logger.info(\"save results to storage\")\n",
    "#     path_to_csv = save_to_storage(bucket, bucket_name, folder, results)\n",
    "    \n",
    "#     # Step 5: Load storage result in BQ\n",
    "#     logger.info(\"load results into BigQuery\")\n",
    "#     status = load_to_bigquery(path_to_csv)\n",
    "    \n",
    "#     logging.info(f\"finished task with status {status}\")\n",
    "    \n",
    "\n",
    "# Step 2:\n",
    "def preprocess(bucket, bucket_name, folder=None):\n",
    "    # TODO: implement\n",
    "    \n",
    "    list_of_text = []\n",
    "    list_of_dst_uri = []\n",
    "\n",
    "    \n",
    "    if folder:\n",
    "        blob_list  = [blob for blob in list(bucket.list_blobs()) if blob.name.startswith(folder) and blob.name.endswith(\".pdf\")]\n",
    "    else: \n",
    "        blob_list  = [blob for blob in list(bucket.list_blobs()) if blob.name.endswith(\".pdf\")]\n",
    "        \n",
    "    last_blob_name = \"\"\n",
    "        \n",
    "    for blob in blob_list:\n",
    "        \n",
    "        if folder:\n",
    "            gcs_destination_uri = f\"gs://{bucket_name}/{folder}-text/{os.path.basename(blob.name)}\"\n",
    "            last_blob_name = f\"{folder}-text/{os.path.basename(blob.name)}output-1-to-1.json\"\n",
    "            \n",
    "        else:\n",
    "            gcs_destination_uri = f\"gs://{bucket_name}/{os.path.basename(blob.name)}\"\n",
    "            last_blob_name = blob.name+\"output-1-to-1.json\"\n",
    "        \n",
    "        list_of_dst_uri.append(last_blob_name)\n",
    "        \n",
    "        \n",
    "        gcs_source_uri = f\"gs://{bucket_name}/{blob.name}\"\n",
    "        async_detect_document(gcs_source_uri, gcs_destination_uri)\n",
    "        \n",
    "        logging.info(f\"started ocr for {blob.name}. Source uri: {gcs_source_uri}. Destination uri: {gcs_destination_uri}\")\n",
    "        \n",
    "#     last_uri = list_of_dst_uri[:-1]\n",
    "    \n",
    "    while(not bucket.blob(last_blob_name).exists()):\n",
    "        logging.info(\"waiting on operation to finish\")\n",
    "        time.sleep(5)\n",
    "        \n",
    "    # NEW: create .txt files\n",
    "    for dst_uri in list_of_dst_uri:\n",
    "        ocr_blob = bucket.blob(dst_uri)\n",
    "        json_string = ocr_blob.download_as_string()\n",
    "        response = json.loads(json_string)\n",
    "        text = response['responses'][0]['fullTextAnnotation']['text']\n",
    "        list_of_text.append(annotation[\"text\"])\n",
    "        \n",
    "        txt_path = dst_uri.replace(\"output-1-to-1.json\", \".txt\")\n",
    "        text_blob = bucket.blob(txt_path)\n",
    "        text_blob.upload_from_string(text)\n",
    "\n",
    "    return list_of_text\n",
    "\n",
    "# Step 3: predict with automl\n",
    "# WARNING: check which type content should be????\n",
    "# gcs source ???? TODO\n",
    "def run_automl_text_batch(bucket, bucket_name, folder, content, gcs_destination):\n",
    "    # TODO: implement\n",
    "    content = content[0]\n",
    "    results = []\n",
    "    \n",
    "    create_batch_prediction_job(\n",
    "        PROJECT, \n",
    "        REGION, \n",
    "        MODEL_RESOURCE_NAME,\n",
    "        gcs_source=[content],\n",
    "        gcs_destination=gcs_destination,\n",
    "        sync=True)\n",
    "    \n",
    "    \n",
    "    # read results \n",
    "    results = []\n",
    "    blob_list = bucket.list_blobs(gcs_destination)\n",
    "    for blob in blob_list:\n",
    "        blob_str = blob.download_as_string()\n",
    "        response = json.loads(blob_str)\n",
    "        \n",
    "        if folder: \n",
    "            file = f\"gs://{bucket_name}/{folder}/{os.path.basename(blob)}\" # ??? see prediction output\n",
    "        else:\n",
    "            file = f\"gs://{bucket_name}/{os.path.basename(blob)}\"\n",
    "        \n",
    "\n",
    "        results.append({\n",
    "            'file': file, \n",
    "            'subject': response[\"prediction\"][\"displaynames\"][0],\n",
    "            'score':  response[\"prediction\"][\"confidences\"][0],\n",
    "            })\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def create_batch_prediction_job(\n",
    "    project: PROJECT,\n",
    "    location: REGION,\n",
    "    model_resource_name: MODEL_RESOURCE_NAME, # TODO\n",
    "    job_display_name: \"tcn_batch\",\n",
    "    gcs_source: str,\n",
    "    gcs_destination: str,\n",
    "    sync: bool = True,\n",
    "):\n",
    "    aiplatform.init(project=project, location=location)\n",
    "\n",
    "    my_model = aiplatform.Model(model_resource_name)\n",
    "\n",
    "    batch_prediction_job = my_model.batch_predict(\n",
    "        job_display_name=job_display_name,\n",
    "        gcs_source=gcs_source,\n",
    "        gcs_destination_prefix=gcs_destination,\n",
    "        sync=sync,\n",
    "    )\n",
    "\n",
    "    batch_prediction_job.wait()\n",
    "\n",
    "    logging.info(batch_prediction_job.display_name)\n",
    "    logging.info(batch_prediction_job.resource_name)\n",
    "    logging.info(batch_prediction_job.state)\n",
    "    return batch_prediction_job\n",
    "\n",
    "\n",
    "# Step 4: save result in storage\n",
    "def save_to_storage(bucket, bucket_name, folder, results: list):\n",
    "    \"\"\"\n",
    "    save batch predictions to storage as csv\n",
    "    returns path\n",
    "    \n",
    "    results\n",
    "    [\n",
    "    {'file': ..., 'subject': ..., 'score': ...},\n",
    "    {'file': ..., 'subject': ..., 'score': ...},\n",
    "    {'file': ..., 'subject': ..., 'score': ...},\n",
    "    ]\n",
    "    \"\"\"\n",
    "    filename = \"predicions.csv\"\n",
    "    dst_path = f\"gs://{bucket_name}/{filename}\"\n",
    "\n",
    "    \n",
    "    df = pd.DataFrame.from_records(results)\n",
    "    \n",
    "    df.to_csv(filename, index=False, header=False)\n",
    "    blob = bucket.blob(filename)\n",
    "    with open(filename, \"rb\") as my_file:\n",
    "        blob.upload_from_file(my_file)\n",
    "    \n",
    "    return dst_path\n",
    "\n",
    "# Step 5: Load storage result in BQ ---  great for utils.py\n",
    "def load_to_bigquery(bucket, bucket_name, gcs_path):\n",
    "    \"\"\"loads csv data in storage to BQ\"\"\"\n",
    "    # create new dataset and table\n",
    "    str_time = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    dataset_id = \"qwiklabs-gcp-00-373ac55d0e0a.docprocessing_20210816_153004\" #  f\"{bq.project}.docprocessing_{str_time}\"\n",
    "    table_id = f\"{dataset_id}.docprocessing_tcn\"\n",
    "    print(dataset_id)\n",
    "\n",
    "    # TODO(developer): Set dataset_id to the ID of the dataset to create.\n",
    "    \n",
    "    # Construct a full Dataset object to send to the API.\n",
    "    dataset = bigquery.Dataset(dataset_id)\n",
    "\n",
    "    # TODO(developer): Specify the geographic location where the dataset should reside.\n",
    "    dataset.location = \"US\"\n",
    "\n",
    "    # Send the dataset to the API for creation, with an explicit timeout.\n",
    "    # Raises google.api_core.exceptions.Conflict if the Dataset already\n",
    "    # exists within the project.\n",
    "    try:\n",
    "        bq.get_dataset(dataset_id)  # Make an API request.\n",
    "        logging.info(\"Dataset {} already exists\".format(dataset_id))\n",
    "    except NotFound:\n",
    "        logging.info(\"Dataset {} is not found\".format(dataset_id))\n",
    "        dataset = bq.create_dataset(dataset, timeout=30)  # Make an API request.\n",
    "        logging.info(\"Created dataset {}.{}\".format(bq.project, dataset.dataset_id))\n",
    "    \n",
    "    # create bigquery table and upload csv\n",
    "    job_config = bigquery.LoadJobConfig(\n",
    "        schema=[\n",
    "            bigquery.SchemaField(\"file\", \"STRING\"),\n",
    "            bigquery.SchemaField(\"subject\", \"STRING\"),\n",
    "            bigquery.SchemaField(\"score\", \"FLOAT\"),\n",
    "        ],\n",
    "        skip_leading_rows=0,\n",
    "        # The source format defaults to CSV, so the line below is optional.\n",
    "        source_format=bigquery.SourceFormat.CSV,\n",
    "        allow_quoted_newlines=True,\n",
    "\n",
    "    )\n",
    "    uri = gcs_path\n",
    "\n",
    "    load_job = bq.load_table_from_uri(\n",
    "        uri, table_id, job_config=job_config\n",
    "    )  # Make an API request.\n",
    "\n",
    "    load_job.result()  # Waits for the job to complete.\n",
    "\n",
    "    destination_table = bq.get_table(table_id)  # Make an API request.\n",
    "    print(\"Loaded {} rows.\".format(destination_table.num_rows))\n",
    "\n",
    "# helper functions ---- great candidates for utils.py\n",
    "def async_detect_document(gcs_source_uri, gcs_destination_uri):\n",
    "    \"\"\"OCR with PDF/TIFF as source files on GCS\n",
    "    Link to documentation (types): https://googleapis.dev/python/vision/latest/vision_v1/types.html#google.cloud.vision_v1.types.OutputConfig\n",
    "    \"\"\"\n",
    "\n",
    "    # Supported mime_types are: 'application/pdf' and 'image/tiff'\n",
    "    mime_type = 'application/pdf'\n",
    "\n",
    "    # How many pages should be grouped into each json output file.\n",
    "    batch_size = 1\n",
    "\n",
    "    client = vision.ImageAnnotatorClient()\n",
    "\n",
    "    feature = vision.Feature(\n",
    "        type_=vision.Feature.Type.DOCUMENT_TEXT_DETECTION)\n",
    "\n",
    "    # source\n",
    "    gcs_source = vision.GcsSource(uri=gcs_source_uri)\n",
    "    input_config = vision.InputConfig(\n",
    "        gcs_source=gcs_source, mime_type=mime_type)\n",
    "    \n",
    "    # destination\n",
    "    gcs_destination = vision.GcsDestination(uri=gcs_destination_uri)\n",
    "    output_config = vision.OutputConfig(\n",
    "        gcs_destination=gcs_destination, \n",
    "        batch_size=batch_size)\n",
    "    \n",
    "\n",
    "    async_request = vision.AsyncAnnotateFileRequest(\n",
    "        features=[feature], \n",
    "        input_config=input_config,\n",
    "        output_config=output_config\n",
    "    )\n",
    "\n",
    "    operation = client.async_batch_annotate_files(\n",
    "        requests=[async_request])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6920700f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "00c1cbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list_of_dst_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "999b5861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict(bucket_name=\"2021_08_16_tcn_dev\", folder=\"2021-08-16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "5d2cbd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = \"2021_08_16_tcn_dev\"\n",
    "storage_client = storage.Client()\n",
    "bucket = storage_client.bucket(bucket_name)\n",
    "    \n",
    "# list_of_dst_uri = preprocess(bucket, bucket_name=\"2021_08_16_tcn_dev\", folder=\"2021-08-16\")\n",
    "\n",
    "# save_to_storage(bucket, bucket_name, None, results=[\n",
    "#     {'file': 'myfile1', 'subject': 'A', 'score': 1},\n",
    "#     {'file': 'myfile2', 'subject': 'B', 'score': 2},\n",
    "#     {'file': 'myfile3', 'subject': 'C', 'score': 3},\n",
    "#     ])\n",
    "\n",
    "# load_to_bigquery(bucket, bucket_name, gcs_path='gs://2021_08_16_tcn_dev/predicions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "92d7ce96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list_of_dst_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "06be886f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://2021_08_16_tcn_dev/2021-08-16-text/computer_vision_1.pdf.txt\n",
      "gs://2021_08_16_tcn_dev/2021-08-16-text/computer_vision_1.pdfoutput-1-to-1.json\n",
      "gs://2021_08_16_tcn_dev/2021-08-16-text/med_tech_8.pdf.txt\n",
      "gs://2021_08_16_tcn_dev/2021-08-16-text/med_tech_8.pdfoutput-1-to-1.json\n",
      "gs://2021_08_16_tcn_dev/2021-08-16-text/us_076.pdf.txt\n",
      "gs://2021_08_16_tcn_dev/2021-08-16-text/us_076.pdfoutput-1-to-1.json\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls gs://2021_08_16_tcn_dev/2021-08-16-text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fd10fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "ce38cdad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dst_uris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fdc221d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96aafa5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f378a87b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-5.m75",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-5:m75"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
