{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bba27a2b",
   "metadata": {},
   "source": [
    "# Text Classification\n",
    "input: prepared pdfs\n",
    "output: created BQ tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "eeca23de",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT = !gcloud config get-value project # returns SList\n",
    "PROJECT = PROJECT[0] # gets first element in list -> str\n",
    "REGION = \"us-central1\"  \n",
    "MODEL_RESOURCE_NAME = \"2393478483993952256\"\n",
    "import os\n",
    "os.environ[\"PROJECT\"] = PROJECT\n",
    "os.environ[\"REGION\"] = REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bcdbff8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "bq = bigquery.Client(project=PROJECT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11780547",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3725dbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "from google.cloud import vision\n",
    "from google.cloud import aiplatform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8e91864",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "import json\n",
    "import pandas as pd\n",
    "import tempfile "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a28aa3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "from google.cloud.aiplatform.gapic.schema import predict\n",
    "from google.cloud.aiplatform.v1.schema.predict.instance_v1.types import TextClassificationPredictionInstance\n",
    "from google.protobuf import json_format\n",
    "from google.protobuf.struct_pb2 import Value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b278c23f",
   "metadata": {},
   "source": [
    "## TO-DO List:\n",
    "1. Receive input: Bucket with PDF files (str) DONE\n",
    "2. Preprocess/Transform PDF for task ONGOING\n",
    "    1. OCR --> Text DONE\n",
    "    2. maybe: save as JSONL or text file, documentation unclear: https://cloud.google.com/vertex-ai/docs/predictions/batch-predictions#text_1\n",
    "3. Batch Predict ONGOING\n",
    "4. Save result in storage DONE (csv)\n",
    "5. Load into BigQuery DONE (csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "f8a052de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ngsutil cp gs://qwiklabs-gcp-00-373ac55d0e0a/labeled_patents/pdf/us_076.pdf gs://2021_08_16_tcn_dev/2021-08-16/us_076.pdf\\ngsutil cp gs://qwiklabs-gcp-00-373ac55d0e0a/labeled_patents/pdf/med_tech_8.pdf gs://2021_08_16_tcn_dev/2021-08-16/med_tech_8.pdf\\ngsutil cp gs://qwiklabs-gcp-00-373ac55d0e0a/labeled_patents/pdf/computer_vision_1.pdf gs://2021_08_16_tcn_dev/2021-08-16/computer_vision_1.pdf\\n'"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "gsutil cp gs://qwiklabs-gcp-00-373ac55d0e0a/labeled_patents/pdf/us_076.pdf gs://2021_08_16_tcn_dev/2021-08-16/us_076.pdf\n",
    "gsutil cp gs://qwiklabs-gcp-00-373ac55d0e0a/labeled_patents/pdf/med_tech_8.pdf gs://2021_08_16_tcn_dev/2021-08-16/med_tech_8.pdf\n",
    "gsutil cp gs://qwiklabs-gcp-00-373ac55d0e0a/labeled_patents/pdf/computer_vision_1.pdf gs://2021_08_16_tcn_dev/2021-08-16/computer_vision_1.pdf\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "75fb4513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: \"main function\" Step 1:\n",
    "def main_predict(bucket_name):\n",
    "    \"\"\"Runs AutoML Text classifier on a GCS bucket and pushes results to BigQuery.\"\"\"\n",
    "    logger.info(\"Starting text classification.\")\n",
    "\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "\n",
    "    # Step 2: Preprocess/Transform PDF for task\n",
    "    logger.info(\"preprocessing files\")\n",
    "    jsonl_path = preprocess(bucket, bucket_name)\n",
    "\n",
    "    # Step 3:predict with automl\n",
    "    logger.info(\"predict with AutoML\")\n",
    "    str_time = datetime.now().strftime(\"%Y-%m-%d_%H%M%S\")\n",
    "    gcs_destination = f\"gs://{bucket_name}/automl-tcn-{str_time}\"\n",
    "\n",
    "\n",
    "    results = run_automl_text_batch(bucket, bucket_name, jsonl_path, gcs_destination)\n",
    "\n",
    "    # Step 4: save result in storage\n",
    "    logger.info(\"save results to storage\")\n",
    "    path_to_csv = save_to_storage(bucket, bucket_name, results)\n",
    "\n",
    "    # Step 5: Load storage result in BQ\n",
    "    logger.info(\"load results into BigQuery\")\n",
    "    status = load_to_bigquery(bucket, bucket_name, path_to_csv)\n",
    "\n",
    "    logging.info(f\"finished task with status {status}\")\n",
    "    \n",
    "\n",
    "# Step 2: run ocr, save results as txt files, save paths in jsonl, push jsonl to storage\n",
    "def preprocess(bucket, bucket_name):\n",
    "    list_of_dst_uri = []\n",
    "    jsonl_items = []\n",
    "\n",
    "    blob_list  = [blob for blob in list(bucket.list_blobs()) if blob.name.endswith(\".pdf\")]\n",
    "    last_blob_name = \"\"\n",
    "        \n",
    "    for blob in blob_list:\n",
    "        gcs_source_uri = f\"gs://{bucket_name}/{blob.name}\"\n",
    "        gcs_destination_uri = f\"gs://{bucket_name}/{os.path.basename(blob.name)}\"\n",
    "        blob_name = blob.name+\"output-1-to-1.json\"\n",
    "        list_of_dst_uri.append(blob_name)\n",
    "        \n",
    "        # run ocr\n",
    "        async_detect_document(gcs_source_uri, gcs_destination_uri)\n",
    "        logging.info(f\"started ocr for {blob.name}. Source uri: {gcs_source_uri}. Destination uri: {gcs_destination_uri}\")\n",
    "        \n",
    "    logging.info(f\"last blob name = {blob_name}\")\n",
    "        \n",
    "    \n",
    "    while(not bucket.blob(blob_name).exists()):\n",
    "        logging.info(\"waiting on operation to finish\")\n",
    "        time.sleep(5)\n",
    "\n",
    "    # NEW: create .txt files\n",
    "    logging.info(f\"list_of_dst_uri: {list_of_dst_uri}\")\n",
    "    for dst_uri in list_of_dst_uri:\n",
    "        ocr_blob = bucket.blob(dst_uri)\n",
    "        json_string = ocr_blob.download_as_string()\n",
    "        response = json.loads(json_string)\n",
    "        text = response['responses'][0]['fullTextAnnotation']['text'] \n",
    "        txt_path = dst_uri.replace(\"output-1-to-1.json\", \".txt\")\n",
    "        text_blob = bucket.blob(txt_path)\n",
    "        text_blob.upload_from_string(text)\n",
    "        \n",
    "        jsonl_items.append({\n",
    "            \"content\": f\"gs://{bucket_name}/{txt_path}\", \n",
    "            \"mimeType\": \"text/plain\"\n",
    "        })\n",
    "        \n",
    "        logging.info(f\"created jsonl: {json.dumps(jsonl_items)}\")\n",
    "        \n",
    "    jsonl_path = create_jsonl(bucket, bucket_name, jsonl_items)\n",
    "\n",
    "    return jsonl_path\n",
    "\n",
    "def create_jsonl(bucket, bucket_name, json_list):\n",
    "    str_time = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    jsonl_filename = f\"textfiles_{str_time}.jsonl\"\n",
    "    blob = bucket.blob(jsonl_filename)\n",
    "    jsonl_content = \"\"\n",
    "    for item in json_list:\n",
    "        d = json.dumps(item)+\"\\n\"\n",
    "#         d = d.encode('utf8')\n",
    "        jsonl_content = jsonl_content+d\n",
    "        \n",
    "    blob.upload_from_string(jsonl_content)\n",
    "    \n",
    "    return f\"gs://{bucket_name}/{jsonl_filename}\"\n",
    "        \n",
    "\n",
    "\n",
    "# Step 3: predict with automl\n",
    "def run_automl_text_batch(bucket, bucket_name, gcs_source, gcs_destination):\n",
    "    results = []\n",
    "    \n",
    "    job = create_batch_prediction_job(\n",
    "        PROJECT, \n",
    "        REGION, \n",
    "        model_resource_name=MODEL_RESOURCE_NAME, \n",
    "        job_display_name=\"tcn-job\", \n",
    "        gcs_source=gcs_source, \n",
    "        gcs_destination=gcs_destination, \n",
    "        sync=True\n",
    "        )\n",
    "    \n",
    "    logging.info(type(job))\n",
    "    \n",
    "    \n",
    "    # read results \n",
    "    results = []\n",
    "    \n",
    "    blob_list  = [blob for blob in list(bucket.list_blobs()) if os.path.basename(gcs_destination) in blob.name and blob.name.endswith(\".jsonl\")]\n",
    "    for blob in blob_list:\n",
    "        blob_str = blob.download_as_string().decode(\"utf-8\") \n",
    "        responses = []\n",
    "        for line in blob_str.split(\"\\n\")[:-1]:\n",
    "            responses.append(json.loads(str(line)))\n",
    "\n",
    "        for response in responses:\n",
    "            results.append({\n",
    "                'file': response[\"instance\"][\"content\"][:-4], # \"gs://bucket/text.txt\" TODO: check if original path is needed\n",
    "                'subject': response[\"prediction\"][\"displayNames\"][0],\n",
    "                'score':  response[\"prediction\"][\"confidences\"][0],\n",
    "                })\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def create_batch_prediction_job(\n",
    "    project,\n",
    "    location,\n",
    "    model_resource_name,\n",
    "    job_display_name,\n",
    "    gcs_source,\n",
    "    gcs_destination,\n",
    "    sync = True,\n",
    "):\n",
    "    aiplatform.init(project=project, location=location)\n",
    "\n",
    "    my_model = aiplatform.Model(model_resource_name)\n",
    "\n",
    "    batch_prediction_job = my_model.batch_predict(\n",
    "        job_display_name=job_display_name,\n",
    "        gcs_source=gcs_source,\n",
    "        gcs_destination_prefix=gcs_destination,\n",
    "        sync=sync,\n",
    "    )\n",
    "\n",
    "    batch_prediction_job.wait()\n",
    "\n",
    "    logging.info(batch_prediction_job.display_name)\n",
    "    logging.info(batch_prediction_job.resource_name)\n",
    "    logging.info(batch_prediction_job.state)\n",
    "    return batch_prediction_job\n",
    "\n",
    "\n",
    "# Step 4: save result in storage\n",
    "def save_to_storage(bucket, bucket_name, results: list):\n",
    "    \"\"\"\n",
    "    save batch predictions to storage as csv\n",
    "    returns path\n",
    "    \n",
    "    results\n",
    "    [\n",
    "    {'file': ..., 'subject': ..., 'score': ...},\n",
    "    {'file': ..., 'subject': ..., 'score': ...},\n",
    "    {'file': ..., 'subject': ..., 'score': ...},\n",
    "    ]\n",
    "    \"\"\"\n",
    "    filename = \"predictions.csv\"\n",
    "    dst_path = f\"gs://{bucket_name}/{filename}\"\n",
    "\n",
    "    \n",
    "    df = pd.DataFrame.from_records(results)\n",
    "    \n",
    "    df.to_csv(filename, index=False, header=False)\n",
    "    blob = bucket.blob(filename)\n",
    "    with open(filename, \"rb\") as my_file:\n",
    "        blob.upload_from_file(my_file)\n",
    "    \n",
    "    return dst_path\n",
    "\n",
    "# Step 5: Load storage result in BQ ---  great for utils.py\n",
    "def load_to_bigquery(bucket, bucket_name, gcs_path):\n",
    "    \"\"\"loads csv data in storage to BQ\"\"\"\n",
    "    # create new dataset and table\n",
    "    str_time = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    dataset_id = f\"{bq.project}.docprocessing_{str_time}\"\n",
    "    table_id = f\"{dataset_id}.docprocessing_tcn\"\n",
    "    logging.info(f\"dataset_id: {dataset_id}\")\n",
    "    \n",
    "    \n",
    "    # Send the dataset to the API for creation, with an explicit timeout.\n",
    "    # Raises google.api_core.exceptions.Conflict if the Dataset already\n",
    "    # exists within the project.\n",
    "    try:\n",
    "        dataset = bigquery.Dataset(dataset_id)\n",
    "        dataset.location = \"US\"\n",
    "        bq.get_dataset(dataset_id)  # Make an API request.\n",
    "        logging.info(\"Dataset {} already exists\".format(dataset_id))\n",
    "    except Exception as e:\n",
    "        logging.info(\"Dataset {} is not found\".format(dataset_id))\n",
    "        dataset = bq.create_dataset(dataset, timeout=30)  # Make an API request.\n",
    "        dataset.location = \"US\"\n",
    "        logging.info(\"Created dataset {}.{}\".format(bq.project, dataset.dataset_id))\n",
    "    \n",
    "    # create bigquery table and upload csv\n",
    "    job_config = bigquery.LoadJobConfig(\n",
    "        schema=[\n",
    "            bigquery.SchemaField(\"file\", \"STRING\"),\n",
    "            bigquery.SchemaField(\"subject\", \"STRING\"),\n",
    "            bigquery.SchemaField(\"score\", \"FLOAT\"),\n",
    "        ],\n",
    "        skip_leading_rows=0,\n",
    "        # The source format defaults to CSV, so the line below is optional.\n",
    "        source_format=bigquery.SourceFormat.CSV,\n",
    "        allow_quoted_newlines=True,\n",
    "\n",
    "    )\n",
    "    uri = gcs_path\n",
    "\n",
    "    load_job = bq.load_table_from_uri(\n",
    "        uri, table_id, job_config=job_config\n",
    "    )  # Make an API request.\n",
    "\n",
    "    load_job.result()  # Waits for the job to complete.\n",
    "\n",
    "    destination_table = bq.get_table(table_id)  # Make an API request.\n",
    "    print(\"Loaded {} rows.\".format(destination_table.num_rows))\n",
    "\n",
    "# helper functions ---- great candidates for utils.py\n",
    "def async_detect_document(gcs_source_uri, gcs_destination_uri):\n",
    "    \"\"\"OCR with PDF/TIFF as source files on GCS\n",
    "    Link to documentation (types): https://googleapis.dev/python/vision/latest/vision_v1/types.html#google.cloud.vision_v1.types.OutputConfig\n",
    "    \"\"\"\n",
    "\n",
    "    # Supported mime_types are: 'application/pdf' and 'image/tiff'\n",
    "    mime_type = 'application/pdf'\n",
    "\n",
    "    # How many pages should be grouped into each json output file.\n",
    "    batch_size = 1\n",
    "\n",
    "    client = vision.ImageAnnotatorClient()\n",
    "\n",
    "    feature = vision.Feature(\n",
    "        type_=vision.Feature.Type.DOCUMENT_TEXT_DETECTION)\n",
    "\n",
    "    # source\n",
    "    gcs_source = vision.GcsSource(uri=gcs_source_uri)\n",
    "    input_config = vision.InputConfig(\n",
    "        gcs_source=gcs_source, mime_type=mime_type)\n",
    "    \n",
    "    # destination\n",
    "    gcs_destination = vision.GcsDestination(uri=gcs_destination_uri)\n",
    "    output_config = vision.OutputConfig(\n",
    "        gcs_destination=gcs_destination, \n",
    "        batch_size=batch_size)\n",
    "    \n",
    "\n",
    "    async_request = vision.AsyncAnnotateFileRequest(\n",
    "        features=[feature], \n",
    "        input_config=input_config,\n",
    "        output_config=output_config\n",
    "    )\n",
    "\n",
    "    operation = client.async_batch_annotate_files(\n",
    "        requests=[async_request])\n",
    "    \n",
    "def predict_text_classification_single_label_sample(\n",
    "    project: str,\n",
    "    endpoint_id: str,\n",
    "    content: str,\n",
    "    location: str = \"us-central1\",\n",
    "    api_endpoint: str = \"us-central1-aiplatform.googleapis.com\",\n",
    "):\n",
    "    # The AI Platform services require regional API endpoints.\n",
    "    client_options = {\"api_endpoint\": api_endpoint}\n",
    "    # Initialize client that will be used to create and send requests.\n",
    "    # This client only needs to be created once, and can be reused for multiple requests.\n",
    "    client = aiplatform.gapic.PredictionServiceClient(client_options=client_options)\n",
    "    instance = TextClassificationPredictionInstance(\n",
    "        content=content,\n",
    "    ).to_value()\n",
    "    instances = [instance]\n",
    "    parameters_dict = {}\n",
    "    parameters = json_format.ParseDict(parameters_dict, Value())\n",
    "    endpoint = client.endpoint_path(\n",
    "        project=project, location=location, endpoint=endpoint_id\n",
    "    )\n",
    "    response = client.predict(\n",
    "        endpoint=endpoint, instances=instances, parameters=parameters\n",
    "    )\n",
    "    print(\"response\")\n",
    "    print(\" deployed_model_id:\", response.deployed_model_id)\n",
    "    # See gs://google-cloud-aiplatform/schema/predict/prediction/text_classification.yaml for the format of the predictions.\n",
    "    predictions = response.predictions\n",
    "    for prediction in predictions:\n",
    "        print(\" prediction:\", dict(prediction))\n",
    "        \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ed0add",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0af5182",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "db31815b",
   "metadata": {},
   "source": [
    "## Testing area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "8884f400",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = \"2021_08_16_tcn_dev\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "b75915a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://2021_08_16_tcn_dev/computer_vision_1.pdf\n",
      "gs://2021_08_16_tcn_dev/med_tech_8.pdf\n",
      "gs://2021_08_16_tcn_dev/us_076.pdf\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls gs://2021_08_16_tcn_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "3858fc50",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_predict(bucket_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188ebaf7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed868a8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d495b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac6f77c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643ffac3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860d1882",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fb820a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48b5c0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4442d0aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6fb229c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-5.m75",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-5:m75"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
