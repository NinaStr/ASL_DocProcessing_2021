{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19b3f2f6",
   "metadata": {},
   "source": [
    "# Prepare pdfs for later in pipeline (Obj Det, Img, text, NER)\n",
    "\n",
    "- user provides\n",
    "    - Google Cloud project (input)\n",
    "    - bucket in GCS of pdfs (input)\n",
    "    - BQ dataset to write prediction results (output)\n",
    "        - BQ table: aggregated results (pdf_name, icn_pred, objdet_pred(coords), text_cn, ner1, ner2, ...., ner)\n",
    "            created with JOIN on pdf_name\n",
    "        - BQ table: icn_preds (pdf_name, icn_pred)    --> this table is made in icn_predict.ipynb\n",
    "        - BQ table: objdet_pred (pdf_name, objdet_pred(coords)) --> this table is made in objdet_predict.ipynb\n",
    "        - BQ table: text_cn (pdf_name, text_cn)    --> this table is made in text_cn_predict.ipynb\n",
    "        - BQ table: ner (pdf_name, ner1, ner2, ...., ner)\n",
    "        \n",
    "- see utils.py for utils functions\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8528b9",
   "metadata": {},
   "source": [
    "Steps: \n",
    " 1. convert pdf to png and write to bucket (for ICN, ObjDet)\n",
    " 2. do ocr on pdf and write to bucket \n",
    " 3. create dataset \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0608f930",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT = !gcloud config get-value project # returns SList\n",
    "PROJECT = PROJECT[0] # gets first element in list -> str\n",
    "REGION = \"us-central1\"  \n",
    "MODEL_RESOURCE_NAME = \"2393478483993952256\"\n",
    "\n",
    "import os\n",
    "os.environ[\"PROJECT\"] = PROJECT\n",
    "os.environ[\"REGION\"] = REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65fa0f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "bq = bigquery.Client(project=PROJECT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d6e1366",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "4177365a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "from google.cloud import vision\n",
    "from google.cloud import aiplatform\n",
    "import tempfile\n",
    "import traceback as tb\n",
    "from importlib import reload\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# for jupyter only\n",
    "import logging\n",
    "reload(logging)\n",
    "logging.basicConfig(format='%(asctime)s %(levelname)s:%(message)s', level=logging.DEBUG, datefmt='%I:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c8e0db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdf2image import convert_from_path\n",
    "import io\n",
    "import base64\n",
    "import cv2\n",
    "from datetime import datetime\n",
    "import time\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c5f3280",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03:03:22 INFO:test if logging works\n"
     ]
    }
   ],
   "source": [
    "logging.info(\"test if logging works\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "dc0520e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_trace_str(e):\n",
    "    return ''.join(tb.format_exception(None, e, e.__traceback__))\n",
    "\n",
    "class Utils():\n",
    "    def __init__(self):\n",
    "        self.storage_client = storage.Client()\n",
    "        \n",
    "    def dismantle_path(self, gcs_path):\n",
    "        parts = Path(gcs_path).parts\n",
    "        bucket_idx = 1 if parts[0].startswith(\"gs\") else 0\n",
    "        filename_idx = -1 if \".\" in parts[-1] else None\n",
    "\n",
    "        bucket_name = parts[bucket_idx]\n",
    "        filename = parts[filename_idx] if filename_idx else \"\"\n",
    "        directory = \"/\".join(parts[bucket_idx:filename_idx] if filename_idx else parts[bucket_idx+1:])\n",
    "        return bucket_name, directory, filename\n",
    "        \n",
    "    \n",
    "    def convert_pdf_to_png(self, src_path, dst_path):\n",
    "        \"\"\"Takes pdfs from src_bucket_name and transforms them into png. Then it saves the result in dst_bucket_name\"\"\"\n",
    "        try:\n",
    "            logging.info(\"started conversion pdf -> png\")\n",
    "        \n",
    "            src_bucket_name, src_directory, _ = self.dismantle_path(src_path)\n",
    "            dst_bucket_name, dst_directory, _ = self.dismantle_path(dst_path)\n",
    "            \n",
    "            src_bucket = self.storage_client.bucket(src_bucket_name)\n",
    "            dst_bucket = self.storage_client.bucket(dst_bucket_name)\n",
    "\n",
    "            blob_list = [blob for blob in list(src_bucket.list_blobs()) if \\\n",
    "                         os.path.basename(src_directory) in blob.name and \\\n",
    "                         blob.name.endswith(\".pdf\")]\n",
    "\n",
    "            encoded_img_lst = []\n",
    "            imgs = []\n",
    "            logging.info(f\"found {len(blob_list)} pdfs in bucket  {src_bucket_name}\")\n",
    "\n",
    "            for b_idx, blob in enumerate(blob_list):\n",
    "                _, tmp_pdf = tempfile.mkstemp()\n",
    "                blob.download_to_filename(tmp_pdf)\n",
    "                logging.info(f\"downloaded {b_idx+1} of {len(blob_list)} files\")\n",
    "                image = convert_from_path(tmp_pdf)\n",
    "                logging.info(f\"converted {b_idx+1} of {len(blob_list)} images\")\n",
    "                image = image[0]                # Only the firs page is going to be analyzed.\n",
    "                image = np.array(image)\n",
    "                is_success, im_buf_arr = cv2.imencode(\".png\", image)\n",
    "                byte_im = im_buf_arr.tobytes()\n",
    "                filename = os.path.join(dst_directory, blob.name+\".png\")\n",
    "                dst_bucket.blob(filename).upload_from_string(byte_im)\n",
    "                logging.info(f\"saved {b_idx+1} of {len(blob_list)} images with filename {filename}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in method convert_pdf_to_png: {to_trace_str(e)}\")\n",
    "            return False\n",
    "    \n",
    "    def ocr(self, src_path, dst_path):\n",
    "        \"\"\"Perform optical character recognition in pdf files.\n",
    "        \n",
    "        Args\n",
    "            src_path\n",
    "            dst_path\n",
    "        \n",
    "        Returns\n",
    "            google.api_core.operation.Operation\n",
    "            To check if done use method .done()\n",
    "            \n",
    "        Link to documentation:  \n",
    "            https://googleapis.dev/python/vision/latest/vision_v1/types.html#google.cloud.vision_v1.types.OutputConfig\n",
    "            https://cloud.google.com/vision/docs/pdf\n",
    "        \n",
    "        \"\"\"\n",
    "        try:\n",
    "            logging.info(\"started optical character recognition\")\n",
    "        \n",
    "            src_bucket_name, src_directory, _ = self.dismantle_path(src_path)\n",
    "            dst_bucket_name, dst_directory, _ = self.dismantle_path(dst_path)\n",
    "            \n",
    "            src_bucket = self.storage_client.bucket(src_bucket_name)\n",
    "            dst_bucket = self.storage_client.bucket(dst_bucket_name)\n",
    "            \n",
    "            logging.info(f\"src_bucket_name {src_bucket_name}, src_directory {src_directory}\")\n",
    "\n",
    "            blob_list = [blob for blob in list(src_bucket.list_blobs()) if \\\n",
    "                         os.path.basename(src_directory) in blob.name and \\\n",
    "                         blob.name.endswith(\".pdf\")]\n",
    "            \n",
    "            logging.info(f\"found {len(blob_list)} pdf files in bucket {src_bucket_name}\")\n",
    "\n",
    "            client = vision.ImageAnnotatorClient()\n",
    "            feature = vision.Feature(type_=vision.Feature.Type.DOCUMENT_TEXT_DETECTION)\n",
    "            \n",
    "            operations = []\n",
    "            async_requests = []\n",
    "            \n",
    "            for b_idx, blob in enumerate(blob_list):\n",
    "                gcs_source_uri = os.path.join(src_path, blob.name)\n",
    "                gcs_destination_uri = os.path.join(dst_path, blob.name)\n",
    "\n",
    "                # source\n",
    "                gcs_source = vision.GcsSource(uri=gcs_source_uri)\n",
    "                input_config = vision.InputConfig(gcs_source=gcs_source, mime_type='application/pdf')\n",
    "\n",
    "                # destination\n",
    "                gcs_destination = vision.GcsDestination(uri=gcs_destination_uri)\n",
    "                output_config = vision.OutputConfig(gcs_destination=gcs_destination, batch_size=1)\n",
    "\n",
    "                logging.info(f\"started ocr for {b_idx} of {len(blob_list)} files\")\n",
    "                async_request = vision.AsyncAnnotateFileRequest(\n",
    "                    features=[feature], \n",
    "                    input_config=input_config,\n",
    "                    output_config=output_config\n",
    "                )\n",
    "                async_requests.append(async_request)\n",
    "\n",
    "            operation = client.async_batch_annotate_files(requests=async_requests)\n",
    "            return operation\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in method ocr: {to_trace_str(e)}\")\n",
    "            \n",
    "    def get_extension(self, mime_type):\n",
    "        if mime_type == \"text/plain\":\n",
    "            return \".txt\"\n",
    "        elif mime_type == \"image/png\":\n",
    "            return \".png\"\n",
    "        else:\n",
    "            return \".txt\"\n",
    "        \n",
    "    def create_jsonl(self, gcs_path, mime_type, filename):\n",
    "        \"\"\"create jsonl out of files in bucket\n",
    "        \n",
    "        Args\n",
    "            gcs_path (str): bucket or dir where files are located\n",
    "            mime_type (str): the files mimetype \n",
    "            filename (str): the jsonl filename\n",
    "        \n",
    "        Returns\n",
    "            full path of jsonl\n",
    "        \"\"\"\n",
    "        try:\n",
    "            bucket_name, directory, _ = self.dismantle_path(gcs_path)\n",
    "            bucket = self.storage_client.bucket(bucket_name)\n",
    "            extension = self.get_extension(mime_type)\n",
    "\n",
    "            blob_list = [blob for blob in list(bucket.list_blobs()) if \\\n",
    "                             os.path.basename(directory) in blob.name and \\\n",
    "                             blob.name.endswith(extension)]\n",
    "\n",
    "            jsonl_content = \"\"\n",
    "\n",
    "            for b_idx, blob in enumerate(blob_list):\n",
    "                full_path = os.path.join(gcs_path,blob.name)\n",
    "\n",
    "                d = json.dumps(\n",
    "                    {\n",
    "                    \"content\": full_path,\n",
    "                    \"mimeType\": mime_type\n",
    "                    }\n",
    "                )+\"\\n\"\n",
    "\n",
    "                jsonl_content = jsonl_content+d\n",
    "\n",
    "\n",
    "\n",
    "            bucket.blob(filename).upload_from_string(jsonl_content)\n",
    "            logging.info(f\"uploaded jsonl {filename} to bucket {bucket_name}\")\n",
    "\n",
    "            return os.path.join(gcs_path,filename)\n",
    "        \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in jsonl creation: {to_trace_str(e)}\")\n",
    "            \n",
    "    def create_text_files(self, gcs_path):\n",
    "        try:\n",
    "            # init bucket\n",
    "            bucket_name, directory, _ = self.dismantle_path(gcs_path)\n",
    "            bucket = self.storage_client.bucket(bucket_name)\n",
    "            blob_list = [blob for blob in list(bucket.list_blobs()) if \\\n",
    "                             os.path.basename(directory) in blob.name and \\\n",
    "                             blob.name.endswith(\"output-1-to-1.json\")]\n",
    "            \n",
    "            for b_idx, blob in enumerate(blob_list):\n",
    "                logging.info(f\"creating {b_idx+1} of {len(blob_list)} text files\")\n",
    "                json_string = blob.download_as_string()\n",
    "                response = json.loads(json_string)\n",
    "                text = response['responses'][0]['fullTextAnnotation']['text'] \n",
    "                txt_path = blob.name.replace(\"output-1-to-1.json\", \".txt\")\n",
    "                text_blob = bucket.blob(txt_path)\n",
    "                text_blob.upload_from_string(text)\n",
    "                \n",
    "            logging.info(\"finished creating text files\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in method save_result_as_csv_in_storage: {to_trace_str(e)}\") \n",
    "    \n",
    "    \n",
    "    def save_to_storage(self, gcs_path, filename, predictions):\n",
    "        \"\"\"converts list of json into df, saves as temp csv file\"\"\"\n",
    "        try:\n",
    "            # init bucket\n",
    "            bucket_name, directory, _ = self.dismantle_path(gcs_path)\n",
    "            bucket = self.storage_client.bucket(bucket_name)\n",
    "\n",
    "            # create df\n",
    "            df = pd.DataFrame.from_records(predictions)\n",
    "\n",
    "            # save as tmpfile\n",
    "            _, path = tempfile.mkstemp()\n",
    "            df.to_csv(path, index=False)\n",
    "\n",
    "            # create new blob\n",
    "            blob = bucket.blob(filename)\n",
    "\n",
    "            # upload csv to blob\n",
    "            full_path = f\"{gcs_path}/{filename}\"\n",
    "            logging.info(f\"writing csv {full_path} to storage\")\n",
    "            with open(path, \"rb\") as my_file:\n",
    "                blob.upload_from_file(my_file)\n",
    "                \n",
    "            return full_path\n",
    "        \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in method save_result_as_csv_in_storage: {to_trace_str(e)}\")  \n",
    "                         \n",
    "    def load_to_bigquery(self, gcs_path, dataset_id, table_id, schema):\n",
    "        \"\"\"loads csv data in storage to BQ\"\"\"\n",
    "        # Send the dataset to the API for creation, with an explicit timeout.\n",
    "        # Raises google.api_core.exceptions.Conflict if the Dataset already\n",
    "        # exists within the project.\n",
    "        try:\n",
    "            dataset = bigquery.Dataset(dataset_id)\n",
    "            dataset.location = \"US\"\n",
    "            bq.get_dataset(dataset_id)  # Make an API request.\n",
    "            logging.info(\"Dataset {} already exists\".format(dataset_id))\n",
    "        except Exception as e:\n",
    "            logging.info(\"Dataset {} is not found\".format(dataset_id))\n",
    "            dataset = bq.create_dataset(dataset, timeout=30)  # Make an API request.\n",
    "            dataset.location = \"US\"\n",
    "            logging.info(\"Created dataset {}.{}\".format(bq.project, dataset.dataset_id))\n",
    "        finally:\n",
    "            # create bigquery table and upload csv\n",
    "            job_config = bigquery.LoadJobConfig(\n",
    "                schema=schema,\n",
    "                skip_leading_rows=1,\n",
    "                # The source format defaults to CSV, so the line below is optional.\n",
    "                source_format=bigquery.SourceFormat.CSV,\n",
    "                allow_quoted_newlines=True,\n",
    "\n",
    "            )\n",
    "            uri = gcs_path\n",
    "\n",
    "            load_job = bq.load_table_from_uri(\n",
    "                uri, table_id, job_config=job_config\n",
    "            )  # Make an API request.\n",
    "\n",
    "            load_job.result()  # Waits for the job to complete.\n",
    "\n",
    "            destination_table = bq.get_table(table_id)  # Make an API request.\n",
    "            print(\"Loaded {} rows.\".format(destination_table.num_rows))\n",
    "            \n",
    "    def run_automl_image_batch(self, project, region, model_resource_name, job_display_name, gcs_source, gcs_destination):\n",
    "        job = self.create_batch_prediction_job(\n",
    "            project, \n",
    "            region, \n",
    "            model_resource_name=model_resource_name, \n",
    "            job_display_name=job_display_name, \n",
    "            gcs_source=gcs_source, \n",
    "            gcs_destination=gcs_destination, \n",
    "            sync=True\n",
    "            )\n",
    "        \n",
    "        logging.info(f\"job started {type(job)} for automl image\")\n",
    "        \n",
    "        bucket_name, directory, _ = self.dismantle_path(gcs_destination)\n",
    "        logging.info(f\"bucket name {bucket_name}\")\n",
    "        bucket = self.storage_client.bucket(bucket_name)\n",
    "\n",
    "        # read results \n",
    "        results = []\n",
    "\n",
    "        blob_list  = [blob for blob in list(bucket.list_blobs()) if os.path.basename(gcs_destination) in blob.name and blob.name.endswith(\".jsonl\")]\n",
    "        for blob in blob_list:\n",
    "            blob_str = blob.download_as_string().decode(\"utf-8\") \n",
    "            responses = []\n",
    "            for line in blob_str.split(\"\\n\")[:-1]:\n",
    "                responses.append(json.loads(str(line)))\n",
    "\n",
    "            for response in responses:\n",
    "                \n",
    "                results.append({\n",
    "                    'file': response[\"instance\"][\"content\"][:-4], # \"gs://bucket/text.txt\" TODO: check if original path is needed\n",
    "                    'subject': response[\"prediction\"][\"displayNames\"][0],\n",
    "                    'score':  response[\"prediction\"][\"confidences\"][0],\n",
    "                    })\n",
    "\n",
    "        return results\n",
    "        \n",
    "            \n",
    "    def run_automl_text_batch(self, project, region, model_resource_name, job_display_name, gcs_source, gcs_destination):\n",
    "\n",
    "        job = self.create_batch_prediction_job(\n",
    "            project, \n",
    "            region, \n",
    "            model_resource_name=model_resource_name, \n",
    "            job_display_name=job_display_name, \n",
    "            gcs_source=gcs_source, \n",
    "            gcs_destination=gcs_destination, \n",
    "            sync=True\n",
    "            )\n",
    "\n",
    "        logging.info(f\"job started {type(job)} for automl text\")\n",
    "        \n",
    "        bucket_name, directory, _ = self.dismantle_path(gcs_destination)\n",
    "        logging.info(f\"bucket name {bucket_name}\")\n",
    "        bucket = self.storage_client.bucket(bucket_name)\n",
    "\n",
    "        # read results \n",
    "        results = []\n",
    "\n",
    "        blob_list  = [blob for blob in list(bucket.list_blobs()) if os.path.basename(gcs_destination) in blob.name and blob.name.endswith(\".jsonl\")]\n",
    "        for blob in blob_list:\n",
    "            blob_str = blob.download_as_string().decode(\"utf-8\") \n",
    "            responses = []\n",
    "            for line in blob_str.split(\"\\n\")[:-1]:\n",
    "                responses.append(json.loads(str(line)))\n",
    "\n",
    "            for response in responses:\n",
    "                results.append({\n",
    "                    'file': response[\"instance\"][\"content\"][:-4], # \"gs://bucket/text.txt\" TODO: check if original path is needed\n",
    "                    'subject': response[\"prediction\"][\"displayNames\"][0],\n",
    "                    'score':  response[\"prediction\"][\"confidences\"][0],\n",
    "                    })\n",
    "\n",
    "        return results\n",
    "\n",
    "\n",
    "    def create_batch_prediction_job(\n",
    "        self,\n",
    "        project,\n",
    "        location,\n",
    "        model_resource_name,\n",
    "        job_display_name,\n",
    "        gcs_source,\n",
    "        gcs_destination,\n",
    "        sync = True\n",
    "    ):\n",
    "        aiplatform.init(project=project, location=location)\n",
    "\n",
    "        my_model = aiplatform.Model(model_resource_name)\n",
    "\n",
    "        batch_prediction_job = my_model.batch_predict(\n",
    "            job_display_name=job_display_name,\n",
    "            gcs_source=gcs_source,\n",
    "            gcs_destination_prefix=gcs_destination,\n",
    "            sync=True\n",
    "        )\n",
    "\n",
    "        batch_prediction_job.wait()\n",
    "        \n",
    "        logging.info(f\"state type: {type(batch_prediction_job.state)}\")\n",
    "\n",
    "        logging.info(batch_prediction_job.display_name)\n",
    "        logging.info(batch_prediction_job.resource_name)\n",
    "        logging.info(batch_prediction_job.state)\n",
    "        return batch_prediction_job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "0730b9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pipeline():\n",
    "    def __init__(self, dataset_id=None):\n",
    "        self.utils = Utils()\n",
    "        self.uuid = datetime.now().strftime('%y%m%d_%H%M%S') #str\n",
    "        \n",
    "        self.project = \"qwiklabs-gcp-00-373ac55d0e0a\"\n",
    "        \n",
    "        self.region = \"us-central1\"  \n",
    "        \n",
    "        \n",
    "        self.dataset_id = dataset_id if dataset_id else f\"{self.project}.docprocessing_\"+self.uuid\n",
    "        \n",
    "        # find ids via !gcloud ai models list\n",
    "        self.tcn_model_resource_name = \"2393478483993952256\"\n",
    "        self.icn_model_resource_name = \"8925034949820547072\"\n",
    "        \n",
    "        \n",
    "        self.table_id_tcn = f\"{self.dataset_id}.tcn\" \n",
    "        self.table_id_icn = f\"{self.dataset_id}.icn\" \n",
    "        \n",
    "        self.tcn_schema = [\n",
    "                    bigquery.SchemaField(\"file\", \"STRING\", mode=\"REQUIRED\", description=\"File path.\"),\n",
    "                    bigquery.SchemaField(\"subject\", \"STRING\", mode=\"REQUIRED\", description=\"Predicted class.\"),\n",
    "                    bigquery.SchemaField(\"score\", \"FLOAT\", mode=\"REQUIRED\", description=\"Confidence of the prediction.\"),\n",
    "                ]\n",
    "        \n",
    "        self.icn_schema = [\n",
    "                    bigquery.SchemaField(\"image_name\", \"STRING\", mode=\"REQUIRED\", description='Name of the image analyzed.'),\n",
    "                    bigquery.SchemaField(\"label\", \"STRING\", mode=\"REQUIRED\", description='Predicted class. It can be US or EU'),\n",
    "                    bigquery.SchemaField(\"confidence\", \"FLOAT\", mode=\"REQUIRED\", description='Confidence of the prediction.'),\n",
    "                ]\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def start_pipeline(self, src_path):\n",
    "        # TODO: multiprocessing??\n",
    "        logging.info(f\"started pipeline\")\n",
    "        \n",
    "        # save everything in the same bucket\n",
    "        dst_path = src_path\n",
    "        jsonl_filename_tcn = f\"tcn_{self.uuid}.jsonl\"\n",
    "        jsonl_filename_icn = f\"icn_{self.uuid}.jsonl\"\n",
    "        \n",
    "        # create png\n",
    "        jsonl_path_icn = self.preprocess_pdf_to_png(src_path, dst_path, jsonl_filename_icn)\n",
    "        \n",
    "\n",
    "        # create ocr\n",
    "        jsonl_path_tcn = self.preprocess_ocr(src_path, dst_path, jsonl_filename_tcn)\n",
    "        \n",
    "        \n",
    "\n",
    "        # prediction\n",
    "        self.text_classification_task(\n",
    "            src_path=jsonl_path_tcn, \n",
    "            dst_path=dst_path, \n",
    "            job_display_name=\"job_tcn_\"+self.uuid)\n",
    "\n",
    "        self.image_classification_task(\n",
    "            src_path=jsonl_path_icn, \n",
    "            dst_path=dst_path, \n",
    "            job_display_name=\"job_icn_\"+self.uuid)\n",
    "        \n",
    "       \n",
    "        \n",
    "        logging.info(f\"finished pipelines\")\n",
    "        \n",
    "    def preprocess_pdf_to_png(self, src_path, dst_path, jsonl_filename):\n",
    "        self.utils.convert_pdf_to_png(src_path, dst_path)\n",
    "        \n",
    "        return self.utils.create_jsonl(gcs_path=dst_path, mime_type=\"image/png\", filename=jsonl_filename)\n",
    "    \n",
    "    def preprocess_ocr(self, src_path, dst_path, jsonl_filename):\n",
    "        ocr_operation = self.utils.ocr(src_path, dst_path)\n",
    "        \n",
    "        while not ocr_operation.done():\n",
    "            logging.info(\"wait for ocr to finish\")\n",
    "            time.sleep(5)\n",
    "            \n",
    "        self.utils.create_text_files(dst_path)\n",
    "        return self.utils.create_jsonl(gcs_path=dst_path, mime_type=\"text/plain\", filename=jsonl_filename)\n",
    "        \n",
    "        \n",
    "    def text_classification_task(self, src_path, dst_path, job_display_name):\n",
    "        gcs_source = src_path\n",
    "        gcs_destination = os.path.join(dst_path, job_display_name)\n",
    "        \n",
    "        logging.info(f\"starting tcn with gsc_source {gcs_source} and gcs_destination {gcs_destination}\")\n",
    "        \n",
    "        if not gcs_destination.startswith(\"gs://\"):\n",
    "            gcs_destination = \"gs://\" + gcs_destination\n",
    "        \n",
    "        predictions = self.utils.run_automl_text_batch(self.project, self.region, self.tcn_model_resource_name, job_display_name, gcs_source, gcs_destination)\n",
    "    \n",
    "        logger.info(\"save tcn predictions to storage\")\n",
    "        predictions_filename = \"predictions_tcn_\"+self.uuid+\".csv\"\n",
    "        path_to_csv = self.utils.save_to_storage(dst_path, predictions_filename, predictions)\n",
    "\n",
    "        # Step 5: Load storage result in BQ\n",
    "        logger.info(\"load results into BigQuery\")\n",
    "        status = self.utils.load_to_bigquery(path_to_csv, self.dataset_id, self.table_id_tcn, self.tcn_schema)\n",
    "        logging.info(f\"finished task with status {status}\")\n",
    "    \n",
    "    def image_classification_task(self, src_path, dst_path, job_display_name):\n",
    "        gcs_source = src_path\n",
    "        gcs_destination = os.path.join(dst_path, job_display_name)\n",
    "        \n",
    "        logging.info(f\"starting icn with gsc_source {gcs_source} and gcs_destination {gcs_destination}\")\n",
    "        \n",
    "        if not gcs_destination.startswith(\"gs://\"):\n",
    "            gcs_destination = \"gs://\" + gcs_destination\n",
    "            \n",
    "        predictions = self.utils.run_automl_image_batch(self.project, self.region, self.icn_model_resource_name, job_display_name, gcs_source, gcs_destination)\n",
    "        \n",
    "        logger.info(\"save icn predictions to storage\")\n",
    "        predictions_filename = \"predictions_icn_\"+self.uuid+\".csv\"\n",
    "        path_to_csv = self.utils.save_to_storage(dst_path, predictions_filename, predictions)\n",
    "\n",
    "        # Step 5: Load storage result in BQ\n",
    "        logger.info(\"load results into BigQuery\")\n",
    "        status = self.utils.load_to_bigquery(path_to_csv, self.dataset_id, self.table_id_icn, self.icn_schema)\n",
    "        logging.info(f\"finished task with status {status}\")\n",
    "\n",
    "    \n",
    "    def odet(self):\n",
    "        pass\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a72ae59",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be74794",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "e6e4ffd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_path = \"gs://2021_08_16_tcn_dev\"\n",
    "dst_path = \"gs://2021_08_16_tcn_dev\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "8bc2fc56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "d5b19bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(dataset_id=\"qwiklabs-gcp-00-373ac55d0e0a.docprocessing_210818_181824\")\n",
    "# pipeline.start_pipeline(src_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2db0613",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f88c7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac35f28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc69d13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc43dc2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb45a12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-5.m75",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-5:m75"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
