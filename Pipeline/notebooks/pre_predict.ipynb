{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1c9ef15",
   "metadata": {},
   "source": [
    "# Prepare pdfs for later in pipeline (Obj Det, Img, text, NER)\n",
    "\n",
    "- user provides\n",
    "    - Google Cloud project (input)\n",
    "    - bucket in GCS of pdfs (input)\n",
    "    - BQ dataset to write prediction results (output)\n",
    "        - BQ table: aggregated results (pdf_name, icn_pred, objdet_pred(coords), text_cn, ner1, ner2, ...., ner)\n",
    "            created with JOIN on pdf_name\n",
    "        - BQ table: icn_preds (pdf_name, icn_pred)    --> this table is made in icn_predict.ipynb\n",
    "        - BQ table: objdet_pred (pdf_name, objdet_pred(coords)) --> this table is made in objdet_predict.ipynb\n",
    "        - BQ table: text_cn (pdf_name, text_cn)    --> this table is made in text_cn_predict.ipynb\n",
    "        - BQ table: ner (pdf_name, ner1, ner2, ...., ner)\n",
    "        \n",
    "- see utils.py for utils functions\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc48f477",
   "metadata": {},
   "source": [
    "Steps: \n",
    " 1. convert pdf to png and write to bucket (for ICN, ObjDet)\n",
    " 2. do ocr on pdf and write to bucket \n",
    " 3. create dataset \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b19481e",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT = !gcloud config get-value project # returns SList\n",
    "PROJECT = PROJECT[0] # gets first element in list -> str\n",
    "REGION = \"us-central1\"  \n",
    "MODEL_RESOURCE_NAME = \"2393478483993952256\"\n",
    "TCN_ENDPOINT_ID = \"3651416543192940544\"\n",
    "ICN_ENDPOINT_ID = \"7257673944809865216\"\n",
    "\n",
    "import os\n",
    "os.environ[\"PROJECT\"] = PROJECT\n",
    "os.environ[\"REGION\"] = REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0799c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "bq = bigquery.Client(project=PROJECT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e437e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef013228",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "from google.cloud import vision\n",
    "from google.cloud import aiplatform\n",
    "import tempfile\n",
    "import traceback as tb\n",
    "from importlib import reload\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# for jupyter only\n",
    "import logging\n",
    "reload(logging)\n",
    "logging.basicConfig(format='%(asctime)s %(levelname)s:%(message)s', level=logging.DEBUG, datefmt='%I:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a6daf72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdf2image import convert_from_path\n",
    "import io\n",
    "import base64\n",
    "import cv2\n",
    "from datetime import datetime\n",
    "import time\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4e933c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "from google.cloud.aiplatform.gapic.schema import predict\n",
    "from google.cloud.aiplatform.v1.schema.predict.instance_v1.types import TextClassificationPredictionInstance, ImageClassificationPredictionInstance\n",
    "from google.protobuf import json_format\n",
    "from google.protobuf.struct_pb2 import Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "778fe119",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01:20:19 INFO:test if logging works\n"
     ]
    }
   ],
   "source": [
    "logging.info(\"test if logging works\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a89df27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_trace_str(e):\n",
    "    return ''.join(tb.format_exception(None, e, e.__traceback__))\n",
    "\n",
    "class Utils():\n",
    "    def __init__(self):\n",
    "        self.storage_client = storage.Client()\n",
    "        \n",
    "    def dismantle_path(self, gcs_path):\n",
    "        parts = Path(gcs_path).parts\n",
    "        bucket_idx = 1 if parts[0].startswith(\"gs\") else 0\n",
    "        filename_idx = -1 if \".\" in parts[-1] else None\n",
    "\n",
    "        bucket_name = parts[bucket_idx]\n",
    "        filename = parts[filename_idx] if filename_idx else \"\"\n",
    "        directory = \"/\".join(parts[bucket_idx+1:filename_idx] if filename_idx else parts[bucket_idx+1:])\n",
    "        return bucket_name, directory, filename\n",
    "        \n",
    "    \n",
    "    def convert_pdf_to_png(self, src_path, dst_path):\n",
    "        \"\"\"Takes pdfs from src_bucket_name and transforms them into png. Then it saves the result in dst_bucket_name\"\"\"\n",
    "        try:\n",
    "            logging.info(\"started conversion pdf -> png\")\n",
    "        \n",
    "            src_bucket_name, src_directory, _ = self.dismantle_path(src_path)\n",
    "            dst_bucket_name, dst_directory, _ = self.dismantle_path(dst_path)\n",
    "            \n",
    "            src_bucket = self.storage_client.bucket(src_bucket_name)\n",
    "            dst_bucket = self.storage_client.bucket(dst_bucket_name)\n",
    "\n",
    "            blob_list = [blob for blob in list(src_bucket.list_blobs()) if \\\n",
    "                         os.path.basename(src_directory) in blob.name and \\\n",
    "                         blob.name.endswith(\".pdf\")]\n",
    "\n",
    "            encoded_img_lst = []\n",
    "            png_lst = []\n",
    "            logging.info(f\"found {len(blob_list)} pdfs in bucket  {src_bucket_name}\")\n",
    "\n",
    "            for b_idx, blob in enumerate(blob_list):\n",
    "                _, tmp_pdf = tempfile.mkstemp()\n",
    "                blob.download_to_filename(tmp_pdf)\n",
    "                logging.info(f\"downloaded {b_idx+1} of {len(blob_list)} files\")\n",
    "                image = convert_from_path(tmp_pdf)\n",
    "                logging.info(f\"converted {b_idx+1} of {len(blob_list)} images\")\n",
    "                image = image[0]                # Only the firs page is going to be analyzed.\n",
    "                \n",
    "                image = np.array(image)\n",
    "                is_success, im_buf_arr = cv2.imencode(\".png\", image)\n",
    "                byte_im = im_buf_arr.tobytes()\n",
    "                \n",
    "                filename = os.path.join(dst_directory, blob.name+\".png\")\n",
    "                dst_bucket.blob(filename).upload_from_string(byte_im)\n",
    "                \n",
    "                png_lst.append({\"content\": f\"gs://{dst_bucket_name}/{filename}\", \"image\": byte_im})\n",
    "                \n",
    "                logging.info(f\"saved {b_idx+1} of {len(blob_list)} images with filename {filename}\")\n",
    "                \n",
    "            return png_lst\n",
    "        \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in method convert_pdf_to_png: {to_trace_str(e)}\")\n",
    "            return False\n",
    "    \n",
    "    def ocr(self, src_path, dst_path):\n",
    "        \"\"\"Perform optical character recognition in pdf files.\n",
    "        \n",
    "        Args\n",
    "            src_path\n",
    "            dst_path\n",
    "        \n",
    "        Returns\n",
    "            google.api_core.operation.Operation\n",
    "            To check if done use method .done()\n",
    "            \n",
    "        Link to documentation:  \n",
    "            https://googleapis.dev/python/vision/latest/vision_v1/types.html#google.cloud.vision_v1.types.OutputConfig\n",
    "            https://cloud.google.com/vision/docs/pdf\n",
    "        \n",
    "        \"\"\"\n",
    "        try:\n",
    "            logging.info(\"started optical character recognition\")\n",
    "        \n",
    "            src_bucket_name, src_directory, _ = self.dismantle_path(src_path)\n",
    "            dst_bucket_name, dst_directory, _ = self.dismantle_path(dst_path)\n",
    "            \n",
    "            src_bucket = self.storage_client.bucket(src_bucket_name)\n",
    "            dst_bucket = self.storage_client.bucket(dst_bucket_name)\n",
    "            \n",
    "            logging.info(f\"src_bucket_name {src_bucket_name}, src_directory {src_directory}\")\n",
    "\n",
    "            blob_list = [blob for blob in list(src_bucket.list_blobs()) if \\\n",
    "                         os.path.basename(src_directory) in blob.name and \\\n",
    "                         blob.name.endswith(\".pdf\")]\n",
    "            \n",
    "            logging.info(f\"found {len(blob_list)} pdf files in bucket {src_bucket_name}\")\n",
    "\n",
    "            client = vision.ImageAnnotatorClient()\n",
    "            feature = vision.Feature(type_=vision.Feature.Type.DOCUMENT_TEXT_DETECTION)\n",
    "            \n",
    "            operations = []\n",
    "            async_requests = []\n",
    "            \n",
    "            for b_idx, blob in enumerate(blob_list):\n",
    "                gcs_source_uri = \"gs://\" + os.path.join(src_bucket_name, blob.name)\n",
    "                gcs_destination_uri = \"gs://\" +  os.path.join(dst_bucket_name, blob.name)\n",
    "\n",
    "                # source\n",
    "                gcs_source = vision.GcsSource(uri=gcs_source_uri)\n",
    "                input_config = vision.InputConfig(gcs_source=gcs_source, mime_type='application/pdf')\n",
    "\n",
    "                # destination\n",
    "                gcs_destination = vision.GcsDestination(uri=gcs_destination_uri)\n",
    "                output_config = vision.OutputConfig(gcs_destination=gcs_destination, batch_size=1)\n",
    "\n",
    "                logging.info(f\"started ocr for {b_idx} of {len(blob_list)} files\")\n",
    "                async_request = vision.AsyncAnnotateFileRequest(\n",
    "                    features=[feature], \n",
    "                    input_config=input_config,\n",
    "                    output_config=output_config\n",
    "                )\n",
    "                async_requests.append(async_request)\n",
    "\n",
    "            operation = client.async_batch_annotate_files(requests=async_requests)\n",
    "            return operation\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in method ocr: {to_trace_str(e)}\")\n",
    "            \n",
    "    def get_extension(self, mime_type):\n",
    "        if mime_type == \"text/plain\":\n",
    "            return \".txt\"\n",
    "        elif mime_type == \"image/png\":\n",
    "            return \".png\"\n",
    "        else:\n",
    "            return \".txt\"\n",
    "        \n",
    "    def read_text_files(self, list_of_paths):\n",
    "        bucket_name, _ , _ = self.dismantle_path(list_of_paths[0][\"content\"])\n",
    "        bucket = self.storage_client.bucket(bucket_name)\n",
    "        \n",
    "        for idx, element in enumerate(list_of_paths):\n",
    "            path = element[\"content\"]\n",
    "            _, directory, filename = self.dismantle_path(path)\n",
    "            text = bucket.blob(os.path.join(directory, filename)).download_as_string()\n",
    "            list_of_paths[idx][\"text\"] = text\n",
    "            \n",
    "        return list_of_paths    \n",
    "\n",
    "    \n",
    "    def read_jsonl(self, gcs_path):\n",
    "        \"\"\"create jsonl out of files in bucket\n",
    "        \n",
    "        Args\n",
    "            gcs_path (str): bucket or dir where file is located\n",
    "            \n",
    "        Returns\n",
    "            results (list): list of dicts\n",
    "        \"\"\"\n",
    "        try:\n",
    "            bucket_name, directory, filename = utils.dismantle_path(gcs_path)\n",
    "            blob_name = os.path.join(directory, filename)\n",
    "            logging.info(blob_name)\n",
    "            bucket = utils.storage_client.bucket(bucket_name)\n",
    "            blob = bucket.blob(blob_name)\n",
    "            results = []\n",
    "            response = blob.download_as_string().decode(\"utf-8\")\n",
    "            logging.info(response)\n",
    "            for line in response.split(\"\\n\")[:-1]:\n",
    "                results.append(json.loads(line))\n",
    "                \n",
    "            return results\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in read_jsonl: {e}\")\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def create_jsonl(self, gcs_path, mime_type, filename):\n",
    "        \"\"\"create jsonl out of files in bucket\n",
    "        \n",
    "        Args\n",
    "            gcs_path (str): bucket or dir where files are located\n",
    "            mime_type (str): the files mimetype \n",
    "            filename (str): the jsonl filename\n",
    "        \n",
    "        Returns\n",
    "            full path of jsonl\n",
    "        \"\"\"\n",
    "        try:\n",
    "            filename = os.path.basename(filename)\n",
    "            bucket_name, directory, _ = self.dismantle_path(gcs_path)\n",
    "            bucket = self.storage_client.bucket(bucket_name)\n",
    "            extension = self.get_extension(mime_type)\n",
    "\n",
    "            blob_list = [blob for blob in list(bucket.list_blobs()) if \\\n",
    "                             os.path.basename(directory) in blob.name and \\\n",
    "                             blob.name.endswith(extension)]\n",
    "\n",
    "            jsonl_content = \"\"\n",
    "\n",
    "            for b_idx, blob in enumerate(blob_list):\n",
    "                full_path = os.path.join(gcs_path,blob.name)\n",
    "\n",
    "                d = json.dumps(\n",
    "                    {\n",
    "                    \"content\": full_path,\n",
    "                    \"mimeType\": mime_type\n",
    "                    }\n",
    "                )+\"\\n\"\n",
    "\n",
    "                jsonl_content = jsonl_content+d\n",
    "\n",
    "\n",
    "\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            bucket.blob(file_path).upload_from_string(jsonl_content)\n",
    "            logging.info(f\"uploaded jsonl {file_path} to bucket {bucket_name}. Full path: gs://{os.path.join(bucket_name,file_path)}\")\n",
    "\n",
    "        \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in jsonl creation: {to_trace_str(e)}\")\n",
    "            \n",
    "    def create_text_files(self, gcs_path):\n",
    "        \n",
    "        results = []\n",
    "        try:\n",
    "            # init bucket\n",
    "            bucket_name, directory, _ = self.dismantle_path(gcs_path)\n",
    "            bucket = self.storage_client.bucket(bucket_name)\n",
    "            blob_list = [blob for blob in list(bucket.list_blobs()) if \\\n",
    "                             os.path.basename(directory) in blob.name and \\\n",
    "                             blob.name.endswith(\"output-1-to-1.json\")]\n",
    "            \n",
    "            for b_idx, blob in enumerate(blob_list):\n",
    "                logging.info(f\"creating {b_idx+1} of {len(blob_list)} text files\")\n",
    "                json_string = blob.download_as_string()\n",
    "                response = json.loads(json_string)\n",
    "                text = response['responses'][0]['fullTextAnnotation']['text'] \n",
    "                txt_path = blob.name.replace(\"output-1-to-1.json\", \".txt\")\n",
    "                text_blob = bucket.blob(txt_path)\n",
    "                results.append({\"content\":f\"gs://{bucket_name}/{txt_path}\", \"text\":text})\n",
    "                text_blob.upload_from_string(text)\n",
    "                logging.info(f\"created text file gs://{bucket_name}/{txt_path}\")\n",
    "                \n",
    "            logging.info(\"finished creating text files\")\n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in method save_result_as_csv_in_storage: {to_trace_str(e)}\") \n",
    "    \n",
    "    \n",
    "    def save_to_storage(self, gcs_path, filename, predictions):\n",
    "        \"\"\"converts list of json into df, saves as temp csv file\"\"\"\n",
    "        try:\n",
    "            # init bucket\n",
    "            bucket_name, directory, _ = self.dismantle_path(gcs_path)\n",
    "            bucket = self.storage_client.bucket(bucket_name)\n",
    "\n",
    "            # create df\n",
    "            df = pd.DataFrame.from_records(predictions)\n",
    "\n",
    "            # save as tmpfile\n",
    "            _, path = tempfile.mkstemp()\n",
    "            df.to_csv(path, index=False)\n",
    "\n",
    "            # create new blob\n",
    "            blob = bucket.blob(filename)\n",
    "\n",
    "            # upload csv to blob\n",
    "            full_path = f\"{gcs_path}/{filename}\"\n",
    "            logging.info(f\"writing csv {full_path} to storage\")\n",
    "            with open(path, \"rb\") as my_file:\n",
    "                blob.upload_from_file(my_file)\n",
    "                \n",
    "            return full_path\n",
    "        \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in method save_result_as_csv_in_storage: {to_trace_str(e)}\")  \n",
    "                         \n",
    "    def load_to_bigquery(self, gcs_path, dataset_id, table_id, schema):\n",
    "        \"\"\"loads csv data in storage to BQ\"\"\"\n",
    "        # Send the dataset to the API for creation, with an explicit timeout.\n",
    "        # Raises google.api_core.exceptions.Conflict if the Dataset already\n",
    "        # exists within the project.\n",
    "        try:\n",
    "            dataset = bigquery.Dataset(dataset_id)\n",
    "            dataset.location = \"US\"\n",
    "            bq.get_dataset(dataset_id)  # Make an API request.\n",
    "            logging.info(\"Dataset {} already exists\".format(dataset_id))\n",
    "        except Exception as e:\n",
    "            logging.info(\"Dataset {} is not found\".format(dataset_id))\n",
    "            dataset = bq.create_dataset(dataset, timeout=30)  # Make an API request.\n",
    "            dataset.location = \"US\"\n",
    "            logging.info(\"Created dataset {}.{}\".format(bq.project, dataset.dataset_id))\n",
    "        finally:\n",
    "            # create bigquery table and upload csv\n",
    "            job_config = bigquery.LoadJobConfig(\n",
    "                schema=schema,\n",
    "                skip_leading_rows=1,\n",
    "                # The source format defaults to CSV, so the line below is optional.\n",
    "                source_format=bigquery.SourceFormat.CSV,\n",
    "                allow_quoted_newlines=True,\n",
    "\n",
    "            )\n",
    "            uri = gcs_path\n",
    "\n",
    "            load_job = bq.load_table_from_uri(\n",
    "                uri, table_id, job_config=job_config\n",
    "            )  # Make an API request.\n",
    "\n",
    "            load_job.result()  # Waits for the job to complete.\n",
    "\n",
    "            destination_table = bq.get_table(table_id)  # Make an API request.\n",
    "            logging.info(\"Loaded {} rows.\".format(destination_table.num_rows))\n",
    "            \n",
    "    \n",
    "    # image\n",
    "    def predict_online_multiple_image(\n",
    "        self,\n",
    "        project: str,\n",
    "        endpoint_id: str,\n",
    "        img_lst: list,\n",
    "        location: str = \"us-central1\",\n",
    "        api_endpoint: str = \"us-central1-aiplatform.googleapis.com\"\n",
    "    ):\n",
    "        \n",
    "        \n",
    "        # The AI Platform services require regional API endpoints.\n",
    "        client_options = {\"api_endpoint\": api_endpoint}\n",
    "        # Initialize client that will be used to create and send requests.\n",
    "        # This client only needs to be created once, and can be reused for multiple requests.\n",
    "        client = aiplatform.gapic.PredictionServiceClient(client_options=client_options)\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        for item in img_lst:\n",
    "            # The format of each instance should conform to the deployed model's prediction input schema.\n",
    "            encoded_content = base64.b64encode(item[\"image\"]).decode(\"utf-8\")\n",
    "            instance = predict.instance.ImageClassificationPredictionInstance(\n",
    "                content=encoded_content,\n",
    "            ).to_value()\n",
    "            instances = [instance]\n",
    "            # See gs://google-cloud-aiplatform/schema/predict/params/image_classification_1.0.0.yaml for the format of the parameters.\n",
    "            parameters = predict.params.ImageClassificationPredictionParams(\n",
    "                confidence_threshold=0.5, max_predictions=5,\n",
    "            ).to_value()\n",
    "            endpoint = client.endpoint_path(\n",
    "                project=project, location=location, endpoint=endpoint_id\n",
    "            )\n",
    "            response = client.predict(\n",
    "                endpoint=endpoint, instances=instances, parameters=parameters\n",
    "            )\n",
    "\n",
    "            for prediction_ in response.predictions:\n",
    "                max_value = max(prediction_[\"confidences\"])\n",
    "                max_index = prediction_[\"confidences\"].index(max_value)\n",
    "                \n",
    "                results.append({\n",
    "                    'file': item[\"content\"], # \"gs://bucket/text.txt\" TODO: check if original path is needed\n",
    "                    'subject': prediction_[\"displayNames\"][max_index],\n",
    "                    'score':  prediction_[\"confidences\"][max_index],\n",
    "                })\n",
    "        # See gs://google-cloud-aiplatform/schema/predict/prediction/text_classification.yaml for the format of the predictions.\n",
    "\n",
    "        \n",
    "        return results\n",
    "    # text\n",
    "    def predict_online_multiple(\n",
    "        self,\n",
    "        project: str,\n",
    "        endpoint_id: str,\n",
    "        content_lst: list,\n",
    "        location: str = \"us-central1\",\n",
    "        api_endpoint: str = \"us-central1-aiplatform.googleapis.com\"\n",
    "    ):\n",
    "        \n",
    "        \n",
    "        aiplatform.init(project=project, location=location)\n",
    "        endpoint = aiplatform.Endpoint(endpoint_id)\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        for item in content_lst:\n",
    "            response = endpoint.predict(instances=[{\"content\": item[\"text\"]}], parameters={})\n",
    "\n",
    "            for prediction_ in response.predictions:\n",
    "                max_value = max(prediction_[\"confidences\"])\n",
    "                max_index = prediction_[\"confidences\"].index(max_value)\n",
    "                \n",
    "                results.append({\n",
    "                    'file': item[\"content\"], # \"gs://bucket/text.txt\" TODO: check if original path is needed\n",
    "                    'subject': prediction_[\"displayNames\"][max_index],\n",
    "                    'score':  prediction_[\"confidences\"][max_index],\n",
    "                })\n",
    "        # See gs://google-cloud-aiplatform/schema/predict/prediction/text_classification.yaml for the format of the predictions.\n",
    "\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be1846cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pipeline():\n",
    "    def __init__(self, dataset_id=None):\n",
    "        self.utils = Utils()\n",
    "        self.uuid = datetime.now().strftime('%y%m%d_%H%M%S') #str\n",
    "        \n",
    "        self.project = \"qwiklabs-gcp-00-373ac55d0e0a\"\n",
    "        \n",
    "        self.region = \"us-central1\"  \n",
    "        \n",
    "        \n",
    "        self.dataset_id = dataset_id if dataset_id else f\"{self.project}.docprocessing_\"+self.uuid\n",
    "        \n",
    "        # find ids via !gcloud ai models list\n",
    "        self.tcn_model_resource_name = \"2393478483993952256\"\n",
    "        self.icn_model_resource_name = \"8925034949820547072\"\n",
    "        \n",
    "        \n",
    "        self.table_id_tcn = f\"{self.dataset_id}.tcn\" \n",
    "        self.table_id_icn = f\"{self.dataset_id}.icn\" \n",
    "        \n",
    "        self.tcn_schema = [\n",
    "                    bigquery.SchemaField(\"file\", \"STRING\", mode=\"REQUIRED\", description=\"File path.\"),\n",
    "                    bigquery.SchemaField(\"subject\", \"STRING\", mode=\"REQUIRED\", description=\"Predicted class.\"),\n",
    "                    bigquery.SchemaField(\"score\", \"FLOAT\", mode=\"REQUIRED\", description=\"Confidence of the prediction.\"),\n",
    "                ]\n",
    "        \n",
    "        self.icn_schema = [\n",
    "                    bigquery.SchemaField(\"image_name\", \"STRING\", mode=\"REQUIRED\", description='Name of the image analyzed.'),\n",
    "                    bigquery.SchemaField(\"label\", \"STRING\", mode=\"REQUIRED\", description='Predicted class. It can be US or EU'),\n",
    "                    bigquery.SchemaField(\"confidence\", \"FLOAT\", mode=\"REQUIRED\", description='Confidence of the prediction.'),\n",
    "                ]\n",
    "        \n",
    "\n",
    "    def start_pipeline(self, src_path):\n",
    "        logging.info(f\"started pipeline\")\n",
    "        \n",
    "        # save everything in the same bucket\n",
    "        dst_path = src_path\n",
    "        jsonl_filename_tcn = f\"tcn_{self.uuid}.jsonl\"\n",
    "        jsonl_filename_icn = f\"icn_{self.uuid}.jsonl\"\n",
    "        \n",
    "        # create png\n",
    "        jsonl_path_icn, image_lst_icn  = self.preprocess_pdf_to_png(src_path, dst_path, jsonl_filename_icn)\n",
    "        \n",
    "        # create ocr\n",
    "        jsonl_path_tcn, text_lst_tcn = self.preprocess_ocr(src_path, dst_path, jsonl_filename_tcn)\n",
    "        \n",
    "        # prediction\n",
    "        self.text_classification_task(text_lst=text_lst_tcn)\n",
    "\n",
    "        self.image_classification_task(img_lst=image_lst_icn)\n",
    "        \n",
    "        logging.info(f\"finished pipelines\")\n",
    "        \n",
    "    def preprocess_pdf_to_png(self, src_path, dst_path, jsonl_filename):\n",
    "        png_lst = self.utils.convert_pdf_to_png(src_path, dst_path)\n",
    "        \n",
    "        return self.utils.create_jsonl(gcs_path=dst_path, mime_type=\"image/png\", filename=jsonl_filename), png_lst\n",
    "    \n",
    "    def preprocess_ocr(self, src_path, dst_path, jsonl_filename):\n",
    "        ocr_operation = self.utils.ocr(src_path, dst_path)\n",
    "        \n",
    "        while not ocr_operation.done():\n",
    "            logging.info(\"wait for ocr to finish\")\n",
    "            time.sleep(5)\n",
    "            \n",
    "        text_lst = self.utils.create_text_files(dst_path) # returns list of json\n",
    "        return self.utils.create_jsonl(gcs_path=dst_path, mime_type=\"text/plain\", filename=jsonl_filename), text_lst\n",
    "        \n",
    "        \n",
    "    def text_classification_task(self, text_lst):\n",
    "\n",
    "        predictions = self.utils.predict_online_multiple(\n",
    "            project=PROJECT,\n",
    "            endpoint_id=TCN_ENDPOINT_ID,\n",
    "            content_lst=text_lst,\n",
    "            location=\"us-central1\",\n",
    "            api_endpoint=\"us-central1-aiplatform.googleapis.com\"\n",
    "        )\n",
    "\n",
    "    \n",
    "        logger.info(\"save tcn predictions to storage\")\n",
    "        predictions_filename = \"predictions_tcn_\"+self.uuid+\".csv\"\n",
    "        path_to_csv = self.utils.save_to_storage(dst_path, predictions_filename, predictions)\n",
    "\n",
    "#         # Step 5: Load storage result in BQ\n",
    "        logger.info(\"load results into BigQuery\")\n",
    "        status = self.utils.load_to_bigquery(path_to_csv, self.dataset_id, self.table_id_tcn, self.tcn_schema)\n",
    "        logging.info(f\"finished task with status {status}\")\n",
    "    \n",
    "    def image_classification_task(self, img_lst):\n",
    "        \n",
    "        predictions = self.utils.predict_online_multiple_image(\n",
    "            project=PROJECT,\n",
    "            endpoint_id=ICN_ENDPOINT_ID,\n",
    "            img_lst=img_lst,\n",
    "            location=\"us-central1\",\n",
    "            api_endpoint=\"us-central1-aiplatform.googleapis.com\"\n",
    "        )\n",
    "        \n",
    "        logger.info(\"save icn predictions to storage\")\n",
    "        predictions_filename = \"predictions_icn_\"+self.uuid+\".csv\"\n",
    "        path_to_csv = self.utils.save_to_storage(dst_path, predictions_filename, predictions)\n",
    "\n",
    "        # Step 5: Load storage result in BQ\n",
    "        logger.info(\"load results into BigQuery\")\n",
    "        status = self.utils.load_to_bigquery(path_to_csv, self.dataset_id, self.table_id_icn, self.icn_schema)\n",
    "        logging.info(f\"finished task with status {status}\")\n",
    "\n",
    "    \n",
    "    def odet(self):\n",
    "        pass\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f275c84f",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dcb713a",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c60b8db9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01:20:19 INFO:started pipeline\n",
      "01:20:19 INFO:started conversion pdf -> png\n",
      "01:20:19 INFO:found 3 pdfs in bucket  2021_08_16_tcn_dev\n",
      "01:20:19 INFO:downloaded 1 of 3 files\n",
      "01:20:19 INFO:converted 1 of 3 images\n",
      "01:20:19 INFO:saved 1 of 3 images with filename computer_vision_1.pdf.png\n",
      "01:20:19 INFO:downloaded 2 of 3 files\n",
      "01:20:20 INFO:converted 2 of 3 images\n",
      "01:20:20 INFO:saved 2 of 3 images with filename med_tech_8.pdf.png\n",
      "01:20:20 INFO:downloaded 3 of 3 files\n",
      "01:20:20 INFO:converted 3 of 3 images\n",
      "01:20:21 INFO:saved 3 of 3 images with filename us_076.pdf.png\n",
      "01:20:21 INFO:uploaded jsonl icn_210820_132019.jsonl to bucket 2021_08_16_tcn_dev. Full path: gs://2021_08_16_tcn_dev/icn_210820_132019.jsonl\n",
      "01:20:21 INFO:started optical character recognition\n",
      "01:20:21 INFO:src_bucket_name 2021_08_16_tcn_dev, src_directory \n",
      "01:20:21 INFO:found 3 pdf files in bucket 2021_08_16_tcn_dev\n",
      "01:20:21 INFO:started ocr for 0 of 3 files\n",
      "01:20:21 INFO:started ocr for 1 of 3 files\n",
      "01:20:21 INFO:started ocr for 2 of 3 files\n",
      "01:20:21 INFO:wait for ocr to finish\n",
      "01:20:26 INFO:wait for ocr to finish\n",
      "01:20:31 INFO:wait for ocr to finish\n",
      "01:20:36 INFO:wait for ocr to finish\n",
      "01:20:41 INFO:wait for ocr to finish\n",
      "01:20:46 INFO:creating 1 of 3 text files\n",
      "01:20:46 INFO:created text file gs://2021_08_16_tcn_dev/computer_vision_1.pdf.txt\n",
      "01:20:46 INFO:creating 2 of 3 text files\n",
      "01:20:47 INFO:created text file gs://2021_08_16_tcn_dev/med_tech_8.pdf.txt\n",
      "01:20:47 INFO:creating 3 of 3 text files\n",
      "01:20:47 INFO:created text file gs://2021_08_16_tcn_dev/us_076.pdf.txt\n",
      "01:20:47 INFO:finished creating text files\n",
      "01:20:47 INFO:uploaded jsonl tcn_210820_132019.jsonl to bucket 2021_08_16_tcn_dev. Full path: gs://2021_08_16_tcn_dev/tcn_210820_132019.jsonl\n",
      "INFO:__main__:save tcn predictions to storage\n",
      "01:20:48 INFO:writing csv gs://2021_08_16_tcn_dev/predictions_tcn_210820_132019.csv to storage\n",
      "INFO:__main__:load results into BigQuery\n",
      "01:20:48 INFO:Dataset qwiklabs-gcp-00-373ac55d0e0a.docprocessing_demo_nina already exists\n",
      "01:20:54 INFO:Loaded 3 rows.\n",
      "01:20:54 INFO:finished task with status None\n",
      "INFO:__main__:save icn predictions to storage\n",
      "01:20:55 INFO:writing csv gs://2021_08_16_tcn_dev/predictions_icn_210820_132019.csv to storage\n",
      "INFO:__main__:load results into BigQuery\n",
      "01:20:56 INFO:Dataset qwiklabs-gcp-00-373ac55d0e0a.docprocessing_demo_nina already exists\n",
      "01:21:00 INFO:Loaded 3 rows.\n",
      "01:21:00 INFO:finished task with status None\n",
      "01:21:00 INFO:finished pipelines\n"
     ]
    }
   ],
   "source": [
    "src_path = \"gs://2021_08_16_tcn_dev\"\n",
    "dst_path = \"gs://2021_08_16_tcn_dev\"\n",
    "\n",
    "pipeline = Pipeline(dataset_id=\"qwiklabs-gcp-00-373ac55d0e0a.docprocessing_demo_nina\")\n",
    "pipeline.start_pipeline(src_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9cd9e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392dcc3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a34fae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f616cbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline.text_classification_task([\n",
    "#     {\"text\":\"this is an example\", \"content\":\"gs://examplepath\"},\n",
    "#     {\"text\":\"this is another example\", \"content\":\"gs://examplepath\"},\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddba52b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8a6f96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f399ff8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8e1d23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b1314e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-5.m75",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-5:m75"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
