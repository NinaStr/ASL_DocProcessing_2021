{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "385dad31",
   "metadata": {},
   "source": [
    "# Prepare pdfs for later in pipeline (Obj Det, Img, text, NER)\n",
    "\n",
    "- user provides\n",
    "    - Google Cloud project (input)\n",
    "    - bucket in GCS of pdfs (input)\n",
    "    - BQ dataset to write prediction results (output)\n",
    "        - BQ table: aggregated results (pdf_name, icn_pred, objdet_pred(coords), text_cn, ner1, ner2, ...., ner)\n",
    "            created with JOIN on pdf_name\n",
    "        - BQ table: icn_preds (pdf_name, icn_pred)    --> this table is made in icn_predict.ipynb\n",
    "        - BQ table: objdet_pred (pdf_name, objdet_pred(coords)) --> this table is made in objdet_predict.ipynb\n",
    "        - BQ table: text_cn (pdf_name, text_cn)    --> this table is made in text_cn_predict.ipynb\n",
    "        - BQ table: ner (pdf_name, ner1, ner2, ...., ner)\n",
    "        \n",
    "- see utils.py for utils functions\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0289e504",
   "metadata": {},
   "source": [
    "Steps: \n",
    " 1. convert pdf to png and write to bucket (for ICN, ObjDet)\n",
    " 2. do ocr on pdf and write to bucket \n",
    " 3. create dataset \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e78f48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT = !gcloud config get-value project # returns SList\n",
    "PROJECT = PROJECT[0] # gets first element in list -> str\n",
    "REGION = \"us-central1\"  \n",
    "MODEL_RESOURCE_NAME = \"2393478483993952256\"\n",
    "\n",
    "import os\n",
    "os.environ[\"PROJECT\"] = PROJECT\n",
    "os.environ[\"REGION\"] = REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e27141c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "bq = bigquery.Client(project=PROJECT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d05f2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "5ce11900",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "from google.cloud import vision\n",
    "from google.cloud import aiplatform\n",
    "import tempfile\n",
    "\n",
    "\n",
    "\n",
    "from importlib import reload\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# for jupyter only\n",
    "import logging\n",
    "reload(logging)\n",
    "logging.basicConfig(format='%(asctime)s %(levelname)s:%(message)s', level=logging.DEBUG, datefmt='%I:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43c06c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdf2image import convert_from_path\n",
    "import io\n",
    "import base64\n",
    "import cv2\n",
    "from datetime import datetime\n",
    "import time\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb75e5e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03:03:22 INFO:test if logging works\n"
     ]
    }
   ],
   "source": [
    "logging.info(\"test if logging works\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "9880345f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_trace_str(e):\n",
    "    return ''.join(tb.format_exception(None, e, e.__traceback__))\n",
    "\n",
    "class Utils():\n",
    "    def __init__(self):\n",
    "        self.storage_client = storage.Client()\n",
    "        \n",
    "    def dismantle_path(self, gcs_path):\n",
    "        parts = Path(gcs_path).parts\n",
    "        bucket_idx = 1 if parts[0].startswith(\"gs\") else 0\n",
    "        filename_idx = -1 if \".\" in parts[-1] else None\n",
    "\n",
    "        bucket_name = parts[bucket_idx]\n",
    "        filename = parts[filename_idx] if filename_idx else \"\"\n",
    "        directory = \"/\".join(parts[bucket_idx:filename_idx] if filename_idx else parts[bucket_idx+1:])\n",
    "        return bucket_name, directory, filename\n",
    "        \n",
    "    \n",
    "    def convert_pdf_to_png(self, src_path, dst_path):\n",
    "        \"\"\"Takes pdfs from src_bucket_name and transforms them into png. Then it saves the result in dst_bucket_name\"\"\"\n",
    "        try:\n",
    "            logging.info(\"started conversion pdf -> png\")\n",
    "        \n",
    "            src_bucket_name, src_directory, _ = self.dismantle_path(src_path)\n",
    "            dst_bucket_name, dst_directory, _ = self.dismantle_path(dst_path)\n",
    "            \n",
    "            src_bucket = self.storage_client.bucket(src_bucket_name)\n",
    "            dst_bucket = self.storage_client.bucket(dst_bucket_name)\n",
    "\n",
    "            blob_list = [blob for blob in list(src_bucket.list_blobs()) if \\\n",
    "                         os.path.basename(src_directory) in blob.name and \\\n",
    "                         blob.name.endswith(\".pdf\")]\n",
    "\n",
    "            encoded_img_lst = []\n",
    "            imgs = []\n",
    "            logging.info(f\"found {len(blob_list)} pdfs in bucket  {src_bucket_name}\")\n",
    "\n",
    "            for b_idx, blob in enumerate(blob_list):\n",
    "                _, tmp_pdf = tempfile.mkstemp()\n",
    "                blob.download_to_filename(tmp_pdf)\n",
    "                logging.info(f\"downloaded {b_idx+1} of {len(blob_list)} files\")\n",
    "                image = convert_from_path(tmp_pdf)\n",
    "                logging.info(f\"converted {b_idx+1} of {len(blob_list)} images\")\n",
    "                image = image[0]                # Only the firs page is going to be analyzed.\n",
    "                image = np.array(image)\n",
    "                is_success, im_buf_arr = cv2.imencode(\".png\", image)\n",
    "                byte_im = im_buf_arr.tobytes()\n",
    "                filename = os.path.join(dst_directory, blob.name+\".png\")\n",
    "                dst_bucket.blob(filename).upload_from_string(byte_im)\n",
    "                logging.info(f\"saved {b_idx+1} of {len(blob_list)} images with filename {filename}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in method convert_pdf_to_png: {to_trace_str(e)}\")\n",
    "            return False\n",
    "    \n",
    "    def ocr(self, src_path, dst_path):\n",
    "        \"\"\"Perform optical character recognition in pdf files.\n",
    "        \n",
    "        Args\n",
    "            src_path\n",
    "            dst_path\n",
    "        \n",
    "        Returns\n",
    "            google.api_core.operation.Operation\n",
    "            To check if done use method .done()\n",
    "            \n",
    "        Link to documentation:  \n",
    "            https://googleapis.dev/python/vision/latest/vision_v1/types.html#google.cloud.vision_v1.types.OutputConfig\n",
    "            https://cloud.google.com/vision/docs/pdf\n",
    "        \n",
    "        \"\"\"\n",
    "        try:\n",
    "            logging.info(\"started optical character recognition\")\n",
    "        \n",
    "            src_bucket_name, src_directory, _ = self.dismantle_path(src_path)\n",
    "            dst_bucket_name, dst_directory, _ = self.dismantle_path(dst_path)\n",
    "            \n",
    "            src_bucket = self.storage_client.bucket(src_bucket_name)\n",
    "            dst_bucket = self.storage_client.bucket(dst_bucket_name)\n",
    "            \n",
    "            logging.info(f\"src_bucket_name {src_bucket_name}, src_directory {src_directory}\")\n",
    "\n",
    "            blob_list = [blob for blob in list(src_bucket.list_blobs()) if \\\n",
    "                         os.path.basename(src_directory) in blob.name and \\\n",
    "                         blob.name.endswith(\".pdf\")]\n",
    "            \n",
    "            logging.info(f\"found {len(blob_list)} pdf files in bucket {src_bucket_name}\")\n",
    "\n",
    "            client = vision.ImageAnnotatorClient()\n",
    "            feature = vision.Feature(type_=vision.Feature.Type.DOCUMENT_TEXT_DETECTION)\n",
    "            \n",
    "            operations = []\n",
    "            async_requests = []\n",
    "            \n",
    "            for b_idx, blob in enumerate(blob_list):\n",
    "                gcs_source_uri = os.path.join(src_path, blob.name)\n",
    "                gcs_destination_uri = os.path.join(dst_path, blob.name)\n",
    "\n",
    "                # source\n",
    "                gcs_source = vision.GcsSource(uri=gcs_source_uri)\n",
    "                input_config = vision.InputConfig(gcs_source=gcs_source, mime_type='application/pdf')\n",
    "\n",
    "                # destination\n",
    "                gcs_destination = vision.GcsDestination(uri=gcs_destination_uri)\n",
    "                output_config = vision.OutputConfig(gcs_destination=gcs_destination, batch_size=1)\n",
    "\n",
    "                logging.info(f\"started ocr for {b_idx} of {len(blob_list)} files\")\n",
    "                async_request = vision.AsyncAnnotateFileRequest(\n",
    "                    features=[feature], \n",
    "                    input_config=input_config,\n",
    "                    output_config=output_config\n",
    "                )\n",
    "                async_requests.append(async_request)\n",
    "\n",
    "            operation = client.async_batch_annotate_files(requests=async_requests)\n",
    "            return operation\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in method ocr: {to_trace_str(e)}\")\n",
    "            \n",
    "    def get_extension(self, mime_type):\n",
    "        if mime_type == \"text/plain\":\n",
    "            return \".txt\"\n",
    "        elif mime_type == \"image/png\":\n",
    "            return \".png\"\n",
    "        else:\n",
    "            return \".txt\"\n",
    "        \n",
    "    def create_jsonl(self, gcs_path, mime_type, filename):\n",
    "        \"\"\"create jsonl out of files in bucket\n",
    "        \n",
    "        Args\n",
    "            gcs_path (str): bucket or dir where files are located\n",
    "            mime_type (str): the files mimetype \n",
    "            filename (str): the jsonl filename\n",
    "        \n",
    "        Returns\n",
    "            full path of jsonl\n",
    "        \"\"\"\n",
    "        try:\n",
    "            bucket_name, directory, _ = self.dismantle_path(gcs_path)\n",
    "            bucket = self.storage_client.bucket(bucket_name)\n",
    "            extension = self.get_extension(mime_type)\n",
    "\n",
    "            blob_list = [blob for blob in list(bucket.list_blobs()) if \\\n",
    "                             os.path.basename(directory) in blob.name and \\\n",
    "                             blob.name.endswith(extension)]\n",
    "\n",
    "            jsonl_content = \"\"\n",
    "\n",
    "            for b_idx, blob in enumerate(blob_list):\n",
    "                full_path = os.path.join(gcs_path,blob.name)\n",
    "\n",
    "                d = json.dumps(\n",
    "                    {\n",
    "                    \"content\": full_path,\n",
    "                    \"mimeType\": mime_type\n",
    "                    }\n",
    "                )+\"\\n\"\n",
    "\n",
    "                jsonl_content = jsonl_content+d\n",
    "\n",
    "\n",
    "\n",
    "            bucket.blob(filename).upload_from_string(jsonl_content)\n",
    "            logging.info(f\"uploaded jsonl {filename} to bucket {bucket_name}\")\n",
    "\n",
    "            return os.path.join(gcs_path,filename)\n",
    "        \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in jsonl creation: {to_trace_str(e)}\")\n",
    "            \n",
    "    def create_text_files(self, gcs_path):\n",
    "        try:\n",
    "            # init bucket\n",
    "            bucket_name, directory, _ = self.dismantle_path(gcs_path)\n",
    "            bucket = self.storage_client.bucket(bucket_name)\n",
    "            blob_list = [blob for blob in list(bucket.list_blobs()) if \\\n",
    "                             os.path.basename(directory) in blob.name and \\\n",
    "                             blob.name.endswith(\"output-1-to-1.json\")]\n",
    "            \n",
    "            for b_idx, blob in enumerate(blob_list):\n",
    "                logging.info(f\"creating {b_idx+1} of {len(blob_list)} text files\")\n",
    "                json_string = blob.download_as_string()\n",
    "                response = json.loads(json_string)\n",
    "                text = response['responses'][0]['fullTextAnnotation']['text'] \n",
    "                txt_path = blob.name.replace(\"output-1-to-1.json\", \".txt\")\n",
    "                text_blob = bucket.blob(txt_path)\n",
    "                text_blob.upload_from_string(text)\n",
    "                \n",
    "            logging.info(\"finished creating text files\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in method save_result_as_csv_in_storage: {to_trace_str(e)}\") \n",
    "    \n",
    "    \n",
    "    def save_to_storage(self, gcs_path, filename, predictions):\n",
    "        \"\"\"converts list of json into df, saves as temp csv file\"\"\"\n",
    "        try:\n",
    "            # init bucket\n",
    "            bucket_name, directory, _ = self.dismantle_path(gcs_path)\n",
    "            bucket = self.storage_client.bucket(bucket_name)\n",
    "\n",
    "            # create df\n",
    "            df = pd.DataFrame.from_records(predictions)\n",
    "\n",
    "            # save as tmpfile\n",
    "            _, path = tempfile.mkstemp()\n",
    "            df.to_csv(path, index=False)\n",
    "\n",
    "            # create new blob\n",
    "            blob = bucket.blob(filename)\n",
    "\n",
    "            # upload csv to blob\n",
    "            full_path = f\"{gcs_path}/{filename}\"\n",
    "            logging.info(f\"writing csv {full_path} to storage\")\n",
    "            with open(path, \"rb\") as my_file:\n",
    "                blob.upload_from_file(my_file)\n",
    "                \n",
    "            return full_path\n",
    "        \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in method save_result_as_csv_in_storage: {to_trace_str(e)}\")  \n",
    "                         \n",
    "    def load_to_bigquery(self, gcs_path, dataset_id, table_id, schema):\n",
    "        \"\"\"loads csv data in storage to BQ\"\"\"\n",
    "        # Send the dataset to the API for creation, with an explicit timeout.\n",
    "        # Raises google.api_core.exceptions.Conflict if the Dataset already\n",
    "        # exists within the project.\n",
    "        try:\n",
    "            dataset = bigquery.Dataset(dataset_id)\n",
    "            dataset.location = \"US\"\n",
    "            bq.get_dataset(dataset_id)  # Make an API request.\n",
    "            logging.info(\"Dataset {} already exists\".format(dataset_id))\n",
    "        except Exception as e:\n",
    "            logging.info(\"Dataset {} is not found\".format(dataset_id))\n",
    "            dataset = bq.create_dataset(dataset, timeout=30)  # Make an API request.\n",
    "            dataset.location = \"US\"\n",
    "            logging.info(\"Created dataset {}.{}\".format(bq.project, dataset.dataset_id))\n",
    "        finally:\n",
    "            # create bigquery table and upload csv\n",
    "            job_config = bigquery.LoadJobConfig(\n",
    "                schema=schema,\n",
    "                skip_leading_rows=1,\n",
    "                # The source format defaults to CSV, so the line below is optional.\n",
    "                source_format=bigquery.SourceFormat.CSV,\n",
    "                allow_quoted_newlines=True,\n",
    "\n",
    "            )\n",
    "            uri = gcs_path\n",
    "\n",
    "            load_job = bq.load_table_from_uri(\n",
    "                uri, table_id, job_config=job_config\n",
    "            )  # Make an API request.\n",
    "\n",
    "            load_job.result()  # Waits for the job to complete.\n",
    "\n",
    "            destination_table = bq.get_table(table_id)  # Make an API request.\n",
    "            print(\"Loaded {} rows.\".format(destination_table.num_rows))\n",
    "            \n",
    "    def run_automl_image_batch(self, project, region, model_resource_name, job_display_name, gcs_source, gcs_destination):\n",
    "        job = self.create_batch_prediction_job(\n",
    "            project, \n",
    "            region, \n",
    "            model_resource_name=model_resource_name, \n",
    "            job_display_name=job_display_name, \n",
    "            gcs_source=gcs_source, \n",
    "            gcs_destination=gcs_destination, \n",
    "            sync=True\n",
    "            )\n",
    "        \n",
    "        logging.info(f\"job started {type(job)} for automl image\")\n",
    "        \n",
    "        bucket_name, directory, _ = self.dismantle_path(gcs_destination)\n",
    "        logging.info(f\"bucket name {bucket_name}\")\n",
    "        bucket = self.storage_client.bucket(bucket_name)\n",
    "\n",
    "        # read results \n",
    "        results = []\n",
    "\n",
    "        blob_list  = [blob for blob in list(bucket.list_blobs()) if os.path.basename(gcs_destination) in blob.name and blob.name.endswith(\".jsonl\")]\n",
    "        for blob in blob_list:\n",
    "            blob_str = blob.download_as_string().decode(\"utf-8\") \n",
    "            responses = []\n",
    "            for line in blob_str.split(\"\\n\")[:-1]:\n",
    "                responses.append(json.loads(str(line)))\n",
    "\n",
    "            for response in responses:\n",
    "                \n",
    "                results.append({\n",
    "                    'file': response[\"instance\"][\"content\"][:-4], # \"gs://bucket/text.txt\" TODO: check if original path is needed\n",
    "                    'subject': response[\"prediction\"][\"displayNames\"][0],\n",
    "                    'score':  response[\"prediction\"][\"confidences\"][0],\n",
    "                    })\n",
    "\n",
    "        return results\n",
    "        \n",
    "            \n",
    "    def run_automl_text_batch(self, project, region, model_resource_name, job_display_name, gcs_source, gcs_destination):\n",
    "\n",
    "        job = self.create_batch_prediction_job(\n",
    "            project, \n",
    "            region, \n",
    "            model_resource_name=model_resource_name, \n",
    "            job_display_name=job_display_name, \n",
    "            gcs_source=gcs_source, \n",
    "            gcs_destination=gcs_destination, \n",
    "            sync=True\n",
    "            )\n",
    "\n",
    "        logging.info(f\"job started {type(job)} for automl text\")\n",
    "        \n",
    "        bucket_name, directory, _ = self.dismantle_path(gcs_destination)\n",
    "        logging.info(f\"bucket name {bucket_name}\")\n",
    "        bucket = self.storage_client.bucket(bucket_name)\n",
    "\n",
    "        # read results \n",
    "        results = []\n",
    "\n",
    "        blob_list  = [blob for blob in list(bucket.list_blobs()) if os.path.basename(gcs_destination) in blob.name and blob.name.endswith(\".jsonl\")]\n",
    "        for blob in blob_list:\n",
    "            blob_str = blob.download_as_string().decode(\"utf-8\") \n",
    "            responses = []\n",
    "            for line in blob_str.split(\"\\n\")[:-1]:\n",
    "                responses.append(json.loads(str(line)))\n",
    "\n",
    "            for response in responses:\n",
    "                results.append({\n",
    "                    'file': response[\"instance\"][\"content\"][:-4], # \"gs://bucket/text.txt\" TODO: check if original path is needed\n",
    "                    'subject': response[\"prediction\"][\"displayNames\"][0],\n",
    "                    'score':  response[\"prediction\"][\"confidences\"][0],\n",
    "                    })\n",
    "\n",
    "        return results\n",
    "\n",
    "\n",
    "    def create_batch_prediction_job(\n",
    "        self,\n",
    "        project,\n",
    "        location,\n",
    "        model_resource_name,\n",
    "        job_display_name,\n",
    "        gcs_source,\n",
    "        gcs_destination,\n",
    "        sync = True\n",
    "    ):\n",
    "        aiplatform.init(project=project, location=location)\n",
    "\n",
    "        my_model = aiplatform.Model(model_resource_name)\n",
    "\n",
    "        batch_prediction_job = my_model.batch_predict(\n",
    "            job_display_name=job_display_name,\n",
    "            gcs_source=gcs_source,\n",
    "            gcs_destination_prefix=gcs_destination,\n",
    "            sync=True\n",
    "        )\n",
    "\n",
    "        batch_prediction_job.wait()\n",
    "        \n",
    "        logging.info(f\"state type: {type(batch_prediction_job.state)}\")\n",
    "\n",
    "        logging.info(batch_prediction_job.display_name)\n",
    "        logging.info(batch_prediction_job.resource_name)\n",
    "        logging.info(batch_prediction_job.state)\n",
    "        return batch_prediction_job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "4aaafdcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pipeline():\n",
    "    def __init__(self, dataset_id=None):\n",
    "        self.utils = Utils()\n",
    "        self.uuid = datetime.now().strftime('%y%m%d_%H%M%S') #str\n",
    "        \n",
    "        self.project = \"qwiklabs-gcp-00-373ac55d0e0a\"\n",
    "        \n",
    "        self.region = \"us-central1\"  \n",
    "        \n",
    "        \n",
    "        self.dataset_id = dataset_id if dataset_id else f\"{self.project}.docprocessing_\"+self.uuid\n",
    "        \n",
    "        # find ids via !gcloud ai models list\n",
    "        self.tcn_model_resource_name = \"2393478483993952256\"\n",
    "        self.icn_model_resource_name = \"8925034949820547072\"\n",
    "        \n",
    "        \n",
    "        self.table_id_tcn = f\"{self.dataset_id}.tcn\" \n",
    "        self.table_id_icn = f\"{self.dataset_id}.icn\" \n",
    "        \n",
    "        self.tcn_schema = [\n",
    "                    bigquery.SchemaField(\"file\", \"STRING\", mode=\"REQUIRED\", description=\"File path.\"),\n",
    "                    bigquery.SchemaField(\"subject\", \"STRING\", mode=\"REQUIRED\", description=\"Predicted class.\"),\n",
    "                    bigquery.SchemaField(\"score\", \"FLOAT\", mode=\"REQUIRED\", description=\"Confidence of the prediction.\"),\n",
    "                ]\n",
    "        \n",
    "        self.icn_schema = [\n",
    "                    bigquery.SchemaField(\"image_name\", \"STRING\", mode=\"REQUIRED\", description='Name of the image analyzed.'),\n",
    "                    bigquery.SchemaField(\"label\", \"STRING\", mode=\"REQUIRED\", description='Predicted class. It can be US or EU'),\n",
    "                    bigquery.SchemaField(\"confidence\", \"FLOAT\", mode=\"REQUIRED\", description='Confidence of the prediction.'),\n",
    "                ]\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def start_pipeline(self, src_path):\n",
    "        # TODO: multiprocessing??\n",
    "        logging.info(f\"started pipeline\")\n",
    "        \n",
    "        # save everything in the same bucket\n",
    "        dst_path = src_path\n",
    "        jsonl_filename_tcn = f\"tcn_{self.uuid}.jsonl\"\n",
    "        jsonl_filename_icn = f\"icn_{self.uuid}.jsonl\"\n",
    "        \n",
    "        # create png\n",
    "        jsonl_path_icn = self.preprocess_pdf_to_png(src_path, dst_path, jsonl_filename_icn)\n",
    "        \n",
    "\n",
    "        # create ocr\n",
    "        jsonl_path_tcn = self.preprocess_ocr(src_path, dst_path, jsonl_filename_tcn)\n",
    "        \n",
    "        \n",
    "\n",
    "        # prediction\n",
    "        self.text_classification_task(\n",
    "            src_path=jsonl_path_tcn, \n",
    "            dst_path=dst_path, \n",
    "            job_display_name=\"job_tcn_\"+self.uuid)\n",
    "\n",
    "        self.image_classification_task(\n",
    "            src_path=jsonl_path_icn, \n",
    "            dst_path=dst_path, \n",
    "            job_display_name=\"job_icn_\"+self.uuid)\n",
    "        \n",
    "       \n",
    "        \n",
    "        logging.info(f\"finished pipelines\")\n",
    "        \n",
    "    def preprocess_pdf_to_png(self, src_path, dst_path, jsonl_filename):\n",
    "        self.utils.convert_pdf_to_png(src_path, dst_path)\n",
    "        \n",
    "        return self.utils.create_jsonl(gcs_path=dst_path, mime_type=\"image/png\", filename=jsonl_filename)\n",
    "    \n",
    "    def preprocess_ocr(self, src_path, dst_path, jsonl_filename):\n",
    "        ocr_operation = self.utils.ocr(src_path, dst_path)\n",
    "        \n",
    "        while not ocr_operation.done():\n",
    "            logging.info(\"wait for ocr to finish\")\n",
    "            time.sleep(5)\n",
    "            \n",
    "        self.utils.create_text_files(dst_path)\n",
    "        return self.utils.create_jsonl(gcs_path=dst_path, mime_type=\"text/plain\", filename=jsonl_filename)\n",
    "        \n",
    "        \n",
    "    def text_classification_task(self, src_path, dst_path, job_display_name):\n",
    "        gcs_source = src_path\n",
    "        gcs_destination = os.path.join(dst_path, job_display_name)\n",
    "        \n",
    "        logging.info(f\"starting tcn with gsc_source {gcs_source} and gcs_destination {gcs_destination}\")\n",
    "        \n",
    "        if not gcs_destination.startswith(\"gs://\"):\n",
    "            gcs_destination = \"gs://\" + gcs_destination\n",
    "        \n",
    "        predictions = self.utils.run_automl_text_batch(self.project, self.region, self.tcn_model_resource_name, job_display_name, gcs_source, gcs_destination)\n",
    "    \n",
    "        logger.info(\"save tcn predictions to storage\")\n",
    "        predictions_filename = \"predictions_tcn_\"+self.uuid+\".csv\"\n",
    "        path_to_csv = self.utils.save_to_storage(dst_path, predictions_filename, predictions)\n",
    "\n",
    "        # Step 5: Load storage result in BQ\n",
    "        logger.info(\"load results into BigQuery\")\n",
    "        status = self.utils.load_to_bigquery(path_to_csv, self.dataset_id, self.table_id_tcn, self.tcn_schema)\n",
    "        logging.info(f\"finished task with status {status}\")\n",
    "    \n",
    "    def image_classification_task(self, src_path, dst_path, job_display_name):\n",
    "        gcs_source = src_path\n",
    "        gcs_destination = os.path.join(dst_path, job_display_name)\n",
    "        \n",
    "        logging.info(f\"starting icn with gsc_source {gcs_source} and gcs_destination {gcs_destination}\")\n",
    "        \n",
    "        if not gcs_destination.startswith(\"gs://\"):\n",
    "            gcs_destination = \"gs://\" + gcs_destination\n",
    "            \n",
    "        predictions = self.utils.run_automl_image_batch(self.project, self.region, self.icn_model_resource_name, job_display_name, gcs_source, gcs_destination)\n",
    "        \n",
    "        logger.info(\"save icn predictions to storage\")\n",
    "        predictions_filename = \"predictions_icn_\"+self.uuid+\".csv\"\n",
    "        path_to_csv = self.utils.save_to_storage(dst_path, predictions_filename, predictions)\n",
    "\n",
    "        # Step 5: Load storage result in BQ\n",
    "        logger.info(\"load results into BigQuery\")\n",
    "        status = self.utils.load_to_bigquery(path_to_csv, self.dataset_id, self.table_id_icn, self.icn_schema)\n",
    "        logging.info(f\"finished task with status {status}\")\n",
    "\n",
    "    \n",
    "    def odet(self):\n",
    "        pass\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba35d32",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7236dc5",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "d9ce3dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_path = \"gs://2021_08_16_tcn_dev\"\n",
    "dst_path = \"gs://2021_08_16_tcn_dev\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "856f34cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "93f9049d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(dataset_id=\"qwiklabs-gcp-00-373ac55d0e0a.docprocessing_210818_181824\")\n",
    "# pipeline.start_pipeline(src_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90078bd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1ba9918d",
   "metadata": {},
   "source": [
    "# Labeled Patents - Vertex AI Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2efea730",
   "metadata": {},
   "source": [
    "## Importing Auxiliary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07646162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kubeflow pipelines version: 1.7.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "#!pip install --upgrade kfp\n",
    "import kfp\n",
    "from google.cloud import aiplatform\n",
    "from kfp.v2 import compiler\n",
    "from kfp.v2.dsl import component\n",
    "from kfp.v2.google import experimental\n",
    "from kfp.v2.google.client import AIPlatformClient\n",
    "\n",
    "print('Kubeflow pipelines version: {}'.format(kfp.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6e6bbb",
   "metadata": {},
   "source": [
    "## Setting Notebook Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6952f412",
   "metadata": {},
   "outputs": [],
   "source": [
    "UUID = datetime.now().strftime('%y%m%d_%H%M%S') #str\n",
    "PROJECT = 'qwiklabs-gcp-00-373ac55d0e0a'\n",
    "REGION = 'us-central1'\n",
    "\n",
    "BUCKET = 'patents_pipetest'\n",
    "PDF_BUCKET_PATH = 'pdf'\n",
    "\n",
    "RES_DATASET_NAME = 'docprocessing_' + UUID\n",
    "RES_DATASET_ID = f'{PROJECT}.{RES_DATASET_NAME}'\n",
    "\n",
    "TCN_MODEL_NAME = '2393478483993952256'\n",
    "TCN_RESTABLE_NAME = f'{RES_DATASET_ID}.tcn'\n",
    "TCN_RESTABLE_SCHEMA = [('file', 'STRING', 'REQUIRED', 'File path.'),\n",
    "                       ('subject', 'STRING', 'REQUIRED', 'Predicted class.'),\n",
    "                       ('score', 'FLOAT',  'REQUIRED', 'Confidence of the prediction.')]\n",
    "\n",
    "\n",
    "src_path = \"gs://2021_08_16_tcn_dev\"\n",
    "dst_path = \"gs://2021_08_16_tcn_dev\"\n",
    "\n",
    "\n",
    "\n",
    "PIPELINE_NAME = 'process-pdf-patents'\n",
    "PIPELINE_ROOT = f\"gs://{BUCKET}/labeled_patents/pipeline_root\"\n",
    "LOCAL_PIPELINE_PATH = './vertex_pipelines'\n",
    "LOCAL_PIPELINE_JSON = os.path.join(LOCAL_PIPELINE_PATH, 'labeled_patents_pipeline2.json')\n",
    "\n",
    "RESULTS_BQ_DATASET='demo_dataset'\n",
    "RESULTS_OBJDET_TABLE='objdet'\n",
    "\n",
    "\n",
    "\n",
    "MODEL_DISPLAY_NAME=f\"labpat_model\"\n",
    "MACHINE_TYPE=\"n1-standard-16\"\n",
    "REPLICA_COUNT=1\n",
    "DOCKER_IMAGE_URI_CREATE_BQDATASET=\"us-docker.pkg.dev/vertex-ai/training/tf-cpu.2-3:latest\"\n",
    "\n",
    "\n",
    "os.environ['PROJECT'] = PROJECT\n",
    "os.environ['REGION'] = REGION\n",
    "os.environ['BUCKET'] = BUCKET \n",
    "os.environ['PDF_BUCKET_PATH'] = PDF_BUCKET_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19a36f2",
   "metadata": {},
   "source": [
    "**Copying some demo files into the Bucket**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85cdad65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying gs://2021_08_16_tcn_dev/med_tech_8.pdf [Content-Type=application/pdf]...\n",
      "Copying gs://2021_08_16_tcn_dev/computer_vision_1.pdf [Content-Type=application/pdf]...\n",
      "Copying gs://2021_08_16_tcn_dev/us_076.pdf [Content-Type=application/pdf]...    \n",
      "/ [3/3 files][168.9 KiB/168.9 KiB] 100% Done                                    \n",
      "Operation completed over 3 objects/168.9 KiB.                                    \n"
     ]
    }
   ],
   "source": [
    "!gsutil -m cp gs://2021_08_16_tcn_dev/*.pdf gs://$BUCKET/$PDF_BUCKET_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5228f56a",
   "metadata": {},
   "source": [
    "## Defining Vertex AI Components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8decfd6d",
   "metadata": {},
   "source": [
    "### Component 1: Performing OCR on PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "08307446",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(packages_to_install=['google-cloud-storage',  'google-cloud-vision'])\n",
    "def perform_ocr_on_pdfs(src_path: str, \n",
    "                        dst_path: str,\n",
    "                        uuid: str):\n",
    "    \n",
    "    # IMPORTS:\n",
    "    import os\n",
    "    import logging\n",
    "    import traceback as tb\n",
    "    from pathlib import Path\n",
    "    from google.cloud import storage\n",
    "    from google.cloud import vision\n",
    "    # from google.cloud import aiplatform\n",
    "\n",
    "    \n",
    "    # AUXILIARY FUNCTIONS:\n",
    "    def to_trace_str(e):\n",
    "        return ''.join(tb.format_exception(None, e, e.__traceback__))   \n",
    "    \n",
    "    \n",
    "    def dismantle_path(self, gcs_path):\n",
    "        parts = Path(gcs_path).parts\n",
    "        bucket_idx = 1 if parts[0].startswith(\"gs\") else 0\n",
    "        filename_idx = -1 if \".\" in parts[-1] else None\n",
    "\n",
    "        bucket_name = parts[bucket_idx]\n",
    "        filename = parts[filename_idx] if filename_idx else \"\"\n",
    "        directory = \"/\".join(parts[bucket_idx:filename_idx] if filename_idx else parts[bucket_idx+1:])\n",
    "        return bucket_name, directory, filename\n",
    "    \n",
    "    \n",
    "    def ocr(self, src_path, dst_path):\n",
    "        \"\"\"Perform optical character recognition in pdf files.\n",
    "        \n",
    "        Args\n",
    "            src_path\n",
    "            dst_path\n",
    "        \n",
    "        Returns\n",
    "            google.api_core.operation.Operation\n",
    "            To check if done use method .done()\n",
    "            \n",
    "        Link to documentation:  \n",
    "            https://googleapis.dev/python/vision/latest/vision_v1/types.html#google.cloud.vision_v1.types.OutputConfig\n",
    "            https://cloud.google.com/vision/docs/pdf\n",
    "        \n",
    "        \"\"\"\n",
    "        try:\n",
    "            logging.info(\"started optical character recognition\")\n",
    "            \n",
    "            src_bucket_name, src_directory, _ = dismantle_path(src_path)\n",
    "            dst_bucket_name, dst_directory, _ = dismantle_path(dst_path)\n",
    "            \n",
    "            storage_client = storage.Client()\n",
    "            src_bucket = storage_client.bucket(src_bucket_name)\n",
    "            dst_bucket = storage_client.bucket(dst_bucket_name)\n",
    "            \n",
    "            logging.info(f\"src_bucket_name {src_bucket_name}, src_directory {src_directory}\")\n",
    "\n",
    "            blob_list = [blob for blob in list(src_bucket.list_blobs()) if \\\n",
    "                         os.path.basename(src_directory) in blob.name and \\\n",
    "                         blob.name.endswith(\".pdf\")]\n",
    "            \n",
    "            logging.info(f\"found {len(blob_list)} pdf files in bucket {src_bucket_name}\")\n",
    "\n",
    "            client = vision.ImageAnnotatorClient()\n",
    "            feature = vision.Feature(type_=vision.Feature.Type.DOCUMENT_TEXT_DETECTION)\n",
    "            \n",
    "            operations = []\n",
    "            async_requests = []\n",
    "            \n",
    "            for b_idx, blob in enumerate(blob_list):\n",
    "                gcs_source_uri = os.path.join(src_path, blob.name)\n",
    "                gcs_destination_uri = os.path.join(dst_path, blob.name)\n",
    "\n",
    "                # source\n",
    "                gcs_source = vision.GcsSource(uri=gcs_source_uri)\n",
    "                input_config = vision.InputConfig(gcs_source=gcs_source, mime_type='application/pdf')\n",
    "\n",
    "                # destination\n",
    "                gcs_destination = vision.GcsDestination(uri=gcs_destination_uri)\n",
    "                output_config = vision.OutputConfig(gcs_destination=gcs_destination, batch_size=1)\n",
    "\n",
    "                logging.info(f\"started ocr for {b_idx} of {len(blob_list)} files\")\n",
    "                async_request = vision.AsyncAnnotateFileRequest(\n",
    "                    features=[feature], \n",
    "                    input_config=input_config,\n",
    "                    output_config=output_config\n",
    "                )\n",
    "                async_requests.append(async_request)\n",
    "\n",
    "            operation = client.async_batch_annotate_files(requests=async_requests)\n",
    "            return operation\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in method ocr: {to_trace_str(e)}\")\n",
    "    \n",
    "    \n",
    "    def preprocess_ocr(self, src_path, dst_path, jsonl_filename):\n",
    "        ocr_operation = self.utils.ocr(src_path, dst_path)\n",
    "        \n",
    "        while not ocr_operation.done():\n",
    "            logging.info(\"wait for ocr to finish\")\n",
    "            time.sleep(5)\n",
    "            \n",
    "        self.utils.create_text_files(dst_path)\n",
    "        return self.utils.create_jsonl(gcs_path=dst_path, mime_type=\"text/plain\", filename=jsonl_filename)\n",
    "    \n",
    "    \n",
    "    def text_classification_task(src_path, dst_path, job_display_name):\n",
    "        gcs_source = src_path\n",
    "        gcs_destination = os.path.join(dst_path, job_display_name)\n",
    "        \n",
    "        logging.info(f\"starting tcn with gsc_source {gcs_source} and gcs_destination {gcs_destination}\")\n",
    "        \n",
    "        if not gcs_destination.startswith(\"gs://\"):\n",
    "            gcs_destination = \"gs://\" + gcs_destination\n",
    "        \n",
    "        predictions = self.utils.run_automl_text_batch(self.project, self.region, self.tcn_model_resource_name, job_display_name, gcs_source, gcs_destination)\n",
    "    \n",
    "        logger.info(\"save tcn predictions to storage\")\n",
    "        predictions_filename = \"predictions_tcn_\"+self.uuid+\".csv\"\n",
    "        path_to_csv = self.utils.save_to_storage(dst_path, predictions_filename, predictions)\n",
    "\n",
    "        # Step 5: Load storage result in BQ\n",
    "        logger.info(\"load results into BigQuery\")\n",
    "        status = self.utils.load_to_bigquery(path_to_csv, self.dataset_id, self.table_id_tcn, self.tcn_schema)\n",
    "        logging.info(f\"finished task with status {status}\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    # PIPELINE COMPONENT MAIN CODE:\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    logger = logging.getLogger(__name__)\n",
    "    \n",
    "    logging.info(f\"Starting the processing of pdfs with the OCR functionality of Google Vision API.\")\n",
    "        \n",
    "    # save everything in the same bucket\n",
    "    dst_path = src_path\n",
    "    jsonl_filename_tcn = f\"tcn_{uuid}.jsonl\"\n",
    "        \n",
    "    # create ocr\n",
    "    jsonl_path_tcn = preprocess_ocr(src_path, dst_path, jsonl_filename_tcn)\n",
    "        \n",
    "    # prediction\n",
    "    text_classification_task(\n",
    "            src_path=jsonl_path_tcn, \n",
    "            dst_path=dst_path, \n",
    "            job_display_name=\"job_tcn_\"+ uuid)\n",
    "        \n",
    "    logging.info(f\"PDF files have succesfully processed with Google Vison OCR\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8f7477",
   "metadata": {},
   "source": [
    "### Component 2: PDF to PNG conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f6de4514",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component()\n",
    "def transform_pdfs_into_png():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6959010d",
   "metadata": {},
   "source": [
    "### Component 3: Creating a BigQuery dataset to save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f99978ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component()\n",
    "def create_bq_results_dataset():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02fcffaf",
   "metadata": {},
   "source": [
    "### Component 4.1: Creating image classification results table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cea45a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component()\n",
    "def create_text_class_results_table():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f12fa22",
   "metadata": {},
   "source": [
    "### Component 4.2: Performing text classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8b2de1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component()\n",
    "def text_class_predict():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6244ad",
   "metadata": {},
   "source": [
    "### Component 4.3: Storing text classification results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "995edb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component()\n",
    "def store_text_class_results():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6348fe02",
   "metadata": {},
   "source": [
    "### Component 5.1: Creating image classification results table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "33b52304",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component()\n",
    "def create_img_class_results_table():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4383cce",
   "metadata": {},
   "source": [
    "### Component 5.2: Performing image classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8be8bbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component()\n",
    "def img_class_predict():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37061c3",
   "metadata": {},
   "source": [
    "### Component 5.3: Storing image classification results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "57ab6521",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component()\n",
    "def store_img_class_results():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78531809",
   "metadata": {},
   "source": [
    "### Component 6.1: Creating object detection results table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d3ba5a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component()\n",
    "def create_obj_detection_results_table():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7addb9dd",
   "metadata": {},
   "source": [
    "### Component 6.2: Performing object detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "61eb1460",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component()\n",
    "def obj_detection_predict():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c56a487",
   "metadata": {},
   "source": [
    "### Component 6.3: Storing object detection results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8563ed7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component()\n",
    "def store_obj_detection_results():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f16f9d",
   "metadata": {},
   "source": [
    "## Creating and Compiling the Vertex AI Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2ab58782",
   "metadata": {},
   "outputs": [],
   "source": [
    "@kfp.dsl.pipeline(name=PIPELINE_NAME, \n",
    "                  description='Pipeline that process patents pdf files.',\n",
    "                  pipeline_root=PIPELINE_ROOT)\n",
    "\n",
    "def pipeline():\n",
    "    # Preprocessing pipeline:\n",
    "    perform_ocr_on_pdfs_task = perform_ocr_on_pdfs()\n",
    "    \n",
    "    transform_pdfs_into_png_task = transform_pdfs_into_png()\n",
    "    transform_pdfs_into_png_task.after(perform_ocr_on_pdfs_task)\n",
    "\n",
    "    create_bq_results_dataset_task = create_bq_results_dataset()\n",
    "    create_bq_results_dataset_task.after(transform_pdfs_into_png_task)\n",
    "    \n",
    "    # Text classification pipeline:\n",
    "    create_text_class_results_table_task = create_text_class_results_table()\n",
    "    create_text_class_results_table_task.after(create_bq_results_dataset_task)\n",
    "    \n",
    "    text_class_predict_task = text_class_predict()\n",
    "    text_class_predict_task.after(create_text_class_results_table_task)\n",
    "    \n",
    "    store_text_class_results_task = store_text_class_results()\n",
    "    store_text_class_results_task.after(text_class_predict_task)\n",
    "    \n",
    "    # Image classification pipeline:\n",
    "    create_img_class_results_table_task = create_img_class_results_table()\n",
    "    create_img_class_results_table_task.after(create_bq_results_dataset_task)\n",
    "    \n",
    "    img_class_predict_task = img_class_predict()\n",
    "    img_class_predict_task.after(create_img_class_results_table_task)\n",
    "    \n",
    "    store_img_class_results_task = store_img_class_results()\n",
    "    store_img_class_results_task.after(img_class_predict_task)\n",
    "        \n",
    "    # Object detection pipeline:\n",
    "    create_obj_detection_results_table_task = create_obj_detection_results_table()\n",
    "    create_obj_detection_results_table_task.after(create_bq_results_dataset_task)\n",
    "    \n",
    "    obj_detection_predict_task = obj_detection_predict()\n",
    "    obj_detection_predict_task.after(create_obj_detection_results_table_task)\n",
    "    \n",
    "    store_obj_detection_results_task = store_obj_detection_results()\n",
    "    store_obj_detection_results_task.after(obj_detection_predict_task)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "38054c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(LOCAL_PIPELINE_PATH):\n",
    "    os.mkdir(LOCAL_PIPELINE_PATH)\n",
    "\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=pipeline,\n",
    "    package_path=LOCAL_PIPELINE_JSON\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d732390",
   "metadata": {},
   "source": [
    "## Launching the Vertex AI Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3d44c366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating an API client object:\n",
    "# TODO: use the new Vertex AI.\n",
    "api_client = AIPlatformClient(\n",
    "    project_id=PROJECT,\n",
    "    region=REGION,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "09368481",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "See the Pipeline job <a href=\"https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/process-pdf-patents-20210819085956?project=qwiklabs-gcp-00-373ac55d0e0a\" target=\"_blank\" >here</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = api_client.create_run_from_job_spec(\n",
    "    LOCAL_PIPELINE_JSON,\n",
    "    pipeline_root=f\"{PIPELINE_ROOT}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fcb692",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-5.m76",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-5:m76"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
