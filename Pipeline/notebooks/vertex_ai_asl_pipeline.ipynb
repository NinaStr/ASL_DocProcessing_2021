{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ad25038",
   "metadata": {},
   "source": [
    "# Prepare pdfs for later in pipeline (Obj Det, Img, text, NER)\n",
    "\n",
    "- user provides\n",
    "    - Google Cloud project (input)\n",
    "    - bucket in GCS of pdfs (input)\n",
    "    - BQ dataset to write prediction results (output)\n",
    "        - BQ table: aggregated results (pdf_name, icn_pred, objdet_pred(coords), text_cn, ner1, ner2, ...., ner)\n",
    "            created with JOIN on pdf_name\n",
    "        - BQ table: icn_preds (pdf_name, icn_pred)    --> this table is made in icn_predict.ipynb\n",
    "        - BQ table: objdet_pred (pdf_name, objdet_pred(coords)) --> this table is made in objdet_predict.ipynb\n",
    "        - BQ table: text_cn (pdf_name, text_cn)    --> this table is made in text_cn_predict.ipynb\n",
    "        - BQ table: ner (pdf_name, ner1, ner2, ...., ner)\n",
    "        \n",
    "- see utils.py for utils functions\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c592cc2e",
   "metadata": {},
   "source": [
    "Steps: \n",
    " 1. convert pdf to png and write to bucket (for ICN, ObjDet)\n",
    " 2. do ocr on pdf and write to bucket \n",
    " 3. create dataset \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5acacc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT = !gcloud config get-value project # returns SList\n",
    "PROJECT = PROJECT[0] # gets first element in list -> str\n",
    "REGION = \"us-central1\"  \n",
    "MODEL_RESOURCE_NAME = \"2393478483993952256\"\n",
    "\n",
    "import os\n",
    "os.environ[\"PROJECT\"] = PROJECT\n",
    "os.environ[\"REGION\"] = REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a13e1697",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "bq = bigquery.Client(project=PROJECT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99bb76c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "e4753c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "from google.cloud import vision\n",
    "from google.cloud import aiplatform\n",
    "import tempfile\n",
    "\n",
    "\n",
    "\n",
    "from importlib import reload\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# for jupyter only\n",
    "import logging\n",
    "reload(logging)\n",
    "logging.basicConfig(format='%(asctime)s %(levelname)s:%(message)s', level=logging.DEBUG, datefmt='%I:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35f212a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdf2image import convert_from_path\n",
    "import io\n",
    "import base64\n",
    "import cv2\n",
    "from datetime import datetime\n",
    "import time\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ad2b818",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03:03:22 INFO:test if logging works\n"
     ]
    }
   ],
   "source": [
    "logging.info(\"test if logging works\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d773c3fb",
   "metadata": {},
   "source": [
    "# Labeled Patents - Vertex AI Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855456bb",
   "metadata": {},
   "source": [
    "## Importing Auxiliary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe7b31cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kubeflow pipelines version: 1.6.6\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "#!pip install --upgrade kfp\n",
    "import kfp\n",
    "from google.cloud import aiplatform\n",
    "from kfp.v2 import compiler\n",
    "from kfp.v2.dsl import component\n",
    "from kfp.v2.google import experimental\n",
    "from kfp.v2.google.client import AIPlatformClient\n",
    "\n",
    "print('Kubeflow pipelines version: {}'.format(kfp.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f742a9",
   "metadata": {},
   "source": [
    "## Setting Notebook Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "93be30f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "UUID = datetime.now().strftime('%y%m%d_%H%M%S') #str\n",
    "PROJECT = 'qwiklabs-gcp-00-373ac55d0e0a'\n",
    "REGION = 'us-central1'\n",
    "\n",
    "BUCKET = 'patents_pipetest'\n",
    "PDF_BUCKET_PATH = 'pdf'\n",
    "\n",
    "RES_DATASET_NAME = 'docprocessing_' + UUID\n",
    "RES_DATASET_ID = f'{PROJECT}.{RES_DATASET_NAME}'\n",
    "\n",
    "TCN_MODEL_NAME = '2393478483993952256'\n",
    "TCN_RESTABLE_NAME = f'{RES_DATASET_ID}.tcn'\n",
    "TCN_RESTABLE_SCHEMA = \"\"\"\n",
    "[\n",
    "   {\n",
    "      \"name\": \"file\",\n",
    "      \"field_type\": \"STRING\",\n",
    "      \"mode\": \"REQUIRED\",\n",
    "      \"description\": \"File path.\"\n",
    "   },\n",
    "   {\n",
    "      \"name\": \"subject\",\n",
    "      \"field_type\": \"STRING\",\n",
    "      \"mode\": \"REQUIRED\",\n",
    "      \"description\": \"Predicted class.\"\n",
    "   },\n",
    "   {\n",
    "      \"name\": \"score\",\n",
    "      \"field_type\": \"STRING\",\n",
    "      \"mode\": \"REQUIRED\",\n",
    "      \"description\": \"Confidence of the prediction.\"\n",
    "   }\n",
    "]\n",
    "\"\"\"\n",
    "\n",
    "src_path = \"gs://2021_08_16_tcn_dev\"\n",
    "dst_path = \"gs://2021_08_16_tcn_dev\"\n",
    "\n",
    "\n",
    "\n",
    "PIPELINE_NAME = 'process-pdf-patents'\n",
    "PIPELINE_ROOT = f\"gs://{BUCKET}/labeled_patents/pipeline_root\"\n",
    "LOCAL_PIPELINE_PATH = './vertex_pipelines'\n",
    "LOCAL_PIPELINE_JSON = os.path.join(LOCAL_PIPELINE_PATH, 'labeled_patents_pipeline2.json')\n",
    "\n",
    "RESULTS_BQ_DATASET='demo_dataset'\n",
    "RESULTS_OBJDET_TABLE='objdet'\n",
    "\n",
    "\n",
    "\n",
    "MODEL_DISPLAY_NAME=f\"labpat_model\"\n",
    "MACHINE_TYPE=\"n1-standard-16\"\n",
    "REPLICA_COUNT=1\n",
    "DOCKER_IMAGE_URI_CREATE_BQDATASET=\"us-docker.pkg.dev/vertex-ai/training/tf-cpu.2-3:latest\"\n",
    "\n",
    "\n",
    "os.environ['PROJECT'] = PROJECT\n",
    "os.environ['REGION'] = REGION\n",
    "os.environ['BUCKET'] = BUCKET \n",
    "os.environ['PDF_BUCKET_PATH'] = PDF_BUCKET_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a5c8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "[\n",
    "    bigquery.SchemaField(\"file\", \"STRING\", mode=\"REQUIRED\", description=\"File path.\"),\n",
    "    bigquery.SchemaField(\"subject\", \"STRING\", mode=\"REQUIRED\", description=\"Predicted class.\"),\n",
    "    bigquery.SchemaField(\"score\", \"FLOAT\", mode=\"REQUIRED\", description=\"Confidence of the prediction.\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e5853c70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SchemaField('file', 'STRING', 'REQUIRED', 'File path.', (), None),\n",
       " SchemaField('subject', 'STRING', 'REQUIRED', 'Predicted class.', (), None),\n",
       " SchemaField('score', 'STRING', 'REQUIRED', 'Confidence of the prediction.', (), None)]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import ast\n",
    "# from google.cloud import bigquery\n",
    "\n",
    "# # schema_str = json.dumps(TCN_RESTABLE_SCHEMA, indent=3)\n",
    "# # schema_list= json.loads(ast.literal_eval(schema_str))\n",
    "# schema_lst = ast.literal_eval(TCN_RESTABLE_SCHEMA)\n",
    "# schema = [bigquery.SchemaField(**tup) for tup in schema_lst]\n",
    "# schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20f4cbf",
   "metadata": {},
   "source": [
    "**Copying some demo files into the Bucket**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78d191e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying gs://2021_08_16_tcn_dev/med_tech_8.pdf [Content-Type=application/pdf]...\n",
      "Copying gs://2021_08_16_tcn_dev/computer_vision_1.pdf [Content-Type=application/pdf]...\n",
      "Copying gs://2021_08_16_tcn_dev/us_076.pdf [Content-Type=application/pdf]...    \n",
      "/ [3/3 files][168.9 KiB/168.9 KiB] 100% Done                                    \n",
      "Operation completed over 3 objects/168.9 KiB.                                    \n"
     ]
    }
   ],
   "source": [
    "!gsutil -m cp gs://2021_08_16_tcn_dev/*.pdf gs://$BUCKET/$PDF_BUCKET_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6bcdac3",
   "metadata": {},
   "source": [
    "## Defining Vertex AI Components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd1d7da",
   "metadata": {},
   "source": [
    "### Component 1: Performing OCR on PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4b699aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "# time.sleep(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3b6b4251",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(packages_to_install=['google-cloud-storage',  'google-cloud-vision'])\n",
    "def perform_ocr_on_pdfs(src_path: str, \n",
    "                        dst_path: str,\n",
    "                        uuid: str,\n",
    "                        project: str):\n",
    "    \n",
    "    # IMPORTS:\n",
    "    import os\n",
    "    import logging\n",
    "    import traceback as tb\n",
    "    import time\n",
    "    from pathlib import Path\n",
    "    from google.cloud import storage\n",
    "    from google.cloud import vision\n",
    "    # from google.cloud import aiplatform\n",
    "\n",
    "    \n",
    "    # AUXILIARY FUNCTIONS:\n",
    "    def to_trace_str(e):\n",
    "        return ''.join(tb.format_exception(None, e, e.__traceback__))   \n",
    "    \n",
    "    \n",
    "    def dismantle_path(gcs_path):\n",
    "        parts = Path(gcs_path).parts\n",
    "        bucket_idx = 1 if parts[0].startswith(\"gs\") else 0\n",
    "        filename_idx = -1 if \".\" in parts[-1] else None\n",
    "\n",
    "        bucket_name = parts[bucket_idx]\n",
    "        filename = parts[filename_idx] if filename_idx else \"\"\n",
    "        directory = \"/\".join(parts[bucket_idx:filename_idx] if filename_idx else parts[bucket_idx+1:])\n",
    "        return bucket_name, directory, filename\n",
    "    \n",
    "    \n",
    "    def ocr(src_path, dst_path, project):\n",
    "        \"\"\"Perform optical character recognition in pdf files.\n",
    "        \n",
    "        Args\n",
    "            src_path\n",
    "            dst_path\n",
    "        \n",
    "        Returns\n",
    "            google.api_core.operation.Operation\n",
    "            To check if done use method .done()\n",
    "            \n",
    "        Link to documentation:  \n",
    "            https://googleapis.dev/python/vision/latest/vision_v1/types.html#google.cloud.vision_v1.types.OutputConfig\n",
    "            https://cloud.google.com/vision/docs/pdf\n",
    "        \n",
    "        \"\"\"\n",
    "        try:\n",
    "            logging.info(\"started optical character recognition\")\n",
    "            \n",
    "            src_bucket_name, src_directory, _ = dismantle_path(src_path)\n",
    "            dst_bucket_name, dst_directory, _ = dismantle_path(dst_path)\n",
    "            \n",
    "            storage_client = storage.Client(projet=project)\n",
    "            src_bucket = storage_client.bucket(src_bucket_name)\n",
    "            dst_bucket = storage_client.bucket(dst_bucket_name)\n",
    "            \n",
    "            logging.info(f\"src_bucket_name {src_bucket_name}, src_directory {src_directory}\")\n",
    "\n",
    "            blob_list = [blob for blob in list(src_bucket.list_blobs()) if \\\n",
    "                         os.path.basename(src_directory) in blob.name and \\\n",
    "                         blob.name.endswith(\".pdf\")]\n",
    "            \n",
    "            logging.info(f\"found {len(blob_list)} pdf files in bucket {src_bucket_name}\")\n",
    "\n",
    "            client = vision.ImageAnnotatorClient()\n",
    "            feature = vision.Feature(type_=vision.Feature.Type.DOCUMENT_TEXT_DETECTION)\n",
    "            \n",
    "            operations = []\n",
    "            async_requests = []\n",
    "            \n",
    "            for b_idx, blob in enumerate(blob_list):\n",
    "                gcs_source_uri = os.path.join(src_path, blob.name)\n",
    "                gcs_destination_uri = os.path.join(dst_path, blob.name)\n",
    "\n",
    "                # source\n",
    "                gcs_source = vision.GcsSource(uri=gcs_source_uri)\n",
    "                input_config = vision.InputConfig(gcs_source=gcs_source, mime_type='application/pdf')\n",
    "\n",
    "                # destination\n",
    "                gcs_destination = vision.GcsDestination(uri=gcs_destination_uri)\n",
    "                output_config = vision.OutputConfig(gcs_destination=gcs_destination, batch_size=1)\n",
    "\n",
    "                logging.info(f\"started ocr for {b_idx} of {len(blob_list)} files\")\n",
    "                async_request = vision.AsyncAnnotateFileRequest(\n",
    "                    features=[feature], \n",
    "                    input_config=input_config,\n",
    "                    output_config=output_config\n",
    "                )\n",
    "                async_requests.append(async_request)\n",
    "\n",
    "            operation = client.async_batch_annotate_files(requests=async_requests)\n",
    "            return operation\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in method ocr: {to_trace_str(e)}\")\n",
    "            \n",
    "            \n",
    "    def create_text_files(gcs_path, project):\n",
    "        try:\n",
    "            # init bucket\n",
    "            bucket_name, directory, _ = dismantle_path(gcs_path)\n",
    "            storage_client = storage.Client(projet=project)\n",
    "            bucket = storage_client.bucket(bucket_name)\n",
    "            blob_list = [blob for blob in list(bucket.list_blobs()) if \\\n",
    "                             os.path.basename(directory) in blob.name and \\\n",
    "                             blob.name.endswith(\"output-1-to-1.json\")]\n",
    "            \n",
    "            for b_idx, blob in enumerate(blob_list):\n",
    "                logging.info(f\"creating {b_idx+1} of {len(blob_list)} text files\")\n",
    "                json_string = blob.download_as_string()\n",
    "                response = json.loads(json_string)\n",
    "                text = response['responses'][0]['fullTextAnnotation']['text'] \n",
    "                txt_path = blob.name.replace(\"output-1-to-1.json\", \".txt\")\n",
    "                text_blob = bucket.blob(txt_path)\n",
    "                text_blob.upload_from_string(text)\n",
    "                \n",
    "            logging.info(\"finished creating text files\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in method create_text_files: {to_trace_str(e)}\") \n",
    "            \n",
    "    def get_extension(mime_type):\n",
    "        if mime_type == \"text/plain\":\n",
    "            return \".txt\"\n",
    "        elif mime_type == \"image/png\":\n",
    "            return \".png\"\n",
    "        else:\n",
    "            return \".txt\"\n",
    "    \n",
    "    def create_jsonl(gcs_path, mime_type, filename,project):\n",
    "        \"\"\"create jsonl out of files in bucket\n",
    "        \n",
    "        Args\n",
    "            gcs_path (str): bucket or dir where files are located\n",
    "            mime_type (str): the files mimetype \n",
    "            filename (str): the jsonl filename\n",
    "        \n",
    "        Returns\n",
    "            full path of jsonl\n",
    "        \"\"\"\n",
    "        try:\n",
    "            bucket_name, directory, _ = dismantle_path(gcs_path)\n",
    "            storage_client = storage.Client(projet=project)\n",
    "            bucket = storage_client.bucket(bucket_name)\n",
    "            extension = get_extension(mime_type)\n",
    "\n",
    "            blob_list = [blob for blob in list(bucket.list_blobs()) if \\\n",
    "                             os.path.basename(directory) in blob.name and \\\n",
    "                             blob.name.endswith(extension)]\n",
    "\n",
    "            jsonl_content = \"\"\n",
    "\n",
    "            for b_idx, blob in enumerate(blob_list):\n",
    "                full_path = os.path.join(gcs_path,blob.name)\n",
    "\n",
    "                d = json.dumps(\n",
    "                    {\n",
    "                    \"content\": full_path,\n",
    "                    \"mimeType\": mime_type\n",
    "                    }\n",
    "                )+\"\\n\"\n",
    "\n",
    "                jsonl_content = jsonl_content+d\n",
    "\n",
    "\n",
    "\n",
    "            bucket.blob(filename).upload_from_string(jsonl_content)\n",
    "            logging.info(f\"uploaded jsonl {filename} to bucket {bucket_name}\")\n",
    "\n",
    "            return os.path.join(gcs_path,filename)\n",
    "        \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in jsonl creation: {to_trace_str(e)}\")\n",
    "    \n",
    "    \n",
    "    def preprocess_ocr(src_path, dst_path, jsonl_filename, project):\n",
    "        ocr_operation = ocr(src_path, dst_path, project)\n",
    "        \n",
    "        while not ocr_operation.done():\n",
    "            logging.info(\"wait for ocr to finish\")\n",
    "            time.sleep(5)\n",
    "            \n",
    "        create_text_files(dst_path, project)\n",
    "        return create_jsonl(gcs_path=dst_path, mime_type=\"text/plain\", filename=jsonl_filename, project=project)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # PIPELINE COMPONENT MAIN CODE:\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logging.info(f\"Starting the processing of pdfs with the OCR functionality of Google Vision API.\")\n",
    "    \n",
    "        \n",
    "    # save everything in the same bucket\n",
    "    dst_path = src_path\n",
    "    jsonl_filename_tcn = f\"tcn_{uuid}.jsonl\"\n",
    "        \n",
    "    # create ocr\n",
    "    jsonl_path_tcn = preprocess_ocr(src_path, src_path, jsonl_filename_tcn, project)\n",
    "    \n",
    "    # return path where jsonl with .txt files is saved\n",
    "    return jsonl_path_tcn "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a436f7a",
   "metadata": {},
   "source": [
    "### Component 2: PDF to PNG conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dce9c309",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component()\n",
    "def transform_pdfs_into_png():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee2bbfa",
   "metadata": {},
   "source": [
    "### Component 3: Creating a BigQuery dataset to save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9fccd721",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(packages_to_install=['google-cloud-bigquery'])\n",
    "def create_bq_results_dataset(project: str, \n",
    "                              dataset_id: str):\n",
    "        \"\"\"loads csv data in storage to BQ\"\"\"\n",
    "        # Send the dataset to the API for creation, with an explicit timeout.\n",
    "        # Raises google.api_core.exceptions.Conflict if the Dataset already\n",
    "        # exists within the project.\n",
    "        from google.cloud import bigquery\n",
    "        bq = bigquery.Client(project=project)\n",
    "        try:\n",
    "            dataset = bigquery.Dataset(dataset_id)\n",
    "            dataset.location = \"US\"\n",
    "            bq.get_dataset(dataset_id)  # Make an API request.\n",
    "            logging.info(\"Dataset {} already exists\".format(dataset_id))\n",
    "        except Exception as e:\n",
    "            logging.info(\"Dataset {} is not found\".format(dataset_id))\n",
    "            dataset = bq.create_dataset(dataset, timeout=30)  # Make an API request.\n",
    "            dataset.location = \"US\"\n",
    "            logging.info(\"Created dataset {}.{}\".format(bq.project, dataset.dataset_id))\n",
    "        finally:\n",
    "            logging.info(f\"Finished creating or loading dataset {dataset_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8b5437",
   "metadata": {},
   "source": [
    "### Component 4.1: Creating image classification results table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "30116dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(packages_to_install=['google-cloud-bigquery'])\n",
    "def create_text_class_results_table(project:str, \n",
    "                                    dataset_id:str, \n",
    "                                    table_id:str, \n",
    "                                    schema:str):\n",
    "    \n",
    "    import ast\n",
    "    from google.cloud import bigquery\n",
    "    bq = bigquery.Client(project=project)\n",
    "    \n",
    "    try:\n",
    "        dataset = bigquery.Dataset(dataset_id)\n",
    "        dataset.location = \"US\"\n",
    "        bq.get_dataset(dataset_id)  # Make an API request.\n",
    "        logging.info(\"Dataset {} already exists\".format(dataset_id))\n",
    "    except Exception as e:\n",
    "        logging.info(\"Dataset {} is not found\".format(dataset_id))\n",
    "        dataset = bq.create_dataset(dataset, timeout=30)  # Make an API request.\n",
    "        dataset.location = \"US\"\n",
    "        logging.info(\"Created dataset {}.{}\".format(bq.project, dataset.dataset_id))\n",
    "    finally:\n",
    "        # create table\n",
    "        schema = [bigquery.SchemaField(**tup) for tup in ast.literal_eval(schema)]\n",
    "        \n",
    "        table = bigquery.Table(table_id, schema=[bigquery.SchemaField(*tup) for tup in schema])\n",
    "        table = bq.create_table(table)\n",
    "        logging.info(f\"Created table {table_id}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b7c2d8",
   "metadata": {},
   "source": [
    "### Component 4.2: Performing text classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8126e145",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component()\n",
    "def text_class_predict():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79b4e1d",
   "metadata": {},
   "source": [
    "### Component 4.3: Storing text classification results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "239cbe73",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component()\n",
    "def store_text_class_results():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8b01eb",
   "metadata": {},
   "source": [
    "### Component 5.1: Creating image classification results table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bb59c1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component()\n",
    "def create_img_class_results_table():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41930f3",
   "metadata": {},
   "source": [
    "### Component 5.2: Performing image classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "755948d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component()\n",
    "def img_class_predict():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788af958",
   "metadata": {},
   "source": [
    "### Component 5.3: Storing image classification results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3883123b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component()\n",
    "def store_img_class_results():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0620047d",
   "metadata": {},
   "source": [
    "### Component 6.1: Creating object detection results table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fd0dc10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component()\n",
    "def create_obj_detection_results_table():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d5afd5",
   "metadata": {},
   "source": [
    "### Component 6.2: Performing object detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7ce60b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component()\n",
    "def obj_detection_predict():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cdeaa74",
   "metadata": {},
   "source": [
    "### Component 6.3: Storing object detection results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d940befc",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component()\n",
    "def store_obj_detection_results():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51613711",
   "metadata": {},
   "source": [
    "## Creating and Compiling the Vertex AI Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "9ac6cad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@kfp.dsl.pipeline(name=PIPELINE_NAME, \n",
    "                  description='Pipeline that process patents pdf files.',\n",
    "                  pipeline_root=PIPELINE_ROOT)\n",
    "\n",
    "def pipeline():\n",
    "    # Preprocessing pipeline:\n",
    "    perform_ocr_on_pdfs_task = perform_ocr_on_pdfs(\n",
    "    src_path=f\"gs://{BUCKET}/{PDF_BUCKET_PATH}\", \n",
    "    dst_path=f\"gs://{BUCKET}/{PDF_BUCKET_PATH}\",\n",
    "    uuid=UUID,\n",
    "    project=PROJECT)\n",
    "    \n",
    "    transform_pdfs_into_png_task = transform_pdfs_into_png()\n",
    "    transform_pdfs_into_png_task.after(perform_ocr_on_pdfs_task)\n",
    "\n",
    "    create_bq_results_dataset_task = create_bq_results_dataset(project=PROJECT, dataset_id=RES_DATASET_ID)\n",
    "    create_bq_results_dataset_task.after(transform_pdfs_into_png_task)\n",
    "    \n",
    "    # Text classification pipeline:\n",
    "    create_text_class_results_table_task = create_text_class_results_table(project=PROJECT, \n",
    "                                                                           dataset_id=RES_DATASET_ID, \n",
    "                                                                           table_id=TCN_RESTABLE_NAME, \n",
    "                                                                           schema=TCN_RESTABLE_SCHEMA)\n",
    "    create_text_class_results_table_task.after(create_bq_results_dataset_task)\n",
    "    \n",
    "    text_class_predict_task = text_class_predict()\n",
    "    text_class_predict_task.after(create_text_class_results_table_task)\n",
    "    \n",
    "    store_text_class_results_task = store_text_class_results()\n",
    "    store_text_class_results_task.after(text_class_predict_task)\n",
    "    \n",
    "    # Image classification pipeline:\n",
    "    create_img_class_results_table_task = create_img_class_results_table()\n",
    "    create_img_class_results_table_task.after(create_bq_results_dataset_task)\n",
    "    \n",
    "    img_class_predict_task = img_class_predict()\n",
    "    img_class_predict_task.after(create_img_class_results_table_task)\n",
    "    \n",
    "    store_img_class_results_task = store_img_class_results()\n",
    "    store_img_class_results_task.after(img_class_predict_task)\n",
    "        \n",
    "    # Object detection pipeline:\n",
    "    create_obj_detection_results_table_task = create_obj_detection_results_table()\n",
    "    create_obj_detection_results_table_task.after(create_bq_results_dataset_task)\n",
    "    \n",
    "    obj_detection_predict_task = obj_detection_predict()\n",
    "    obj_detection_predict_task.after(create_obj_detection_results_table_task)\n",
    "    \n",
    "    store_obj_detection_results_task = store_obj_detection_results()\n",
    "    store_obj_detection_results_task.after(obj_detection_predict_task)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "afd3ce60",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(LOCAL_PIPELINE_PATH):\n",
    "    os.mkdir(LOCAL_PIPELINE_PATH)\n",
    "\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=pipeline,\n",
    "    package_path=LOCAL_PIPELINE_JSON\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eedb4ee",
   "metadata": {},
   "source": [
    "## Launching the Vertex AI Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "43330fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating an API client object:\n",
    "# TODO: use the new Vertex AI.\n",
    "api_client = AIPlatformClient(\n",
    "    project_id=PROJECT,\n",
    "    region=REGION,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "3677dbe3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "See the Pipeline job <a href=\"https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/process-pdf-patents-20210819135111?project=qwiklabs-gcp-00-373ac55d0e0a\" target=\"_blank\" >here</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = api_client.create_run_from_job_spec(\n",
    "    LOCAL_PIPELINE_JSON,\n",
    "    pipeline_root=f\"{PIPELINE_ROOT}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b0c135",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-5.m75",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-5:m75"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
