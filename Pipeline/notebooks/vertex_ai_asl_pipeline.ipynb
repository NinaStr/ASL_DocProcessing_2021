{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57edf720",
   "metadata": {},
   "source": [
    "# Prepare pdfs for later in pipeline (Obj Det, Img, text, NER)\n",
    "\n",
    "- user provides\n",
    "    - Google Cloud project (input)\n",
    "    - bucket in GCS of pdfs (input)\n",
    "    - BQ dataset to write prediction results (output)\n",
    "        - BQ table: aggregated results (pdf_name, icn_pred, objdet_pred(coords), text_cn, ner1, ner2, ...., ner)\n",
    "            created with JOIN on pdf_name\n",
    "        - BQ table: icn_preds (pdf_name, icn_pred)    --> this table is made in icn_predict.ipynb\n",
    "        - BQ table: objdet_pred (pdf_name, objdet_pred(coords)) --> this table is made in objdet_predict.ipynb\n",
    "        - BQ table: text_cn (pdf_name, text_cn)    --> this table is made in text_cn_predict.ipynb\n",
    "        - BQ table: ner (pdf_name, ner1, ner2, ...., ner)\n",
    "        \n",
    "- see utils.py for utils functions\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7b5443",
   "metadata": {},
   "source": [
    "Steps: \n",
    " 1. convert pdf to png and write to bucket (for ICN, ObjDet)\n",
    " 2. do ocr on pdf and write to bucket \n",
    " 3. create dataset \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "id": "e5bb84c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT = !gcloud config get-value project # returns SList\n",
    "PROJECT = PROJECT[0] # gets first element in list -> str\n",
    "REGION = \"us-central1\"  \n",
    "MODEL_RESOURCE_NAME = \"2393478483993952256\"\n",
    "\n",
    "import os\n",
    "os.environ[\"PROJECT\"] = PROJECT\n",
    "os.environ[\"REGION\"] = REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "id": "ddd0fc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "bq = bigquery.Client(project=PROJECT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "id": "c87e669f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "id": "373b1c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "from google.cloud import vision\n",
    "from google.cloud import aiplatform\n",
    "import tempfile\n",
    "\n",
    "\n",
    "\n",
    "from importlib import reload\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# for jupyter only\n",
    "import logging\n",
    "reload(logging)\n",
    "logging.basicConfig(format='%(asctime)s %(levelname)s:%(message)s', level=logging.DEBUG, datefmt='%I:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "id": "0a974577",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdf2image import convert_from_path\n",
    "import io\n",
    "import base64\n",
    "import cv2\n",
    "from datetime import datetime\n",
    "import time\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "id": "d5c218d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05:14:48 INFO:test if logging works\n"
     ]
    }
   ],
   "source": [
    "logging.info(\"test if logging works\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2da075",
   "metadata": {},
   "source": [
    "# Labeled Patents - Vertex AI Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05556d4",
   "metadata": {},
   "source": [
    "## Importing Auxiliary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9579bf80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kubeflow pipelines version: 1.7.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "from typing import NamedTuple\n",
    "\n",
    "#!pip install --upgrade kfp\n",
    "import kfp\n",
    "from google.cloud import aiplatform\n",
    "from kfp.v2 import compiler\n",
    "from kfp.v2.dsl import component, Input, Artifact\n",
    "from kfp.v2.google import experimental\n",
    "from kfp.v2.google.client import AIPlatformClient\n",
    "\n",
    "print('Kubeflow pipelines version: {}'.format(kfp.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d67fc0",
   "metadata": {},
   "source": [
    "## Setting Notebook Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c618c434",
   "metadata": {},
   "outputs": [],
   "source": [
    "UUID = datetime.now().strftime('%y%m%d_%H%M%S') #str\n",
    "PROJECT = 'qwiklabs-gcp-00-373ac55d0e0a'\n",
    "REGION = 'us-central1'\n",
    "\n",
    "BUCKET = 'patents_pipetest'\n",
    "PDF_BUCKET_PATH = 'pdf'\n",
    "\n",
    "RES_DATASET_NAME = 'docprocessing_' + UUID\n",
    "RES_DATASET_ID = f'{PROJECT}.{RES_DATASET_NAME}'\n",
    "\n",
    "TCN_MODEL_NAME = '2393478483993952256'\n",
    "TCN_RESTABLE_NAME = f'{RES_DATASET_ID}.tcn'\n",
    "TCN_RESTABLE_SCHEMA = \"\"\"\n",
    "[\n",
    " {\"name\": \"file\", \"field_type\": \"STRING\", \"mode\": \"REQUIRED\", \"description\": \"File path.\"},\n",
    " {\"name\": \"subject\", \"field_type\": \"STRING\", \"mode\": \"REQUIRED\", \"description\": \"Predicted class.\"},\n",
    " {\"name\": \"score\", \"field_type\": \"STRING\", \"mode\": \"REQUIRED\", \"description\": \"Confidence of the prediction.\"}\n",
    "]\n",
    "\"\"\"\n",
    "\n",
    "ICN_MODEL_NAME = '8925034949820547072'\n",
    "ICN_ENDPT_NAME = ''\n",
    "ICN_RESTABLE_NAME = f'{RES_DATASET_ID}.icn'\n",
    "ICN_RESTABLE_SCHEMA = \"\"\"\n",
    "[\n",
    " {\"name\":  \"file\", \"field_type\": \"STRING\", \"mode\": \"REQUIRED\", \"description\": \"File path.\"},\n",
    " {\"name\": \"label\", \"field_type\": \"STRING\", \"mode\": \"REQUIRED\", \"description\": \"Predicted class.\"},\n",
    " {\"name\": \"score\", \"field_type\": \"STRING\", \"mode\": \"REQUIRED\", \"description\": \"Confidence of the prediction.\"}\n",
    "]\n",
    "\"\"\"\n",
    "\n",
    "ODM_MODEL_NAME = '3409814256151953408'\n",
    "ODM_ENDPT_NAME = '2074030773706424320'\n",
    "ODM_RESTABLE_NAME = f'{RES_DATASET_ID}.odm'\n",
    "ODM_RESTABLE_SCHEMA = \"\"\"\n",
    "[\n",
    " {\"name\": \"file\",  \"field_type\": \"STRING\", \"mode\": \"REQUIRED\", \"description\": \"File path.\"},\n",
    " {\"name\": \"label\", \"field_type\": \"STRING\", \"mode\": \"REQUIRED\", \"description\": \"Predicted class.\"},\n",
    " {\"name\": \"score\", \"field_type\":  \"FLOAT\", \"mode\": \"REQUIRED\", \"description\": \"Confidence of the prediction.\"},\n",
    " {\"name\": \"xmin\",  \"field_type\":  \"FLOAT\", \"mode\": \"REQUIRED\", \"description\": \"X coordinate of the top left corner.\"},\n",
    " {\"name\": \"xmax\",  \"field_type\":  \"FLOAT\", \"mode\": \"REQUIRED\", \"description\": \"Y coordinate of the top left corner.\"},\n",
    " {\"name\": \"ymin\",  \"field_type\":  \"FLOAT\", \"mode\": \"REQUIRED\", \"description\": \"X coordinate of the bottom right corner.\"},\n",
    " {\"name\": \"ymax\",  \"field_type\":  \"FLOAT\", \"mode\": \"REQUIRED\", \"description\": \"Y coordinate of the bottom right corner.\"}\n",
    "]\n",
    "\"\"\"\n",
    "\n",
    "src_path = \"gs://2021_08_16_tcn_dev\"\n",
    "dst_path = \"gs://2021_08_16_tcn_dev\"\n",
    "\n",
    "\n",
    "\n",
    "PIPELINE_NAME = 'process-pdf-patents-nina'\n",
    "PIPELINE_ROOT = f\"gs://{BUCKET}/labeled_patents/pipeline_root\"\n",
    "LOCAL_PIPELINE_PATH = './vertex_pipelines'\n",
    "LOCAL_PIPELINE_JSON = os.path.join(LOCAL_PIPELINE_PATH, 'labeled_patents_pipeline2.json')\n",
    "\n",
    "RESULTS_BQ_DATASET='demo_dataset'\n",
    "RESULTS_OBJDET_TABLE='objdet'\n",
    "\n",
    "\n",
    "\n",
    "MODEL_DISPLAY_NAME=f\"labpat_model\"\n",
    "MACHINE_TYPE=\"n1-standard-16\"\n",
    "REPLICA_COUNT=1\n",
    "DOCKER_IMAGE_URI_CREATE_BQDATASET=\"us-docker.pkg.dev/vertex-ai/training/tf-cpu.2-3:latest\"\n",
    "\n",
    "\n",
    "os.environ['PROJECT'] = PROJECT\n",
    "os.environ['REGION'] = REGION\n",
    "os.environ['BUCKET'] = BUCKET \n",
    "os.environ['PDF_BUCKET_PATH'] = PDF_BUCKET_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb10568",
   "metadata": {},
   "source": [
    "**Copying some demo files into the Bucket**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "id": "ff553c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !gsutil -m cp gs://2021_08_16_tcn_dev/*.pdf gs://$BUCKET/$PDF_BUCKET_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da41e7e3",
   "metadata": {},
   "source": [
    "# Create new Docker image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "id": "127546d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated property [core/account].\n"
     ]
    }
   ],
   "source": [
    "# !gcloud config set account student-04-1e37ebc5f596@qwiklabs.net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "id": "bfea8deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !gcloud auth login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "id": "a6cbd113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !cat base_image/Dockerfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "id": "89e581c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BASE_IMAGE='gcr.io/{}/{}:{}'.format(PROJECT, IMAGE_NAME, TAG)\n",
    "# print(BASE_IMAGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "id": "55f8a58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMAGE_NAME='pdf_to_png_image'\n",
    "# TAG='latest'\n",
    "# BASE_IMAGE='gcr.io/qwiklabs-gcp-00-373ac55d0e0a/pdf_to_png_image:latest'\n",
    "# gcloud builds submit --timeout 15m --tag $BASE_IMAGE base_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48cf7fd",
   "metadata": {},
   "source": [
    "## Defining Vertex AI Components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e9d694",
   "metadata": {},
   "source": [
    "### Component 1: Performing OCR on PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a050de6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(packages_to_install=['google-cloud-storage',  'google-cloud-vision'])\n",
    "def perform_ocr_on_pdfs(src_path: str, \n",
    "                        dst_path: str,\n",
    "                        uuid: str,\n",
    "                        project: str):\n",
    "    \n",
    "    # IMPORTS:\n",
    "    import os\n",
    "    import logging\n",
    "    import traceback as tb\n",
    "    import time\n",
    "    import json\n",
    "    from pathlib import Path\n",
    "    from google.cloud import storage\n",
    "    from google.cloud import vision\n",
    "    # from google.cloud import aiplatform\n",
    "\n",
    "    \n",
    "    # AUXILIARY FUNCTIONS:\n",
    "    def to_trace_str(e):\n",
    "        return ''.join(tb.format_exception(None, e, e.__traceback__))   \n",
    "    \n",
    "    \n",
    "    def dismantle_path(gcs_path):\n",
    "        parts = Path(gcs_path).parts\n",
    "        bucket_idx = 1 if parts[0].startswith(\"gs\") else 0\n",
    "        filename_idx = -1 if \".\" in parts[-1] else None\n",
    "\n",
    "        bucket_name = parts[bucket_idx]\n",
    "        filename = parts[filename_idx] if filename_idx else \"\"\n",
    "        directory = \"/\".join(parts[bucket_idx:filename_idx] if filename_idx else parts[bucket_idx+1:])\n",
    "        return bucket_name, directory, filename\n",
    "    \n",
    "    \n",
    "    def ocr(src_path, dst_path, project):\n",
    "        \"\"\"Perform optical character recognition in pdf files.\n",
    "        \n",
    "        Args\n",
    "            src_path\n",
    "            dst_path\n",
    "        \n",
    "        Returns\n",
    "            google.api_core.operation.Operation\n",
    "            To check if done use method .done()\n",
    "            \n",
    "        Link to documentation:  \n",
    "            https://googleapis.dev/python/vision/latest/vision_v1/types.html#google.cloud.vision_v1.types.OutputConfig\n",
    "            https://cloud.google.com/vision/docs/pdf\n",
    "        \n",
    "        \"\"\"\n",
    "        try:\n",
    "            logging.info(\"started optical character recognition\")\n",
    "            \n",
    "            src_bucket_name, src_directory, _ = dismantle_path(src_path)\n",
    "            dst_bucket_name, dst_directory, _ = dismantle_path(dst_path)\n",
    "            \n",
    "            storage_client = storage.Client(project=project)\n",
    "            src_bucket = storage_client.bucket(src_bucket_name)\n",
    "            dst_bucket = storage_client.bucket(dst_bucket_name)\n",
    "            \n",
    "            logging.info(f\"src_bucket_name {src_bucket_name}, src_directory {src_directory}\")\n",
    "\n",
    "            blob_list = [blob for blob in list(src_bucket.list_blobs()) if \\\n",
    "                         os.path.basename(src_directory) in blob.name and \\\n",
    "                         blob.name.endswith(\".pdf\")]\n",
    "            \n",
    "            logging.info(f\"found {len(blob_list)} pdf files in bucket {src_bucket_name}\")\n",
    "\n",
    "            client = vision.ImageAnnotatorClient()\n",
    "            feature = vision.Feature(type_=vision.Feature.Type.DOCUMENT_TEXT_DETECTION)\n",
    "            \n",
    "            operations = []\n",
    "            async_requests = []\n",
    "            \n",
    "            for b_idx, blob in enumerate(blob_list):\n",
    "                # start ocr with gcs_source_uri patents_pipetest/pdf/us_076.pdf, and gcs_destination_uri patents_pipetest/pdf/us_076.pdf\n",
    "                gcs_source_uri = \"gs://\" +  os.path.join(src_bucket_name, blob.name)\n",
    "                gcs_destination_uri = \"gs://\" + os.path.join(dst_bucket_name, blob.name)\n",
    "                \n",
    "                \n",
    "                \n",
    "                logging.info(f\"start ocr with gcs_source_uri {gcs_source_uri}, and gcs_destination_uri {gcs_destination_uri}\")\n",
    "\n",
    "                # source\n",
    "                gcs_source = vision.GcsSource(uri=gcs_source_uri)\n",
    "                input_config = vision.InputConfig(gcs_source=gcs_source, mime_type='application/pdf')\n",
    "\n",
    "                # destination\n",
    "                gcs_destination = vision.GcsDestination(uri=gcs_destination_uri)\n",
    "                output_config = vision.OutputConfig(gcs_destination=gcs_destination, batch_size=1)\n",
    "\n",
    "                logging.info(f\"started ocr for {b_idx} of {len(blob_list)} files\")\n",
    "                async_request = vision.AsyncAnnotateFileRequest(\n",
    "                    features=[feature], \n",
    "                    input_config=input_config,\n",
    "                    output_config=output_config\n",
    "                )\n",
    "                async_requests.append(async_request)\n",
    "\n",
    "            operation = client.async_batch_annotate_files(requests=async_requests)\n",
    "            return operation\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in method ocr: {to_trace_str(e)}\")\n",
    "            \n",
    "            \n",
    "    def create_text_files(gcs_path, project):\n",
    "        try:\n",
    "            # init bucket\n",
    "            bucket_name, directory, _ = dismantle_path(gcs_path)\n",
    "            storage_client = storage.Client(project=project)\n",
    "            bucket = storage_client.bucket(bucket_name)\n",
    "            blob_list = [blob for blob in list(bucket.list_blobs()) if \\\n",
    "                             os.path.basename(directory) in blob.name and \\\n",
    "                             blob.name.endswith(\"output-1-to-1.json\")]\n",
    "            \n",
    "            for b_idx, blob in enumerate(blob_list):\n",
    "                logging.info(f\"creating {b_idx+1} of {len(blob_list)} text files\")\n",
    "                json_string = blob.download_as_string()\n",
    "                response = json.loads(json_string)\n",
    "                text = response['responses'][0]['fullTextAnnotation']['text'] \n",
    "                txt_path = blob.name.replace(\"output-1-to-1.json\", \".txt\")\n",
    "                text_blob = bucket.blob(txt_path)\n",
    "                logging.info(f\"uploaded {b_idx+1} of {len(blob_list)} text files. Path: gs://{bucket_name}/{txt_path}\")\n",
    "                text_blob.upload_from_string(text)\n",
    "                \n",
    "            logging.info(\"finished creating text files\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in method create_text_files: {to_trace_str(e)}\") \n",
    "            \n",
    "    def get_extension(mime_type):\n",
    "        if mime_type == \"text/plain\":\n",
    "            return \".txt\"\n",
    "        elif mime_type == \"image/png\":\n",
    "            return \".png\"\n",
    "        else:\n",
    "            return \".txt\"\n",
    "    \n",
    "    def create_jsonl(gcs_path, mime_type, filename, project):\n",
    "        \"\"\"create jsonl out of files in bucket\n",
    "        \n",
    "        Args\n",
    "            gcs_path (str): bucket or dir where files are located\n",
    "            mime_type (str): the files mimetype \n",
    "            filename (str): the jsonl filename\n",
    "        \n",
    "        Returns\n",
    "            full path of jsonl\n",
    "        \"\"\"\n",
    "        try:\n",
    "            bucket_name, directory, _ = dismantle_path(gcs_path)\n",
    "            storage_client = storage.Client(project=project)\n",
    "            bucket = storage_client.bucket(bucket_name)\n",
    "            extension = get_extension(mime_type)\n",
    "\n",
    "            blob_list = [blob for blob in list(bucket.list_blobs()) if \\\n",
    "                             os.path.basename(directory) in blob.name and \\\n",
    "                             blob.name.endswith(extension)]\n",
    "\n",
    "            jsonl_content = \"\"\n",
    "\n",
    "            for b_idx, blob in enumerate(blob_list):\n",
    "                full_path = os.path.join(gcs_path,blob.name)\n",
    "\n",
    "                d = json.dumps(\n",
    "                    {\n",
    "                    \"content\": full_path,\n",
    "                    \"mimeType\": mime_type\n",
    "                    }\n",
    "                )+\"\\n\"\n",
    "\n",
    "                jsonl_content = jsonl_content+d\n",
    "\n",
    "\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            bucket.blob(file_path).upload_from_string(jsonl_content)\n",
    "            logging.info(f\"uploaded jsonl {file_path} to bucket {bucket_name}. Full path: gs://{os.path.join(bucket_name,file_path)}\")\n",
    "\n",
    "        \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in jsonl creation: {to_trace_str(e)}\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # PIPELINE COMPONENT MAIN CODE:\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logging.info(f\"Starting the processing of pdfs with the OCR functionality of Google Vision API.\")\n",
    "    \n",
    "        \n",
    "    # save everything in the same bucket\n",
    "    jsonl_filename_tcn = f\"tcn_{uuid}.jsonl\"\n",
    "        \n",
    "    # create ocr\n",
    "    ocr_operation = ocr(src_path, dst_path, project)\n",
    "        \n",
    "    while not ocr_operation.done():\n",
    "        logging.info(\"wait for ocr to finish\")\n",
    "        time.sleep(5)\n",
    "\n",
    "    create_text_files(src_path, project)\n",
    "    create_jsonl(gcs_path=dst_path, mime_type=\"text/plain\", filename=jsonl_filename_tcn, project=project)\n",
    "    \n",
    "    # return path where jsonl with .txt files is saved\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a02d4c",
   "metadata": {},
   "source": [
    "### Component 2: PDF to PNG conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3ff6438f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"gcr.io/qwiklabs-gcp-00-373ac55d0e0a/pdf_to_png_image:latest\",\n",
    "    packages_to_install=['google-cloud-storage', 'pdf2image', 'opencv-contrib-python', 'numpy']\n",
    ")\n",
    "def transform_pdfs_into_png(src_path:str, \n",
    "                            dst_path:str,\n",
    "                            uuid: str,\n",
    "                            project:str):\n",
    "    \n",
    "    import os\n",
    "    import logging\n",
    "    import traceback as tb\n",
    "    import time\n",
    "    from pdf2image import convert_from_path\n",
    "    import io\n",
    "    import base64\n",
    "    import cv2\n",
    "    import tempfile\n",
    "    import numpy as np\n",
    "    \n",
    "    from pathlib import Path\n",
    "    from google.cloud import storage\n",
    "    \n",
    "    \n",
    "    \n",
    "    def to_trace_str(e):\n",
    "        return ''.join(tb.format_exception(None, e, e.__traceback__))   \n",
    "    \n",
    "    def get_extension(mime_type):\n",
    "        if mime_type == \"text/plain\":\n",
    "            return \".txt\"\n",
    "        elif mime_type == \"image/png\":\n",
    "            return \".png\"\n",
    "        else:\n",
    "            return \".txt\"\n",
    "    \n",
    "    def dismantle_path(gcs_path):\n",
    "        parts = Path(gcs_path).parts\n",
    "        bucket_idx = 1 if parts[0].startswith(\"gs\") else 0\n",
    "        filename_idx = -1 if \".\" in parts[-1] else None\n",
    "\n",
    "        bucket_name = parts[bucket_idx]\n",
    "        filename = parts[filename_idx] if filename_idx else \"\"\n",
    "        directory = \"/\".join(parts[bucket_idx:filename_idx] if filename_idx else parts[bucket_idx+1:])\n",
    "        return bucket_name, directory, filename\n",
    "    \n",
    "    def convert_pdf_to_png(src_path, dst_path, project):\n",
    "        \"\"\"Takes pdfs from src_bucket_name and transforms them into png. Then it saves the result in dst_bucket_name\"\"\"\n",
    "        try:\n",
    "            logging.info(\"started conversion pdf -> png\")\n",
    "            \n",
    "            storage_client = storage.Client(project=project)\n",
    "        \n",
    "            src_bucket_name, src_directory, _ = dismantle_path(src_path)\n",
    "            dst_bucket_name, dst_directory, _ = dismantle_path(dst_path)\n",
    "            \n",
    "            src_bucket = storage_client.bucket(src_bucket_name)\n",
    "            dst_bucket = storage_client.bucket(dst_bucket_name)\n",
    "\n",
    "            blob_list = [blob for blob in list(src_bucket.list_blobs()) if \\\n",
    "                         os.path.basename(src_directory) in blob.name and \\\n",
    "                         blob.name.endswith(\".pdf\")]\n",
    "\n",
    "            encoded_img_lst = []\n",
    "            imgs = []\n",
    "            logging.info(f\"found {len(blob_list)} pdfs in bucket  {src_bucket_name}\")\n",
    "\n",
    "            for b_idx, blob in enumerate(blob_list):\n",
    "                _, tmp_pdf = tempfile.mkstemp()\n",
    "                blob.download_to_filename(tmp_pdf)\n",
    "                logging.info(f\"downloaded {b_idx+1} of {len(blob_list)} files\")\n",
    "                image = convert_from_path(tmp_pdf)\n",
    "                logging.info(f\"converted {b_idx+1} of {len(blob_list)} images\")\n",
    "                image = image[0]                # Only the firs page is going to be analyzed.\n",
    "                image = np.array(image)\n",
    "                is_success, im_buf_arr = cv2.imencode(\".png\", image)\n",
    "                byte_im = im_buf_arr.tobytes()\n",
    "                filename = os.path.join(dst_directory, os.path.basename(blob.name)+\".png\")\n",
    "                \n",
    "                dst_bucket.blob(filename).upload_from_string(byte_im)\n",
    "                logging.info(f\"uploading png {b_idx+1} of {len(blob_list)} to gs://{dst_bucket_name}/{filename}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in method convert_pdf_to_png: {to_trace_str(e)}\")\n",
    "            return False\n",
    "        \n",
    "    def create_jsonl(gcs_path, mime_type, filename, project):\n",
    "        \"\"\"create jsonl out of files in bucket\n",
    "        \n",
    "        Args\n",
    "            gcs_path (str): bucket or dir where files are located\n",
    "            mime_type (str): the files mimetype \n",
    "            filename (str): the jsonl filename\n",
    "        \n",
    "        Returns\n",
    "            full path of jsonl\n",
    "        \"\"\"\n",
    "        try:\n",
    "            bucket_name, directory, _ = dismantle_path(gcs_path)\n",
    "            storage_client = storage.Client(project=project)\n",
    "            bucket = storage_client.bucket(bucket_name)\n",
    "            extension = get_extension(mime_type)\n",
    "\n",
    "            blob_list = [blob for blob in list(bucket.list_blobs()) if \\\n",
    "                             os.path.basename(directory) in blob.name and \\\n",
    "                             blob.name.endswith(extension)]\n",
    "\n",
    "            jsonl_content = \"\"\n",
    "\n",
    "            for b_idx, blob in enumerate(blob_list):\n",
    "                full_path = os.path.join(gcs_path,blob.name)\n",
    "\n",
    "                d = json.dumps(\n",
    "                    {\n",
    "                    \"content\": full_path,\n",
    "                    \"mimeType\": mime_type\n",
    "                    }\n",
    "                )+\"\\n\"\n",
    "\n",
    "                jsonl_content = jsonl_content+d\n",
    "\n",
    "\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            bucket.blob(file_path).upload_from_string(jsonl_content)\n",
    "            logging.info(f\"uploaded jsonl {file_path} to bucket {bucket_name}. Full path: gs://{os.path.join(bucket_name,file_path)}\")\n",
    "\n",
    "        \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in jsonl creation: {to_trace_str(e)}\")\n",
    "    \n",
    "    \n",
    "    # Main\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logging.info(f\"Starting the processing of pdfs to png.\")\n",
    "    \n",
    "    jsonl_filename_icn = f\"icn_{uuid}.jsonl\"\n",
    "    \n",
    "    \n",
    "    convert_pdf_to_png(src_path, dst_path, project)\n",
    "    create_jsonl(gcs_path=dst_path, mime_type=\"image/png\", filename=jsonl_filename_icn, project=project)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc720e2",
   "metadata": {},
   "source": [
    "### Component 3: Creating a BigQuery dataset to save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f52e3afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(packages_to_install=['google-cloud-bigquery'])\n",
    "def create_bq_results_dataset(project: str, \n",
    "                              dataset_id: str):\n",
    "    \"\"\"loads csv data in storage to BQ\"\"\"\n",
    "    # Send the dataset to the API for creation, with an explicit timeout.\n",
    "    # Raises google.api_core.exceptions.Conflict if the Dataset already\n",
    "    # exists within the project.\n",
    "    import logging\n",
    "    from google.cloud import bigquery\n",
    "\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logging.info(f\"Starting the creation of a BigQuery dataset to store analyses results.\")\n",
    "\n",
    "    bq = bigquery.Client(project=project)\n",
    "    try:\n",
    "        dataset = bigquery.Dataset(dataset_id)\n",
    "        dataset.location = \"US\"\n",
    "        bq.get_dataset(dataset_id)  # Make an API request.\n",
    "        logging.info(\"Dataset {} already exists\".format(dataset_id))\n",
    "    except Exception as e:\n",
    "        logging.info(\"Dataset {} is not found\".format(dataset_id))\n",
    "        dataset = bq.create_dataset(dataset, timeout=30)  # Make an API request.\n",
    "        dataset.location = \"US\"\n",
    "        logging.info(\"Created dataset {}.{}\".format(bq.project, dataset.dataset_id))\n",
    "    finally:\n",
    "        logging.info(f\"Finished creating or loading dataset {dataset_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec83e76",
   "metadata": {},
   "source": [
    "### Component 4.1: Creating image classification results table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "83f03935",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(packages_to_install=['google-cloud-bigquery'])\n",
    "def create_text_class_results_table(project:str, \n",
    "                                    dataset_id:str, \n",
    "                                    table_id:str, \n",
    "                                    schema:str):\n",
    "    \n",
    "    import ast\n",
    "    from google.cloud import bigquery\n",
    "    import logging\n",
    "    \n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logging.info(f\"Starting the creation of a BQ table to store text classification results.\")\n",
    "    \n",
    "    bq = bigquery.Client(project=project)\n",
    "    \n",
    "    try:\n",
    "        dataset = bigquery.Dataset(dataset_id)\n",
    "        dataset.location = \"US\"\n",
    "        bq.get_dataset(dataset_id)  # Make an API request.\n",
    "        logging.info(\"Dataset {} already exists\".format(dataset_id))\n",
    "    except Exception as e:\n",
    "        logging.info(\"Dataset {} is not found\".format(dataset_id))\n",
    "        dataset = bq.create_dataset(dataset, timeout=30)  # Make an API request.\n",
    "        dataset.location = \"US\"\n",
    "        logging.info(\"Created dataset {}.{}\".format(bq.project, dataset.dataset_id))\n",
    "    finally:\n",
    "        # create table\n",
    "        schema = [bigquery.SchemaField(**dct) for dct in ast.literal_eval(schema)]\n",
    "        table = bigquery.Table(table_id, schema=schema)\n",
    "        table = bq.create_table(table)\n",
    "        logging.info(f\"Created table {table_id}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2312963",
   "metadata": {},
   "source": [
    "### Component 4.2: Performing text classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "36e871d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component()\n",
    "def text_class_predict():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3f8c0f",
   "metadata": {},
   "source": [
    "### Component 4.3: Storing text classification results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "081b4401",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component()\n",
    "def store_text_class_results():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc84e3d",
   "metadata": {},
   "source": [
    "### Component 5.1: Creating image classification results table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f4bfd19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(packages_to_install=['google-cloud-bigquery'])\n",
    "def create_img_class_results_table(project:str, \n",
    "                                    dataset_id:str, \n",
    "                                    table_id:str, \n",
    "                                    schema:str):\n",
    "    \n",
    "    import ast\n",
    "    from google.cloud import bigquery\n",
    "    import logging\n",
    "    \n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logging.info(f\"Starting the creation of a BQ table to store image classification results.\")\n",
    "    \n",
    "    bq = bigquery.Client(project=project)\n",
    "    \n",
    "    try:\n",
    "        dataset = bigquery.Dataset(dataset_id)\n",
    "        dataset.location = \"US\"\n",
    "        bq.get_dataset(dataset_id)  # Make an API request.\n",
    "        logging.info(\"Dataset {} already exists\".format(dataset_id))\n",
    "    except Exception as e:\n",
    "        logging.info(\"Dataset {} is not found\".format(dataset_id))\n",
    "        dataset = bq.create_dataset(dataset, timeout=30)  # Make an API request.\n",
    "        dataset.location = \"US\"\n",
    "        logging.info(\"Created dataset {}.{}\".format(bq.project, dataset.dataset_id))\n",
    "    finally:\n",
    "        # create table\n",
    "        schema = [bigquery.SchemaField(**dct) for dct in ast.literal_eval(schema)]\n",
    "        table = bigquery.Table(table_id, schema=schema)\n",
    "        table = bq.create_table(table)\n",
    "        logging.info(f\"Created table {table_id}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d26b40",
   "metadata": {},
   "source": [
    "### Component 5.2: Performing image classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4fc8cd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component()\n",
    "def img_class_predict():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75b3301",
   "metadata": {},
   "source": [
    "### Component 5.3: Storing image classification results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fc3a67c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component()\n",
    "def store_img_class_results():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed96e18",
   "metadata": {},
   "source": [
    "### Component 6.1: Creating object detection results table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8cf03325",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(packages_to_install=['google-cloud-bigquery'])\n",
    "def create_obj_detection_results_table(project:str, \n",
    "                                       dataset_id:str, \n",
    "                                       table_id:str, \n",
    "                                       schema:str):\n",
    "    \n",
    "    import ast\n",
    "    from google.cloud import bigquery\n",
    "    import logging\n",
    "    \n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logging.info(f\"Starting the creation of a BQ table t store object detection results.\")\n",
    "    \n",
    "    \n",
    "    bq = bigquery.Client(project=project)\n",
    "    \n",
    "    try:\n",
    "        dataset = bigquery.Dataset(dataset_id)\n",
    "        dataset.location = \"US\"\n",
    "        bq.get_dataset(dataset_id)  # Make an API request.\n",
    "        logging.info(\"Dataset {} already exists\".format(dataset_id))\n",
    "    except Exception as e:\n",
    "        logging.info(\"Dataset {} is not found\".format(dataset_id))\n",
    "        dataset = bq.create_dataset(dataset, timeout=30)  # Make an API request.\n",
    "        dataset.location = \"US\"\n",
    "        logging.info(\"Created dataset {}.{}\".format(bq.project, dataset.dataset_id))\n",
    "    finally:\n",
    "        # create table\n",
    "        schema = [bigquery.SchemaField(**dct) for dct in ast.literal_eval(schema)]\n",
    "        table = bigquery.Table(table_id, schema=schema)\n",
    "        table = bq.create_table(table)\n",
    "        logging.info(f\"Created table {table_id}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9f0757",
   "metadata": {},
   "source": [
    "### Component 6.2: Performing object detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "85246378",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(packages_to_install=['google-cloud-bigquery', 'google-cloud-storage',  'google-cloud-aiplatform'])\n",
    "def obj_detection_predict(project: str,\n",
    "                          region: str,\n",
    "                          bucket_name: str,\n",
    "                          img_blob: str,\n",
    "                          objdet_endpoint: str) -> NamedTuple(\"Outputs\", [(\"predictions\", Artifact),]):\n",
    "    \n",
    "    # IMPORTS     \n",
    "    import os\n",
    "    import tempfile\n",
    "    import logging\n",
    "    import traceback as tb\n",
    "    from collections import namedtuple\n",
    "    from google.cloud import bigquery\n",
    "    from google.cloud import storage\n",
    "    from google.cloud import aiplatform\n",
    "    from fnmatch import fnmatch\n",
    "    import base64\n",
    "    from google.cloud.aiplatform.gapic.schema import predict\n",
    "    \n",
    "    \n",
    "    # AUXILIARY LIBRARIES\n",
    "    def get_bucket_file_list(bucket_name, fname_template='*'):\n",
    "        '''!@brief Function that returns the list of files in a bucket.\n",
    "        @param bucket (string) Bucket name.\n",
    "        @param fname_template (string) Template for filtering blob names \n",
    "        that supports Unix shell-style wildcards. For more info: \n",
    "        https://docs.python.org/3/library/fnmatch.html\n",
    "\n",
    "        @return (list of srtings) List of blob names in a bucket which \n",
    "        fullfills template structure.\n",
    "        '''\n",
    "        storage_client = storage.Client()\n",
    "        blobs = storage_client.list_blobs(bucket_name)\n",
    "        blob_lst = [blob.name for blob in blobs]  \n",
    "        file_lst = [fname for fname in blob_lst if fnmatch(fname, fname_template)]\n",
    "\n",
    "        return file_lst\n",
    "    \n",
    "    \n",
    "    def predict_image_classification_sample(\n",
    "        project: str,\n",
    "        endpoint_id: str,\n",
    "        filename: str,\n",
    "        location: str = \"us-central1\",\n",
    "        api_endpoint: str = \"us-central1-aiplatform.googleapis.com\"):\n",
    "        \n",
    "        # The AI Platform services require regional API endpoints.\n",
    "        client_options = {\"api_endpoint\": api_endpoint}\n",
    "        client = aiplatform.gapic.PredictionServiceClient(client_options=client_options)\n",
    "        with open(filename, \"rb\") as f:\n",
    "            file_content = f.read()\n",
    "            print('file: '+ str(file_content))\n",
    "\n",
    "        # The format of each instance should conform to the deployed model's prediction input schema.\n",
    "        encoded_content = base64.b64encode(file_content).decode(\"utf-8\")\n",
    "        print('img encoded: '+ str(encoded_content))\n",
    "        instance = predict.instance.ImageObjectDetectionPredictionInstance(content=encoded_content).to_value()\n",
    "        instances = [instance]\n",
    "        parameters = predict.params.ImageObjectDetectionPredictionParams(confidence_threshold=0.5, max_predictions=5).to_value()\n",
    "        endpoint = client.endpoint_path(project=project, location=location, endpoint=endpoint_id)\n",
    "        response = client.predict(endpoint=endpoint, instances=instances, parameters=parameters)\n",
    "        predictions = response.predictions\n",
    "        return [dict(prediction) for prediction in predictions]\n",
    "    \n",
    "\n",
    "    # MAIN BODY:    \n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logging.info(f\"Starting the object detection task.\")\n",
    "    \n",
    "    client = bigquery.Client()\n",
    "    files = get_bucket_file_list(bucket_name=f'{bucket_name}',\n",
    "                                 fname_template=img_blob+'*')\n",
    "    logging.info(str(files))\n",
    "    predictions = []\n",
    "    for file in files:             \n",
    "        # Downloading the file as a temporal file:\n",
    "        storage_client = storage.Client()\n",
    "        bucket = storage_client.bucket(bucket_name)\n",
    "        blob = bucket.blob(file)\n",
    "        _, path = tempfile.mkstemp()\n",
    "        blob.download_to_filename(path + '.png')    \n",
    "        \n",
    "        #print(str(file))\n",
    "        \n",
    "        # Obtaining online prediction:\n",
    "        preds = predict_image_classification_sample(project=project,\n",
    "                                                    endpoint_id=objdet_endpoint,\n",
    "                                                    filename=f'{path}.png',\n",
    "                                                    location=region,\n",
    "                                                    api_endpoint='us-central1-aiplatform.googleapis.com')\n",
    "    \n",
    "        print(str(preds))\n",
    "        \n",
    "        # Parsing prediction:\n",
    "        objdet_pred = preds[0]['displayNames'][0]\n",
    "        objdet_confidence = preds[0]['confidences'][0]\n",
    "        objdet_xmin, objdet_xmax = preds[0]['bboxes'][0][0], preds[0]['bboxes'][0][1]\n",
    "        objdet_ymin, objdet_ymax = preds[0]['bboxes'][0][2], preds[0]['bboxes'][0][3]\n",
    "        \n",
    "        # Storing prediction into the BQ table:\n",
    "        predictions.append(\n",
    "            {'file': f'{file}'.split('/')[-1],\n",
    "             'label': f'{objdet_pred}',\n",
    "             'score': f'{objdet_confidence}',\n",
    "             'xmin': f'{objdet_xmin}',\n",
    "             'xmax': f'{objdet_xmax}',\n",
    "             'ymin': f'{objdet_ymin}',\n",
    "             'ymax': f'{objdet_ymax}'}\n",
    "        )\n",
    "\n",
    "        logging.info(str(predictions))\n",
    "        \n",
    "    logging.info(f\"The object detection task has finished successfully .\")    \n",
    "    \n",
    "    # Creating the named tuple with the results:\n",
    "    outputs = namedtuple('Outputs',\n",
    "                         ['predictions'])\n",
    "    \n",
    "    return outputs(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447b2706",
   "metadata": {},
   "source": [
    "### Component 6.3: Storing object detection results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d8eadb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(packages_to_install=['google-cloud-bigquery'])\n",
    "def store_obj_detection_results(table_id: str,\n",
    "                                preds: Input[Artifact]):\n",
    "    \n",
    "    import logging\n",
    "    import ast\n",
    "    from google.cloud import bigquery\n",
    "    \n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logging.info(f\"Starting the storage of the object detection results.\")\n",
    "    \n",
    "    client = bigquery.Client()\n",
    "    \n",
    "    # Parsing the artifact:\n",
    "    with open(preds.path, \"r\") as preds_file:\n",
    "        contents = preds_file.read()\n",
    "        print(f\"generic contents: {contents}\")\n",
    "        print(type(contents))\n",
    "        \n",
    "        predictions = ast.literal_eval(contents)\n",
    "\n",
    "        for prediction in predictions:\n",
    "            errors = client.insert_rows_json(table_id, [prediction])\n",
    "            if errors == []:\n",
    "                logging.info(\"New row have been added.\")\n",
    "            else:\n",
    "                logging.info(\"Encountered errors while inserting rows: {}\".format(errors))\n",
    "\n",
    "            logging.info(f\"The object detection results have been stored successfully.\")@component()\n",
    "def store_obj_detection_results():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f50e23",
   "metadata": {},
   "source": [
    "## Creating and Compiling the Vertex AI Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d61543ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "@kfp.dsl.pipeline(name=PIPELINE_NAME, \n",
    "                  description='Pipeline that process patents pdf files.',\n",
    "                  pipeline_root=PIPELINE_ROOT)\n",
    "\n",
    "def pipeline():\n",
    "    # Preprocessing pipeline:\n",
    "#     perform_ocr_on_pdfs_task = perform_ocr_on_pdfs(\n",
    "#     src_path=f\"gs://{BUCKET}/{PDF_BUCKET_PATH}\", \n",
    "#     dst_path=f\"gs://{BUCKET}/{PDF_BUCKET_PATH}\",\n",
    "#     uuid=UUID,\n",
    "#     project=PROJECT)\n",
    "    \n",
    "    transform_pdfs_into_png_task = transform_pdfs_into_png(src_path=f\"gs://{BUCKET}/{PDF_BUCKET_PATH}\", \n",
    "                                                           dst_path=f\"gs://{BUCKET}/{PDF_BUCKET_PATH}\",\n",
    "                                                           uuid=UUID,\n",
    "                                                           project=PROJECT)\n",
    "    \n",
    "#     transform_pdfs_into_png_task.after(perform_ocr_on_pdfs_task)\n",
    "\n",
    "    create_bq_results_dataset_task = create_bq_results_dataset(project=PROJECT, dataset_id=RES_DATASET_ID)\n",
    "    create_bq_results_dataset_task.after(transform_pdfs_into_png_task)\n",
    "    \n",
    "#     # Text classification pipeline:\n",
    "#     create_text_class_results_table_task = create_text_class_results_table(project=PROJECT, \n",
    "#                                                                            dataset_id=RES_DATASET_ID, \n",
    "#                                                                            table_id=TCN_RESTABLE_NAME, \n",
    "#                                                                            schema=TCN_RESTABLE_SCHEMA)\n",
    "#     create_text_class_results_table_task.after(create_bq_results_dataset_task)\n",
    "    \n",
    "#     text_class_predict_task = text_class_predict()\n",
    "#     text_class_predict_task.after(create_text_class_results_table_task)\n",
    "    \n",
    "#     store_text_class_results_task = store_text_class_results()\n",
    "#     store_text_class_results_task.after(text_class_predict_task)\n",
    "    \n",
    "#     # Image classification pipeline:\n",
    "#      # Image classification pipeline:\n",
    "#     create_img_class_results_table_task = create_img_class_results_table(project=PROJECT, \n",
    "#                                                                          dataset_id=RES_DATASET_ID, \n",
    "#                                                                          table_id=ICN_RESTABLE_NAME, \n",
    "#                                                                          schema=ICN_RESTABLE_SCHEMA)\n",
    "#     create_img_class_results_table_task.after(create_bq_results_dataset_task)\n",
    "    \n",
    "#     img_class_predict_task = img_class_predict()\n",
    "#     img_class_predict_task.after(create_img_class_results_table_task)\n",
    "    \n",
    "#     store_img_class_results_task = store_img_class_results()\n",
    "#     store_img_class_results_task.after(img_class_predict_task)\n",
    "        \n",
    "#     # Object detection pipeline:\n",
    "#     create_obj_detection_results_table_task = create_obj_detection_results_table(project=PROJECT, \n",
    "#                                                                                  dataset_id=RES_DATASET_ID, \n",
    "#                                                                                  table_id=ODM_RESTABLE_NAME, \n",
    "#                                                                                  schema=ODM_RESTABLE_SCHEMA)\n",
    "#     create_obj_detection_results_table_task.after(create_bq_results_dataset_task)\n",
    "    \n",
    "#     obj_detection_predict_task = obj_detection_predict()\n",
    "#     obj_detection_predict_task.after(create_obj_detection_results_table_task)\n",
    "    \n",
    "#     store_obj_detection_results_task = store_obj_detection_results()\n",
    "#     store_obj_detection_results_task.after(obj_detection_predict_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "032d51f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(LOCAL_PIPELINE_PATH):\n",
    "    os.mkdir(LOCAL_PIPELINE_PATH)\n",
    "\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=pipeline,\n",
    "    package_path=LOCAL_PIPELINE_JSON\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992d11d2",
   "metadata": {},
   "source": [
    "## Launching the Vertex AI Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ea66abbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating an API client object:\n",
    "# TODO: use the new Vertex AI.\n",
    "api_client = AIPlatformClient(\n",
    "    project_id=PROJECT,\n",
    "    region=REGION,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8828d7d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "See the Pipeline job <a href=\"https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/process-pdf-patents-nina-20210820070935?project=qwiklabs-gcp-00-373ac55d0e0a\" target=\"_blank\" >here</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = api_client.create_run_from_job_spec(\n",
    "    LOCAL_PIPELINE_JSON,\n",
    "    pipeline_root=f\"{PIPELINE_ROOT}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069d49da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-5.m76",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-5:m76"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
