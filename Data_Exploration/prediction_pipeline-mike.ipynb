{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60971aea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172c3aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO LIST\n",
    "#1- Change reading files to filter pdfs.\n",
    "#2- Access to the image classification end point\n",
    "#3- Create a JSON input for the endpoint (with the image)\n",
    "#4- Retrieve prediction\n",
    "#5- Store the prediction in a new BQ table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51abb48d",
   "metadata": {},
   "source": [
    "# Online Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c646b691",
   "metadata": {},
   "source": [
    "* input: GCS bucket of png files\n",
    "* output: BQ table with ObjDet preds\n",
    "\n",
    "1. Create the dataset\n",
    "1. Create the objdet table\n",
    "1. List files in bucket\n",
    "2. Iterate over list and call prediciton\n",
    "3. parse predicition into dict\n",
    "4. write row file to BQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "715a514c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://qwiklabs-gcp-00-373ac55d0e0a/labeled_patents/subsample_images/computer_vision_10.png\n",
      "gs://qwiklabs-gcp-00-373ac55d0e0a/labeled_patents/subsample_images/computer_vision_11.png\n",
      "gs://qwiklabs-gcp-00-373ac55d0e0a/labeled_patents/subsample_images/computer_vision_12.png\n",
      "gs://qwiklabs-gcp-00-373ac55d0e0a/labeled_patents/subsample_images/computer_vision_13.png\n",
      "gs://qwiklabs-gcp-00-373ac55d0e0a/labeled_patents/subsample_images/computer_vision_14.png\n",
      "gs://qwiklabs-gcp-00-373ac55d0e0a/labeled_patents/subsample_images/computer_vision_15.png\n",
      "gs://qwiklabs-gcp-00-373ac55d0e0a/labeled_patents/subsample_images/computer_vision_16.png\n",
      "gs://qwiklabs-gcp-00-373ac55d0e0a/labeled_patents/subsample_images/computer_vision_17.png\n",
      "gs://qwiklabs-gcp-00-373ac55d0e0a/labeled_patents/subsample_images/computer_vision_18.png\n",
      "gs://qwiklabs-gcp-00-373ac55d0e0a/labeled_patents/subsample_images/computer_vision_19.png\n",
      "gs://qwiklabs-gcp-00-373ac55d0e0a/labeled_patents/subsample_images/computer_vision_20.png\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls gs://qwiklabs-gcp-00-373ac55d0e0a/labeled_patents/subsample_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ea0631d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using endpoint [https://us-central1-aiplatform.googleapis.com/]\n",
      "MODEL_ID             DISPLAY_NAME\n",
      "2393478483993952256  text_classification\n",
      "3409814256151953408  object_detection_patent_figures\n",
      "8925034949820547072  docprocessing_2021811144149\n",
      "2880236679656898560  hacker_news_titles_automl\n",
      "886021654033989632   mnist_20210802_154025\n",
      "2243012516756062208  babyweight_model_20210730_125424\n",
      "8763802564723474432  babyweight_model_20210730_124945\n",
      "5534440156922118144  babyweight_automl_2021728151029\n",
      "5491655960462098432  pipelines-ModelUpload-20210727125838\n",
      "657604710433292288   taxifare-20210721144351\n"
     ]
    }
   ],
   "source": [
    "!gcloud ai models list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8500218",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import storage\n",
    "from fnmatch import fnmatch\n",
    "\n",
    "import base64\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud.aiplatform.gapic.schema import predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f8a0133",
   "metadata": {},
   "outputs": [
    {
     "ename": "Conflict",
     "evalue": "409 POST https://bigquery.googleapis.com/bigquery/v2/projects/qwiklabs-gcp-00-373ac55d0e0a/datasets?prettyPrint=false: Already Exists: Dataset qwiklabs-gcp-00-373ac55d0e0a:demo_dataset",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConflict\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-eae137eae8db>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbigquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"US\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Make an API request.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Created dataset {}.{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/google/cloud/bigquery/client.py\u001b[0m in \u001b[0;36mcreate_dataset\u001b[0;34m(self, dataset, exists_ok, retry, timeout)\u001b[0m\n\u001b[1;32m    612\u001b[0m                 \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m                 \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 614\u001b[0;31m                 \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    615\u001b[0m             )\n\u001b[1;32m    616\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_api_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapi_response\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/google/cloud/bigquery/client.py\u001b[0m in \u001b[0;36m_call_api\u001b[0;34m(self, retry, span_name, span_attributes, job_ref, **kwargs)\u001b[0m\n\u001b[1;32m    745\u001b[0m                 \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mspan_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattributes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mspan_attributes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjob_ref\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m             ):\n\u001b[0;32m--> 747\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    748\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/google/api_core/retry.py\u001b[0m in \u001b[0;36mretry_wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    289\u001b[0m                 \u001b[0msleep_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_deadline\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 291\u001b[0;31m                 \u001b[0mon_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mon_error\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m             )\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/google/api_core/retry.py\u001b[0m in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, deadline, on_error)\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msleep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msleep_generator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/google/cloud/_http.py\u001b[0m in \u001b[0;36mapi_request\u001b[0;34m(self, method, path, query_params, data, content_type, headers, api_base_url, api_version, expect_json, _target_object, timeout)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    483\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 484\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_http_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    485\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mexpect_json\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mConflict\u001b[0m: 409 POST https://bigquery.googleapis.com/bigquery/v2/projects/qwiklabs-gcp-00-373ac55d0e0a/datasets?prettyPrint=false: Already Exists: Dataset qwiklabs-gcp-00-373ac55d0e0a:demo_dataset"
     ]
    }
   ],
   "source": [
    "PROJECT='qwiklabs-gcp-00-373ac55d0e0a'\n",
    "BQ_DATASET='demo_dataset'\n",
    "OBJDET_TABLE='objdet'\n",
    "\n",
    "client = bigquery.Client()\n",
    "\n",
    "# Create dataset\n",
    "dataset_id = f'{PROJECT}.{BQ_DATASET}'\n",
    "dataset = bigquery.Dataset(dataset_id)\n",
    "dataset.location = \"US\"\n",
    "dataset = client.create_dataset(dataset, timeout=30)  # Make an API request.\n",
    "print(\"Created dataset {}.{}\".format(client.project, dataset.dataset_id))\n",
    "\n",
    "# Create table\n",
    "OBJDET_SCHEMA = [\n",
    "    bigquery.SchemaField('file_name', 'STRING', mode='NULLABLE'),\n",
    "    bigquery.SchemaField('objdet_pred', 'STRING', mode='NULLABLE'),\n",
    "    bigquery.SchemaField('objdet_confidence', 'STRING', mode='NULLABLE'),\n",
    "    bigquery.SchemaField('objdet_xmin', 'FLOAT', mode='NULLABLE'),\n",
    "    bigquery.SchemaField('objdet_xmax', 'FLOAT', mode='NULLABLE'),\n",
    "    bigquery.SchemaField('objdet_ymin', 'FLOAT', mode='NULLABLE'),\n",
    "    bigquery.SchemaField('objdet_ymax', 'FLOAT', mode='NULLABLE')]\n",
    "\n",
    "table_id = f'{PROJECT}.{BQ_DATASET}.{OBJDET_TABLE}'\n",
    "\n",
    "schema = OBJDET_SCHEMA\n",
    "\n",
    "table = bigquery.Table(table_id, schema=schema)\n",
    "table = client.create_table(table)  # Make an API request.\n",
    "print(\n",
    "    \"Created table {}.{}.{}\".format(table.project, table.dataset_id, table.table_id)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fac90af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bucket_file_list(bucket_name, fname_template='*'):\n",
    "    '''!@brief Function that returns the list of files in a bucket.\n",
    "    @param bucket (string) Bucket name.\n",
    "    @param fname_template (string) Template for filtering blob names \n",
    "    that supports Unix shell-style wildcards. For more info: \n",
    "    https://docs.python.org/3/library/fnmatch.html\n",
    "            \n",
    "    @return (list of srtings) List of blob names in a bucket which \n",
    "    fullfills template structure.\n",
    "    '''\n",
    "    # Instantiating client:\n",
    "    storage_client = storage.Client()\n",
    "\n",
    "    # Note: Client.list_blobs requires at least package version 1.17.0.\n",
    "    blobs = storage_client.list_blobs(bucket_name)\n",
    "    \n",
    "    # Listing all the blobs in a bucket:\n",
    "    blob_lst = [blob.name for blob in blobs]\n",
    "\n",
    "    # Filtering blob names with the template format given:  \n",
    "    file_lst = [fname for fname in blob_lst if fnmatch(fname, fname_template)]\n",
    "    \n",
    "    return file_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc453ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_files_from_bucket(bucket_name, file_lst, dest_folder):\n",
    "    '''!@brief Function that downloads a list of files from a bucket.\n",
    "    @param bucket: (string) Bucket name.\n",
    "    @param file_lst: (string) List of files to be downloaded.\n",
    "    @param dest_folder: (string) Folder where files are downloaded.\n",
    "\n",
    "    @return (list of strings) Names of the downloaded files.\n",
    "    '''\n",
    "    # Instantiating client:\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "\n",
    "    # Saving blob into the destination folder:\n",
    "    new_file_lst = []\n",
    "    for fname in file_lst:\n",
    "        # Loading blob:\n",
    "        blob = bucket.blob(fname)\n",
    "        # Saving blob into a filename:\n",
    "        _, name = os.path.split(fname)\n",
    "        new_fname = os.path.join(dest_folder, name)\n",
    "        blob.download_to_filename(new_fname)\n",
    "        new_file_lst.append(new_fname)\n",
    "\n",
    "    # TODO: A check of the downloaded files should be performed!! Maybe is just \n",
    "    # reading the files of the folder since if it is a temporal folder, every time\n",
    "    # the pipeline is executed, the folder is created empty:\n",
    "    #os.listdir(dest_folder) or similar\n",
    "    print('Number of files downloaded: {:d}'.format(len(new_file_lst)))\n",
    "\n",
    "    return new_file_lst\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e36cb5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image_classification_sample(\n",
    "    project: str,\n",
    "    endpoint_id: str,\n",
    "    filename: str,\n",
    "    location: str = \"us-central1\",\n",
    "    api_endpoint: str = \"us-central1-aiplatform.googleapis.com\",\n",
    "):\n",
    "    # The AI Platform services require regional API endpoints.\n",
    "    client_options = {\"api_endpoint\": api_endpoint}\n",
    "    # Initialize client that will be used to create and send requests.\n",
    "    # This client only needs to be created once, and can be reused for multiple requests.\n",
    "    client = aiplatform.gapic.PredictionServiceClient(client_options=client_options)\n",
    "    with open(filename, \"rb\") as f:\n",
    "        file_content = f.read()\n",
    "\n",
    "    # The format of each instance should conform to the deployed model's prediction input schema.\n",
    "    encoded_content = base64.b64encode(file_content).decode(\"utf-8\")\n",
    "    instance = predict.instance.ImageObjectDetectionPredictionInstance(\n",
    "        content=encoded_content,\n",
    "    ).to_value()\n",
    "    instances = [instance]\n",
    "    parameters = predict.params.ImageObjectDetectionPredictionParams(\n",
    "        confidence_threshold=0.5, max_predictions=5,\n",
    "    ).to_value()\n",
    "    endpoint = client.endpoint_path(\n",
    "        project=project, location=location, endpoint=endpoint_id\n",
    "    )\n",
    "    response = client.predict(\n",
    "        endpoint=endpoint, instances=instances, parameters=parameters\n",
    "    )\n",
    "    predictions = response.predictions\n",
    "    return [dict(prediction) for prediction in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "136f348a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'labeled_patents/subsample_images/computer_vision_10.png'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "305296f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: labeled_patents/subsample_images/computer_vision_10.png.\n",
      "/tmp/tmpif_xr_bo\n",
      "New rows have been added.\n",
      "Processing: labeled_patents/subsample_images/computer_vision_11.png.\n",
      "/tmp/tmpckvo81nd\n",
      "New rows have been added.\n",
      "Processing: labeled_patents/subsample_images/computer_vision_12.png.\n",
      "/tmp/tmp6tbcozsj\n",
      "New rows have been added.\n",
      "Processing: labeled_patents/subsample_images/computer_vision_13.png.\n",
      "/tmp/tmpp5jg32ne\n",
      "New rows have been added.\n",
      "Processing: labeled_patents/subsample_images/computer_vision_14.png.\n",
      "/tmp/tmpa4o2sjtq\n",
      "New rows have been added.\n",
      "Processing: labeled_patents/subsample_images/computer_vision_15.png.\n",
      "/tmp/tmp21_c3e22\n",
      "New rows have been added.\n",
      "Processing: labeled_patents/subsample_images/computer_vision_16.png.\n",
      "/tmp/tmpj04ni2ml\n",
      "New rows have been added.\n",
      "Processing: labeled_patents/subsample_images/computer_vision_17.png.\n",
      "/tmp/tmpclwj21yr\n",
      "New rows have been added.\n",
      "Processing: labeled_patents/subsample_images/computer_vision_18.png.\n",
      "/tmp/tmpucms44yk\n",
      "New rows have been added.\n",
      "Processing: labeled_patents/subsample_images/computer_vision_19.png.\n",
      "/tmp/tmp4jjr7i8p\n",
      "New rows have been added.\n",
      "Processing: labeled_patents/subsample_images/computer_vision_20.png.\n",
      "/tmp/tmp3nf96ujn\n",
      "New rows have been added.\n"
     ]
    }
   ],
   "source": [
    "OBJDET_ENDPOINT='2074030773706424320'\n",
    "TMP_DIR='/home/jupyter/ASL_DocProcessing_2021/Data_Exploration/tmp_png/'\n",
    "\n",
    "if not os.path.exists(TMP_DIR):\n",
    "    os.makedirs(TMP_DIR)\n",
    "    \n",
    "files = get_bucket_file_list(bucket_name=f'{PROJECT}',\n",
    "                             fname_template='labeled_patents/subsample_images/*')\n",
    "\n",
    "for file in files:\n",
    "    print(f'Processing: {file}.')\n",
    "    \n",
    "    # Downloading the file as a temporal file:\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(PROJECT)\n",
    "    blob = bucket.blob(file)\n",
    "    _, path = tempfile.mkstemp()\n",
    "    blob.download_to_filename(path + '.png')    \n",
    "    \n",
    "    # Obtaining online prediction:\n",
    "    preds = predict_image_classification_sample(\n",
    "        project=f'{PROJECT}',\n",
    "        endpoint_id=f'{OBJDET_ENDPOINT}',\n",
    "        filename=f'{path}.png',\n",
    "        location='us-central1',\n",
    "        api_endpoint='us-central1-aiplatform.googleapis.com')\n",
    "    \n",
    "    # Parsing prediction:\n",
    "    objdet_pred = preds[0]['displayNames'][0]\n",
    "    objdet_confidence = preds[0]['confidences'][0]\n",
    "    objdet_xmin, objdet_xmax = preds[0]['bboxes'][0][0], preds[0]['bboxes'][0][1]\n",
    "    objdet_ymin, objdet_ymax = preds[0]['bboxes'][0][2], preds[0]['bboxes'][0][3]\n",
    "    \n",
    "    # Storing prediction into the BQ table:\n",
    "    rows_to_insert = [\n",
    "        {'file_name': f'{file}'.split('/')[-1],\n",
    "         'objdet_pred': f'{objdet_pred}',\n",
    "         'objdet_confidence': f'{objdet_confidence}',\n",
    "         'objdet_xmin': f'{objdet_xmin}',\n",
    "         'objdet_xmax': f'{objdet_xmax}',\n",
    "         'objdet_ymin': f'{objdet_ymin}',\n",
    "         'objdet_ymax': f'{objdet_ymax}'}\n",
    "    ]\n",
    "    \n",
    "    table_id = f'{PROJECT}.{BQ_DATASET}.{OBJDET_TABLE}'\n",
    "    \n",
    "    errors = client.insert_rows_json(table_id, rows_to_insert)  # Make an API request.\n",
    "    if errors == []:\n",
    "        print(\"New rows have been added.\")\n",
    "    else:\n",
    "        print(\"Encountered errors while inserting rows: {}\".format(errors))\n",
    "    os.remove(f'{path}.png')\n",
    "    os.remove(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19e9dd5",
   "metadata": {},
   "source": [
    "# Predictions Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3beda66",
   "metadata": {},
   "source": [
    "## 1. Notebook Configuration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392ea6a2",
   "metadata": {},
   "source": [
    "### 1.1. Loading Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0ca638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General libraries:\n",
    "import os\n",
    "import io\n",
    "#import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Dealing with files:\n",
    "!pip install jsonlines\n",
    "import jsonlines\n",
    "import json\n",
    "\n",
    "# Dealing with images:\n",
    "#import cv2\n",
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "# Google APIs:\n",
    "from google.cloud import storage\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud.aiplatform.gapic.schema import predict\n",
    "\n",
    "# Libraries for string filtering:\n",
    "from fnmatch import fnmatch\n",
    "\n",
    "# Libraries for image encoding\n",
    "import io\n",
    "import base64\n",
    "\n",
    "# Specific PDF libraries:\n",
    "#!conda install -c conda-forge poppler\n",
    "!sudo apt-get install -y poppler-data\n",
    "!sudo apt-get install -y poppler-utils\n",
    "!pip install pdf2image\n",
    "from pdf2image import convert_from_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea4b158",
   "metadata": {},
   "source": [
    "### 1.2. Setting Notebook Inputs\n",
    "#### 1.2.1 Google Cloud Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e958b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud config list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5657dcb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud ai-platform models list --region=us-central1\n",
    "!gcloud ai-platform models list --region=us-west1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68990c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT = 'qwiklabs-gcp-00-373ac55d0e0a'\n",
    "REGION = 'us-central1'\n",
    "BUCKET = 'qwiklabs-gcp-00-373ac55d0e0a'\n",
    "\n",
    "TEMP_FOLDER = './temp'\n",
    "RESULTS_CSV = 'img_class_results.csv'\n",
    "PREDICTION_MODE = 'BATCH' # 'ONLINE would be another possibility, but it is not implemented.'\n",
    "\n",
    "\n",
    "#PDF_FOLDER = os.path.join(TEMP_FOLDER, 'pdf')\n",
    "#PNG_FOLDER = os.path.join(TEMP_FOLDER, 'png')\n",
    "#CSV_FOLDER = os.path.join(TEMP_FOLDER, 'csv')\n",
    "\n",
    "os.environ['PROJECT'] = PROJECT\n",
    "os.environ['REGION'] = REGION\n",
    "os.environ['BUCKET'] = BUCKET"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabf4574",
   "metadata": {},
   "source": [
    "#### 1.2.2. Image Classification Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce686285",
   "metadata": {},
   "outputs": [],
   "source": [
    "IC_ENDPOINT_ID=\"7257673944809865216\"\n",
    "IC_PROJECT_ID=\"136021895401\"\n",
    "IC_INPUT_DATA_FILE=\"INPUT-JSON\"\n",
    "\n",
    "# Example of instance:\n",
    "# {\n",
    "#  \"instances\": [{\n",
    "#    \"content\": \"YOUR_IMAGE_BYTES\"\n",
    "#  }],\n",
    "#   \"parameters\": {\n",
    "#     \"confidenceThreshold\": 0.5,\n",
    "#     \"maxPredictions\": 5\n",
    "#   }\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3283f5f5",
   "metadata": {},
   "source": [
    "#### 1.2.3. Object Detection Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acc093e",
   "metadata": {},
   "outputs": [],
   "source": [
    "OD_ENDPOINT_ID=\"2074030773706424320\"\n",
    "OD_PROJECT_ID=\"136021895401\"\n",
    "OD_INPUT_DATA_FILE=\"INPUT-JSON\"\n",
    "\n",
    "# Example of instance:\n",
    "# {\n",
    "#  \"instances\": [{\n",
    "#    \"content\": \"YOUR_IMAGE_BYTES\"\n",
    "#  }],\n",
    "#   \"parameters\": {\n",
    "#     \"confidenceThreshold\": 0.5,\n",
    "#     \"maxPredictions\": 5\n",
    "#   }\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79cf120",
   "metadata": {},
   "source": [
    "## 2 Auxiliary Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85fb6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bucket_file_list(bucket_name, fname_template='*'):\n",
    "    '''!@brief Function that returns the list of files in a bucket.\n",
    "    @param bucket (string) Bucket name.\n",
    "    @param fname_template (string) Template for filtering blob names \n",
    "    that supports Unix shell-style wildcards. For more info: \n",
    "    https://docs.python.org/3/library/fnmatch.html\n",
    "            \n",
    "    @return (list of srtings) List of blob names in a bucket which \n",
    "    fullfills template structure.\n",
    "    '''\n",
    "    # Instantiating client:\n",
    "    storage_client = storage.Client()\n",
    "\n",
    "    # Note: Client.list_blobs requires at least package version 1.17.0.\n",
    "    blobs = storage_client.list_blobs(bucket_name)\n",
    "    \n",
    "    # Listing all the blobs in a bucket:\n",
    "    blob_lst = [blob.name for blob in blobs]\n",
    "\n",
    "    # Filtering blob names with the template format given:  \n",
    "    file_lst = [fname for fname in blob_lst if fnmatch(fname, fname_template)]\n",
    "    \n",
    "    return file_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6d5a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_bucket(bucket_name, filter =['xxsdsdsds']):\n",
    "    # bucket_name = \"your-bucket-name\"\n",
    "    # blob_name = \"your-object-name\"\n",
    "\n",
    "    storage_client = storage.Client()\n",
    "    \n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    for file in filter:\n",
    "        blob = bucket.blob(file)\n",
    "        blob.delete()\n",
    "        print(\"Blob {} deleted.\".format(file))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b339b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_files_from_bucket(bucket_name, dest_folder, source_folder=\"labeled_patents/pdf/\", ext = \".pdf\" ):\n",
    "    '''@brief! Function that downloads a list of files from a bucket.\n",
    "\n",
    "    @param bucket: (string) Bucket name.\n",
    "    @param dest_folder: (string) Folder where files are downloaded.\n",
    "    '''\n",
    "    if not os.path.exists(dest_folder):\n",
    "        os.makedirs(dest_folder)\n",
    "        \n",
    "    new_file_lst = []\n",
    "    # Instantiating client:\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    \n",
    "    blob_list  = [blob for blob in list(bucket.list_blobs()) if blob.name.startswith(source_folder) and blob.name.endswith(ext)]\n",
    "\n",
    "    # Saving blob into the destination folder:\n",
    "    for blob in blob_list:\n",
    "        # Saving blob into a filename:\n",
    "        _, name = os.path.split(blob.name)\n",
    "        new_fname = os.path.join(dest_folder, name)\n",
    "        blob.download_to_filename(new_fname)\n",
    "        new_file_lst.append(new_fname)\n",
    "    \n",
    "    # TODO: A check of the downloaded files should be performed!! Maybe is just \n",
    "    # reading the files of the folder since if it is a temporal folder, every time\n",
    "    # the pipeline is executed, the folder is created empty:\n",
    "    #os.listdir(dest_folder) or similar\n",
    "    print('Number of files downloaded: {:d}'.format(len(new_file_lst)))\n",
    "    \n",
    "    return new_file_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60dbacf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_images_in_path(path):\n",
    "    '''@brief! Function to encode an image of each pdf to be used as instance \n",
    "    for a AutoML mode.\n",
    "        \n",
    "    @param file_lst (list of strings) PDF file names to be transformed.\n",
    "    '''\n",
    "    file_lst = [os.path.join(path, file) for file in os.listdir(path) if os.path.isfile(os.path.join(path, file))]\n",
    "    \n",
    "    encoded_img_lst = []\n",
    "    for file in file_lst:\n",
    "        image = convert_from_path(file)\n",
    "        image = image[0]                # Only the firs page is going to be analyzed.\n",
    "        img_byte_arr = io.BytesIO()\n",
    "        image.save(img_byte_arr, format='PNG')\n",
    "        img_byte_arr = img_byte_arr.getvalue()\n",
    "        encoded_img_lst.append(base64.b64encode(img_byte_arr).decode(\"utf-8\"))\n",
    "\n",
    "    return encoded_img_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f10ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to create JSONL files for instance creation:\n",
    "# WATCH OUT!! Hardcoded values!!\n",
    "def save_jsonl(fp, json_file):\n",
    "    # needs .jl suffix\n",
    "    d = json.dumps(json_file)+\"\\n\"\n",
    "    d = d.encode('utf8')\n",
    "    try:\n",
    "        with open(fp, \"ab\") as f:\n",
    "            f.write(d)\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR]: {e}\\n{sys.exc_info()}\\n{traceback.format_exc()}\")\n",
    "\n",
    "def create_jsonl(gcs_img_path,fp):\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(BUCKET)\n",
    "    # create jsonl\n",
    "    blob_list  = [blob.name for blob in list(bucket.list_blobs()) if blob.name.startswith(\"labeled_patents/images\") and blob.name.endswith(\".png\")]\n",
    "    \n",
    "    for filename in blob_list:\n",
    "        temp_json = {\"content\": f\"gs://{BUCKET}/{filename}\", \"mimeType\": \"image/png\"}\n",
    "        save_jsonl(fp, temp_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17cca6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launching batch predictions:\n",
    "def create_batch_prediction_job_sample(\n",
    "    project='qwiklabs-gcp-00-373ac55d0e0a',\n",
    "    location='us-central1',\n",
    "    model_resource_name='8925034949820547072',\n",
    "    job_display_name='batch_img_classification',\n",
    "    gcs_source='gs://qwiklabs-gcp-00-373ac55d0e0a/labeled_patents/images_icn.jsonl',\n",
    "    gcs_destination='gs://qwiklabs-gcp-00-373ac55d0e0a/labeled_patents/img_class_preds'):\n",
    "    aiplatform.init(project=project, location=location)\n",
    "\n",
    "    my_model = aiplatform.Model(model_resource_name)\n",
    "\n",
    "    batch_prediction_job = my_model.batch_predict(\n",
    "        job_display_name=job_display_name,\n",
    "        gcs_source=gcs_source,\n",
    "        gcs_destination_prefix=gcs_destination,\n",
    "        sync=True,\n",
    "    )\n",
    "\n",
    "    batch_prediction_job.wait()\n",
    "\n",
    "    print(batch_prediction_job.display_name)\n",
    "    print(batch_prediction_job.resource_name)\n",
    "    print(batch_prediction_job.state)\n",
    "    return batch_prediction_job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa32ff7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_imgclass_results_from_jsonl(filename):\n",
    "    '''!@brief Function that reads the results of image classification prediction\n",
    "    from the jsonl files created during batch prediction.\n",
    "    \n",
    "    @param filename (string) JSONL file path and name\n",
    "    \n",
    "    @return (Dataframe) Table with the image classification results.\n",
    "    '''\n",
    "    # Creating an empty dataframe to store the image classification results:\n",
    "    results_df = pd.DataFrame(columns=['image_name', 'label', 'confidence'])\n",
    "\n",
    "    # Reading the JSONL file and processing each JSON:\n",
    "    with jsonlines.open(filename, 'r') as file:\n",
    "        for i, line in enumerate(file):\n",
    "            # Extracting results from the jsonl file:\n",
    "            _, image_name = os.path.split(line['instance']['content'])\n",
    "            pos = np.argmax(line['prediction']['confidences'])\n",
    "            confidence = line['prediction']['confidences'][pos]\n",
    "            label = line['prediction']['displayNames'][pos]\n",
    "\n",
    "            # Storing results into a dataframe:\n",
    "            results_df.loc[i, 'image_name'] = image_name \n",
    "            results_df.loc[i, 'label'] = label\n",
    "            results_df.loc[i, 'confidence'] = confidence\n",
    "\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc074eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_file_to_bucket(bucket_name, source_file_name, destination_blob_name):\n",
    "    \"\"\"!@brief Function that uploads a file to a bucket.\n",
    "    \n",
    "    @param bucket_name (string) ID/name of the bucket.\n",
    "    @param source_file_name (string) Path to the file to be uploaded.\n",
    "    @param destination_blob_name (string) Desired storage object name.   \n",
    "    \"\"\"\n",
    "    \n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(destination_blob_name)\n",
    "\n",
    "    blob.upload_from_filename(source_file_name)\n",
    "\n",
    "    print(\"File {} uploaded to {}.\".format(source_file_name, destination_blob_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d78f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_table_from_csv(dataset_name, table_name, schema_lst, csv_blob_name):\n",
    "    '''!@brief Function that create a table in an existing dataset with\n",
    "    the data contained into a CSV.\n",
    "    \n",
    "    @param dataset_name (string) Name of the dataset which will store \n",
    "    the table.\n",
    "    @param table_name (string) Name of the table to be created.\n",
    "    @param schema_lst (list of tuples) Contains the schema of the table\n",
    "    to be created. The format must be the next one: \n",
    "    [()'column name', 'field format', 'mode', 'Description')]\n",
    "    Example:\n",
    "    schema_lst = [('col_A_name',  'STRING', 'REQUIRED', 'Description 1'), \n",
    "                  ('col_B_name', 'INTEGER', 'REQUIRED', 'Description 2'),\n",
    "                  ('col_C_name',   'FLOAT', 'REQUIRED', 'Description 3')]\n",
    "    For more info:\n",
    "    https://cloud.google.com/bigquery/docs/reference/rest/v2/tables#TableFieldSchema.FIELDS.type\n",
    "    @param csv_blob_name (string) GS URI of the CSV file.\n",
    "    '''\n",
    "    \n",
    "    # Construct a BigQuery client object.\n",
    "    client = bigquery.Client()\n",
    "\n",
    "    # Setting table_id to the ID of the table to create.\n",
    "    table_id = \"{}.{}.{}\".format(client.project, dataset_name, table_name)\n",
    "    \n",
    "    # Creating table schema:\n",
    "    schema = [bigquery.SchemaField(*tup) for tup in schema_lst]\n",
    "    \n",
    "    # Configuring the job which builds the table:\n",
    "    job_config = bigquery.LoadJobConfig(schema=schema,\n",
    "                                        skip_leading_rows=1,\n",
    "                                        source_format=bigquery.SourceFormat.CSV)\n",
    "\n",
    "    # Making an API request to create the job:\n",
    "    load_job = client.load_table_from_uri(csv_blob_name, table_id, job_config=job_config)\n",
    "\n",
    "    # Waiting for the job to be completed.\n",
    "    load_job.result()\n",
    "\n",
    "    destination_table = client.get_table(table_id)  # Make an API request.\n",
    "    print(\"Loaded {} rows.\".format(destination_table.num_rows))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0740a7ee",
   "metadata": {},
   "source": [
    "## 3. Pipeline Functional"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14a3e91",
   "metadata": {},
   "source": [
    "### 3.1. Donwload PDFs to a temporal folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b0f2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the temporal folder if it does not exists:\n",
    "if not os.path.exists(TEMP_FOLDER):\n",
    "    # Create folder:\n",
    "    os.mkdir(TEMP_FOLDER)\n",
    "    \n",
    "# Downloading PDFs from the bucket to the temporal folder:\n",
    "file_lst = download_files_from_bucket(BUCKET, TEMP_FOLDER, source_folder=\"labeled_patents/pdf/\", ext = \".pdf\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44399506",
   "metadata": {},
   "source": [
    "### 3.2. Transforming PDFs into PNGs (Only for Online prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad16f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'ONLINE'==PREDICTION_MODE:\n",
    "    # Encoding images as base64:\n",
    "    imgs = encode_images_in_path(dest_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e800e38",
   "metadata": {},
   "source": [
    "### 3.3. Cleaning old predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7d2fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "filelist = get_bucket_file_list(BUCKET, fname_template='*img_class_preds*.jsonl')\n",
    "clean_bucket(BUCKET, filelist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6bde2d3",
   "metadata": {},
   "source": [
    "### 3.4. Performing predictions in the cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba3d78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the batch of instances to perform a prediction:\n",
    "import json\n",
    "gcs_img_path = f\"gs:/{PROJECT}/{BUCKET}/labeled_patents/images\"\n",
    "fp = \"images_icn.jsonl\"\n",
    "        \n",
    "# Creating the JSONL file with all the instances:\n",
    "create_jsonl(gcs_img_path, fp)\n",
    "\n",
    "# Uploading the JSONL file to a bucket:\n",
    "!gsutil -m cp ./images_icn.jsonl gs://qwiklabs-gcp-00-373ac55d0e0a/labeled_patents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352fab35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launching predictions:\n",
    "create_batch_prediction_job_sample(\n",
    "    project='qwiklabs-gcp-00-373ac55d0e0a',\n",
    "    location='us-central1',\n",
    "    model_resource_name='8925034949820547072',\n",
    "    job_display_name='batch_img_classification',\n",
    "    gcs_source='gs://qwiklabs-gcp-00-373ac55d0e0a/labeled_patents/images_icn.jsonl',\n",
    "    gcs_destination='gs://qwiklabs-gcp-00-373ac55d0e0a/labeled_patents/img_class_preds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d632df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud ai endpoints list --region=us-central1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8048a5a",
   "metadata": {},
   "source": [
    "### 3.5. Downloding the JSONL files with the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea690253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading the results files from Google Storage:\n",
    "gcs_destination='gs://qwiklabs-gcp-00-373ac55d0e0a/labeled_patents/img_class_preds'\n",
    "source_folder = 'labeled_patents/img_class_preds' \n",
    "ext = \".jsonl\"\n",
    "resfile_lst = download_files_from_bucket(BUCKET, TEMP_FOLDER, source_folder, ext)\n",
    "\n",
    "print('\\nDownloaded files:')\n",
    "print(*resfile_lst, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952de3e0",
   "metadata": {},
   "source": [
    "### 3.6. Parsing the predictions from the JSONL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc1025c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parsing the JSONL files:\n",
    "for i, file in enumerate(resfile_lst):\n",
    "    if i==0:\n",
    "        res_df = read_imgclass_results_from_jsonl(file)\n",
    "    else:\n",
    "        res_df = res_df.append(read_imgclass_results_from_jsonl(file))\n",
    "        \n",
    "print('Number of results read: {:d}'.format(res_df.shape[0]))\n",
    "res_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0292fdbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the results dataframe as a CSV file:\n",
    "res_df.to_csv(os.path.join(TEMP_FOLDER, RESULTS_CSV), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab3dd8e",
   "metadata": {},
   "source": [
    "### 3.7.Upload results to a BQ table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b102dfbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uploading the CSV file to a GS bucket:\n",
    "upload_file_to_bucket(bucket_name=BUCKET, \n",
    "                      source_file_name=os.path.join(TEMP_FOLDER, RESULTS_CSV), \n",
    "                      destination_blob_name=os.path.join('labeled_patents', 'img_class_preds', RESULTS_CSV))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb799f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing the CSV content into a BQ table:\n",
    "dataset_name = 'labeled_patents'\n",
    "table_name = 'image_classification_results'\n",
    "schema_lst = [('image_name', 'STRING', 'REQUIRED', 'Name of the image analyzed.'), \n",
    "              ('label',      'STRING', 'REQUIRED', 'Predicted class. It can be US or EU'),\n",
    "              ('confidence',  'FLOAT', 'REQUIRED', 'Confidence of the prediction.')]\n",
    "csv_blob_name = os.path.join('gs://', BUCKET, 'labeled_patents', 'img_class_preds', RESULTS_CSV)\n",
    "\n",
    "create_table_from_csv(dataset_name, table_name, schema_lst, csv_blob_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971661e5",
   "metadata": {},
   "source": [
    "### 3.8. Cleaning temporal folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8baaac6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deleting the temporal folder:\n",
    "os.rmdir(TEMP_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef0a43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-5.m76",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-5:m76"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
