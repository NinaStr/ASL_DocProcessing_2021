{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6162ce5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "675a9afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO LIST\n",
    "#1- Change reading files to filter pdfs.\n",
    "#2- Access to the image classification end point\n",
    "#3- Create a JSON input for the endpoint (with the image)\n",
    "#4- Retrieve prediction\n",
    "#5- Store the prediction in a new BQ table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fdddadc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aeda0974",
   "metadata": {},
   "source": [
    "# Predictions Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd73db2",
   "metadata": {},
   "source": [
    "## 1. Notebook Configuration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e63cf01",
   "metadata": {},
   "source": [
    "### 1.1. Loading Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "bd8072da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jsonlines in /opt/conda/lib/python3.7/site-packages (2.0.0)\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "poppler-data is already the newest version (0.4.9-2).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 1 not upgraded.\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "poppler-utils is already the newest version (0.71.0-5).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 1 not upgraded.\n",
      "Requirement already satisfied: pdf2image in /opt/conda/lib/python3.7/site-packages (1.16.0)\n",
      "Requirement already satisfied: pillow in /opt/conda/lib/python3.7/site-packages (from pdf2image) (8.3.1)\n"
     ]
    }
   ],
   "source": [
    "# General libraries:\n",
    "import os\n",
    "import io\n",
    "#import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Dealing with files:\n",
    "!pip install jsonlines\n",
    "import jsonlines\n",
    "import json\n",
    "\n",
    "# Dealing with images:\n",
    "#import cv2\n",
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "# Google APIs:\n",
    "from google.cloud import storage\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud.aiplatform.gapic.schema import predict\n",
    "\n",
    "# Libraries for string filtering:\n",
    "from fnmatch import fnmatch\n",
    "\n",
    "# Libraries for image encoding\n",
    "import io\n",
    "import base64\n",
    "\n",
    "# Specific PDF libraries:\n",
    "#!conda install -c conda-forge poppler\n",
    "!sudo apt-get install -y poppler-data\n",
    "!sudo apt-get install -y poppler-utils\n",
    "!pip install pdf2image\n",
    "from pdf2image import convert_from_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0bbfdb",
   "metadata": {},
   "source": [
    "### 1.2. Setting Notebook Inputs\n",
    "#### 1.2.1 Google Cloud Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "716eda49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ai]\n",
      "region = us-west1\n",
      "[compute]\n",
      "region = us-central1\n",
      "[core]\n",
      "account = 136021895401-compute@developer.gserviceaccount.com\n",
      "disable_usage_reporting = True\n",
      "project = qwiklabs-gcp-00-373ac55d0e0a\n",
      "\n",
      "Your active configuration is: [default]\n"
     ]
    }
   ],
   "source": [
    "!gcloud config list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cece91af",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT = 'qwiklabs-gcp-00-373ac55d0e0a'\n",
    "REGION = 'us-central1'\n",
    "BUCKET = 'qwiklabs-gcp-00-373ac55d0e0a'\n",
    "\n",
    "TEMP_FOLDER = './temp'\n",
    "RESULTS_CSV = 'img_class_results.csv'\n",
    "PREDICTION_MODE = 'BATCH' # 'ONLINE would be another possibility, but it is not implemented.'\n",
    "\n",
    "\n",
    "#PDF_FOLDER = os.path.join(TEMP_FOLDER, 'pdf')\n",
    "#PNG_FOLDER = os.path.join(TEMP_FOLDER, 'png')\n",
    "#CSV_FOLDER = os.path.join(TEMP_FOLDER, 'csv')\n",
    "\n",
    "os.environ['PROJECT'] = PROJECT\n",
    "os.environ['REGION'] = REGION\n",
    "os.environ['BUCKET'] = BUCKET"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5437c4e9",
   "metadata": {},
   "source": [
    "#### 1.2.2. Image Classification Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4bb25700",
   "metadata": {},
   "outputs": [],
   "source": [
    "IC_ENDPOINT_ID=\"7257673944809865216\"\n",
    "IC_PROJECT_ID=\"136021895401\"\n",
    "IC_INPUT_DATA_FILE=\"INPUT-JSON\"\n",
    "\n",
    "# Example of instance:\n",
    "# {\n",
    "#  \"instances\": [{\n",
    "#    \"content\": \"YOUR_IMAGE_BYTES\"\n",
    "#  }],\n",
    "#   \"parameters\": {\n",
    "#     \"confidenceThreshold\": 0.5,\n",
    "#     \"maxPredictions\": 5\n",
    "#   }\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0c960a",
   "metadata": {},
   "source": [
    "#### 1.2.3. Object Detection Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3f9e72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "OD_ENDPOINT_ID=\"2074030773706424320\"\n",
    "OD_PROJECT_ID=\"136021895401\"\n",
    "OD_INPUT_DATA_FILE=\"INPUT-JSON\"\n",
    "\n",
    "# Example of instance:\n",
    "# {\n",
    "#  \"instances\": [{\n",
    "#    \"content\": \"YOUR_IMAGE_BYTES\"\n",
    "#  }],\n",
    "#   \"parameters\": {\n",
    "#     \"confidenceThreshold\": 0.5,\n",
    "#     \"maxPredictions\": 5\n",
    "#   }\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192505d2",
   "metadata": {},
   "source": [
    "## 2 Auxiliary Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10be3530",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bucket_file_list(bucket_name, fname_template='*'):\n",
    "    '''!@brief Function that returns the list of files in a bucket.\n",
    "    @param bucket (string) Bucket name.\n",
    "    @param fname_template (string) Template for filtering blob names \n",
    "    that supports Unix shell-style wildcards. For more info: \n",
    "    https://docs.python.org/3/library/fnmatch.html\n",
    "            \n",
    "    @return (list of srtings) List of blob names in a bucket which \n",
    "    fullfills template structure.\n",
    "    '''\n",
    "    # Instantiating client:\n",
    "    storage_client = storage.Client()\n",
    "\n",
    "    # Note: Client.list_blobs requires at least package version 1.17.0.\n",
    "    blobs = storage_client.list_blobs(bucket_name)\n",
    "    \n",
    "    # Listing all the blobs in a bucket:\n",
    "    blob_lst = [blob.name for blob in blobs]\n",
    "\n",
    "    # Filtering blob names with the template format given:  \n",
    "    file_lst = [fname for fname in blob_lst if fnmatch(fname, fname_template)]\n",
    "    \n",
    "    return file_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97206188",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_bucket(bucket_name, filter =['xxsdsdsds']):\n",
    "    # bucket_name = \"your-bucket-name\"\n",
    "    # blob_name = \"your-object-name\"\n",
    "\n",
    "    storage_client = storage.Client()\n",
    "    \n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    for file in filter:\n",
    "        blob = bucket.blob(file)\n",
    "        blob.delete()\n",
    "        print(\"Blob {} deleted.\".format(file))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "12197c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_files_from_bucket(bucket_name, dest_folder, source_folder=\"labeled_patents/pdf/\", ext = \".pdf\" ):\n",
    "    '''@brief! Function that downloads a list of files from a bucket.\n",
    "\n",
    "    @param bucket: (string) Bucket name.\n",
    "    @param dest_folder: (string) Folder where files are downloaded.\n",
    "    '''\n",
    "    if not os.path.exists(dest_folder):\n",
    "        os.makedirs(dest_folder)\n",
    "        \n",
    "    new_file_lst = []\n",
    "    # Instantiating client:\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    \n",
    "    blob_list  = [blob for blob in list(bucket.list_blobs()) if blob.name.startswith(source_folder) and blob.name.endswith(ext)]\n",
    "\n",
    "    # Saving blob into the destination folder:\n",
    "    for blob in blob_list:\n",
    "        # Saving blob into a filename:\n",
    "        _, name = os.path.split(blob.name)\n",
    "        new_fname = os.path.join(dest_folder, name)\n",
    "        blob.download_to_filename(new_fname)\n",
    "        new_file_lst.append(new_fname)\n",
    "    \n",
    "    # TODO: A check of the downloaded files should be performed!! Maybe is just \n",
    "    # reading the files of the folder since if it is a temporal folder, every time\n",
    "    # the pipeline is executed, the folder is created empty:\n",
    "    #os.listdir(dest_folder) or similar\n",
    "    print('Number of files downloaded: {:d}'.format(len(new_file_lst)))\n",
    "    \n",
    "    return new_file_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17b54f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_images_in_path(path):\n",
    "    '''@brief! Function to encode an image of each pdf to be used as instance \n",
    "    for a AutoML mode.\n",
    "        \n",
    "    @param file_lst (list of strings) PDF file names to be transformed.\n",
    "    '''\n",
    "    file_lst = [os.path.join(path, file) for file in os.listdir(path) if os.path.isfile(os.path.join(path, file))]\n",
    "    \n",
    "    encoded_img_lst = []\n",
    "    for file in file_lst:\n",
    "        image = convert_from_path(file)\n",
    "        image = image[0]                # Only the firs page is going to be analyzed.\n",
    "        img_byte_arr = io.BytesIO()\n",
    "        image.save(img_byte_arr, format='PNG')\n",
    "        img_byte_arr = img_byte_arr.getvalue()\n",
    "        encoded_img_lst.append(base64.b64encode(img_byte_arr).decode(\"utf-8\"))\n",
    "\n",
    "    return encoded_img_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6bed8ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to create JSONL files for instance creation:\n",
    "# WATCH OUT!! Hardcoded values!!\n",
    "def save_jsonl(fp, json_file):\n",
    "    # needs .jl suffix\n",
    "    d = json.dumps(json_file)+\"\\n\"\n",
    "    d = d.encode('utf8')\n",
    "    try:\n",
    "        with open(fp, \"ab\") as f:\n",
    "            f.write(d)\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR]: {e}\\n{sys.exc_info()}\\n{traceback.format_exc()}\")\n",
    "\n",
    "def create_jsonl(gcs_img_path,fp):\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(BUCKET)\n",
    "    # create jsonl\n",
    "    blob_list  = [blob.name for blob in list(bucket.list_blobs()) if blob.name.startswith(\"labeled_patents/images\") and blob.name.endswith(\".png\")]\n",
    "    \n",
    "    for filename in blob_list:\n",
    "        temp_json = {\"content\": f\"gs://{BUCKET}/{filename}\", \"mimeType\": \"image/png\"}\n",
    "        save_jsonl(fp, temp_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1bd9a5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launching batch predictions:\n",
    "def create_batch_prediction_job_sample(\n",
    "    project='qwiklabs-gcp-00-373ac55d0e0a',\n",
    "    location='us-central1',\n",
    "    model_resource_name='8925034949820547072',\n",
    "    job_display_name='batch_img_classification',\n",
    "    gcs_source='gs://qwiklabs-gcp-00-373ac55d0e0a/labeled_patents/images_icn.jsonl',\n",
    "    gcs_destination='gs://qwiklabs-gcp-00-373ac55d0e0a/labeled_patents/img_class_preds'):\n",
    "    aiplatform.init(project=project, location=location)\n",
    "\n",
    "    my_model = aiplatform.Model(model_resource_name)\n",
    "\n",
    "    batch_prediction_job = my_model.batch_predict(\n",
    "        job_display_name=job_display_name,\n",
    "        gcs_source=gcs_source,\n",
    "        gcs_destination_prefix=gcs_destination,\n",
    "        sync=True,\n",
    "    )\n",
    "\n",
    "    batch_prediction_job.wait()\n",
    "\n",
    "    print(batch_prediction_job.display_name)\n",
    "    print(batch_prediction_job.resource_name)\n",
    "    print(batch_prediction_job.state)\n",
    "    return batch_prediction_job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "164e1a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_imgclass_results_from_jsonl(filename):\n",
    "    '''!@brief Function that reads the results of image classification prediction\n",
    "    from the jsonl files created during batch prediction.\n",
    "    \n",
    "    @param filename (string) JSONL file path and name\n",
    "    \n",
    "    @return (Dataframe) Table with the image classification results.\n",
    "    '''\n",
    "    # Creating an empty dataframe to store the image classification results:\n",
    "    results_df = pd.DataFrame(columns=['image_name', 'label', 'confidence'])\n",
    "\n",
    "    # Reading the JSONL file and processing each JSON:\n",
    "    with jsonlines.open(filename, 'r') as file:\n",
    "        for i, line in enumerate(file):\n",
    "            # Extracting results from the jsonl file:\n",
    "            _, image_name = os.path.split(line['instance']['content'])\n",
    "            pos = np.argmax(line['prediction']['confidences'])\n",
    "            confidence = line['prediction']['confidences'][pos]\n",
    "            label = line['prediction']['displayNames'][pos]\n",
    "\n",
    "            # Storing results into a dataframe:\n",
    "            results_df.loc[i, 'image_name'] = image_name \n",
    "            results_df.loc[i, 'label'] = label\n",
    "            results_df.loc[i, 'confidence'] = confidence\n",
    "\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "73408ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_file_to_bucket(bucket_name, source_file_name, destination_blob_name):\n",
    "    \"\"\"!@brief Function that uploads a file to a bucket.\n",
    "    \n",
    "    @param bucket_name (string) ID/name of the bucket.\n",
    "    @param source_file_name (string) Path to the file to be uploaded.\n",
    "    @param destination_blob_name (string) Desired storage object name.   \n",
    "    \"\"\"\n",
    "    \n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(destination_blob_name)\n",
    "\n",
    "    blob.upload_from_filename(source_file_name)\n",
    "\n",
    "    print(\"File {} uploaded to {}.\".format(source_file_name, destination_blob_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "98dbb65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_table_from_csv(dataset_name, table_name, schema_lst, csv_blob_name):\n",
    "    '''!@brief Function that create a table in an existing dataset with\n",
    "    the data contained into a CSV.\n",
    "    \n",
    "    @param dataset_name (string) Name of the dataset which will store \n",
    "    the table.\n",
    "    @param table_name (string) Name of the table to be created.\n",
    "    @param schema_lst (list of tuples) Contains the schema of the table\n",
    "    to be created. The format must be the next one: \n",
    "    [()'column name', 'field format', 'mode', 'Description')]\n",
    "    Example:\n",
    "    schema_lst = [('col_A_name',  'STRING', 'REQUIRED', 'Description 1'), \n",
    "                  ('col_B_name', 'INTEGER', 'REQUIRED', 'Description 2'),\n",
    "                  ('col_C_name',   'FLOAT', 'REQUIRED', 'Description 3')]\n",
    "    For more info:\n",
    "    https://cloud.google.com/bigquery/docs/reference/rest/v2/tables#TableFieldSchema.FIELDS.type\n",
    "    @param csv_blob_name (string) GS URI of the CSV file.\n",
    "    '''\n",
    "    \n",
    "    # Construct a BigQuery client object.\n",
    "    client = bigquery.Client()\n",
    "\n",
    "    # Setting table_id to the ID of the table to create.\n",
    "    table_id = \"{}.{}.{}\".format(client.project, dataset_name, table_name)\n",
    "    \n",
    "    # Creating table schema:\n",
    "    schema = [bigquery.SchemaField(*tup) for tup in schema_lst]\n",
    "    \n",
    "    # Configuring the job which builds the table:\n",
    "    job_config = bigquery.LoadJobConfig(schema=schema,\n",
    "                                        skip_leading_rows=1,\n",
    "                                        source_format=bigquery.SourceFormat.CSV)\n",
    "\n",
    "    # Making an API request to create the job:\n",
    "    load_job = client.load_table_from_uri(csv_blob_name, table_id, job_config=job_config)\n",
    "\n",
    "    # Waiting for the job to be completed.\n",
    "    load_job.result()\n",
    "\n",
    "    destination_table = client.get_table(table_id)  # Make an API request.\n",
    "    print(\"Loaded {} rows.\".format(destination_table.num_rows))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6370b4ee",
   "metadata": {},
   "source": [
    "## 3. Pipeline Functional"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48fe49f4",
   "metadata": {},
   "source": [
    "### 3.1. Donwload PDFs to a temporal folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "04bf722e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files downloaded: 403\n"
     ]
    }
   ],
   "source": [
    "# Creating the temporal folder if it does not exists:\n",
    "if not os.path.exists(TEMP_FOLDER):\n",
    "    # Create folder:\n",
    "    os.mkdir(TEMP_FOLDER)\n",
    "    \n",
    "# Downloading PDFs from the bucket to the temporal folder:\n",
    "file_lst = download_files_from_bucket(BUCKET, TEMP_FOLDER, source_folder=\"labeled_patents/pdf/\", ext = \".pdf\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5668b32d",
   "metadata": {},
   "source": [
    "### 3.2. Transforming PDFs into PNGs (Only for Online prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d52dee5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'ONLINE'==PREDICTION_MODE:\n",
    "    # Encoding images as base64:\n",
    "    imgs = encode_images_in_path(dest_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a0ff6e",
   "metadata": {},
   "source": [
    "### 3.3. Cleaning old predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "381969d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blob labeled_patents/img_class_preds/prediction-docprocessing_2021811144149-2021-08-16T14:36:01.583226Z/predictions_00001.jsonl deleted.\n",
      "Blob labeled_patents/img_class_preds/prediction-docprocessing_2021811144149-2021-08-16T14:36:01.583226Z/predictions_00002.jsonl deleted.\n",
      "Blob labeled_patents/img_class_preds/prediction-docprocessing_2021811144149-2021-08-16T14:36:01.583226Z/predictions_00003.jsonl deleted.\n",
      "Blob labeled_patents/img_class_preds/prediction-docprocessing_2021811144149-2021-08-16T14:36:01.583226Z/predictions_00004.jsonl deleted.\n",
      "Blob labeled_patents/img_class_preds/prediction-docprocessing_2021811144149-2021-08-16T14:36:01.583226Z/predictions_00005.jsonl deleted.\n"
     ]
    }
   ],
   "source": [
    "filelist = get_bucket_file_list(BUCKET, fname_template='*img_class_preds*.jsonl')\n",
    "clean_bucket(BUCKET, filelist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0582de6",
   "metadata": {},
   "source": [
    "### 3.4. Performing predictions in the cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9e48c415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://./images_icn.jsonl [Content-Type=application/octet-stream]...\n",
      "/ [1/1 files][ 44.8 KiB/ 44.8 KiB] 100% Done                                    \n",
      "Operation completed over 1 objects/44.8 KiB.                                     \n"
     ]
    }
   ],
   "source": [
    "# Creating the batch of instances to perform a prediction:\n",
    "import json\n",
    "gcs_img_path = f\"gs:/{PROJECT}/{BUCKET}/labeled_patents/images\"\n",
    "fp = \"images_icn.jsonl\"\n",
    "        \n",
    "# Creating the JSONL file with all the instances:\n",
    "create_jsonl(gcs_img_path, fp)\n",
    "\n",
    "# Uploading the JSONL file to a bucket:\n",
    "!gsutil -m cp ./images_icn.jsonl gs://qwiklabs-gcp-00-373ac55d0e0a/labeled_patents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7b341642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.jobs:Creating BatchPredictionJob\n",
      "INFO:google.cloud.aiplatform.jobs:BatchPredictionJob created. Resource name: projects/136021895401/locations/us-central1/batchPredictionJobs/1856611145406742528\n",
      "INFO:google.cloud.aiplatform.jobs:To use this BatchPredictionJob in another session:\n",
      "INFO:google.cloud.aiplatform.jobs:bpj = aiplatform.BatchPredictionJob('projects/136021895401/locations/us-central1/batchPredictionJobs/1856611145406742528')\n",
      "INFO:google.cloud.aiplatform.jobs:View Batch Prediction Job:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/batch-predictions/1856611145406742528?project=136021895401\n",
      "INFO:google.cloud.aiplatform.jobs:BatchPredictionJob projects/136021895401/locations/us-central1/batchPredictionJobs/1856611145406742528 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:BatchPredictionJob projects/136021895401/locations/us-central1/batchPredictionJobs/1856611145406742528 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:BatchPredictionJob projects/136021895401/locations/us-central1/batchPredictionJobs/1856611145406742528 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:BatchPredictionJob projects/136021895401/locations/us-central1/batchPredictionJobs/1856611145406742528 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:BatchPredictionJob projects/136021895401/locations/us-central1/batchPredictionJobs/1856611145406742528 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:BatchPredictionJob projects/136021895401/locations/us-central1/batchPredictionJobs/1856611145406742528 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:BatchPredictionJob projects/136021895401/locations/us-central1/batchPredictionJobs/1856611145406742528 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:BatchPredictionJob projects/136021895401/locations/us-central1/batchPredictionJobs/1856611145406742528 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:BatchPredictionJob projects/136021895401/locations/us-central1/batchPredictionJobs/1856611145406742528 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:BatchPredictionJob projects/136021895401/locations/us-central1/batchPredictionJobs/1856611145406742528 current state:\n",
      "JobState.JOB_STATE_SUCCEEDED\n",
      "INFO:google.cloud.aiplatform.jobs:BatchPredictionJob run completed. Resource name: projects/136021895401/locations/us-central1/batchPredictionJobs/1856611145406742528\n",
      "batch_img_classification\n",
      "projects/136021895401/locations/us-central1/batchPredictionJobs/1856611145406742528\n",
      "JobState.JOB_STATE_SUCCEEDED\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<google.cloud.aiplatform.jobs.BatchPredictionJob object at 0x7f5577bfad50> \n",
       "resource name: projects/136021895401/locations/us-central1/batchPredictionJobs/1856611145406742528"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Launching predictions:\n",
    "create_batch_prediction_job_sample(\n",
    "    project='qwiklabs-gcp-00-373ac55d0e0a',\n",
    "    location='us-central1',\n",
    "    model_resource_name='8925034949820547072',\n",
    "    job_display_name='batch_img_classification',\n",
    "    gcs_source='gs://qwiklabs-gcp-00-373ac55d0e0a/labeled_patents/images_icn.jsonl',\n",
    "    gcs_destination='gs://qwiklabs-gcp-00-373ac55d0e0a/labeled_patents/img_class_preds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5fba4d",
   "metadata": {},
   "source": [
    "### 3.5. Downloding the JSONL files with the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cb8177b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files downloaded: 5\n",
      "\n",
      "Downloaded files:\n",
      "./temp/predictions_00001.jsonl\n",
      "./temp/predictions_00002.jsonl\n",
      "./temp/predictions_00003.jsonl\n",
      "./temp/predictions_00004.jsonl\n",
      "./temp/predictions_00005.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Downloading the results files from Google Storage:\n",
    "gcs_destination='gs://qwiklabs-gcp-00-373ac55d0e0a/labeled_patents/img_class_preds'\n",
    "source_folder = 'labeled_patents/img_class_preds' \n",
    "ext = \".jsonl\"\n",
    "resfile_lst = download_files_from_bucket(BUCKET, TEMP_FOLDER, source_folder, ext)\n",
    "\n",
    "print('\\nDownloaded files:')\n",
    "print(*resfile_lst, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb315ef",
   "metadata": {},
   "source": [
    "### 3.6. Parsing the predictions from the JSONL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "209dfe68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of results read: 403\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_name</th>\n",
       "      <th>label</th>\n",
       "      <th>confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>computer_vision_17.png</td>\n",
       "      <td>US</td>\n",
       "      <td>0.712984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>espacenet_en64.png</td>\n",
       "      <td>EU</td>\n",
       "      <td>0.999885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>espacenet_de68.png</td>\n",
       "      <td>EU</td>\n",
       "      <td>0.999912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>espacenet_de49.png</td>\n",
       "      <td>EU</td>\n",
       "      <td>0.999843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>crypto_13.png</td>\n",
       "      <td>US</td>\n",
       "      <td>0.999836</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               image_name label confidence\n",
       "0  computer_vision_17.png    US   0.712984\n",
       "1      espacenet_en64.png    EU   0.999885\n",
       "2      espacenet_de68.png    EU   0.999912\n",
       "3      espacenet_de49.png    EU   0.999843\n",
       "4           crypto_13.png    US   0.999836"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parsing the JSONL files:\n",
    "for i, file in enumerate(resfile_lst):\n",
    "    if i==0:\n",
    "        res_df = read_imgclass_results_from_jsonl(file)\n",
    "    else:\n",
    "        res_df = res_df.append(read_imgclass_results_from_jsonl(file))\n",
    "        \n",
    "print('Number of results read: {:d}'.format(res_df.shape[0]))\n",
    "res_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "291520b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the results dataframe as a CSV file:\n",
    "res_df.to_csv(os.path.join(TEMP_FOLDER, RESULTS_CSV), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38fdd73c",
   "metadata": {},
   "source": [
    "### 3.7.Upload results to a BQ table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1e2b6b05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ./temp/img_class_results.csv uploaded to labeled_patents/img_class_preds/img_class_results.csv.\n"
     ]
    }
   ],
   "source": [
    "# Uploading the CSV file to a GS bucket:\n",
    "upload_file_to_bucket(bucket_name=BUCKET, \n",
    "                      source_file_name=os.path.join(TEMP_FOLDER, RESULTS_CSV), \n",
    "                      destination_blob_name=os.path.join('labeled_patents', 'img_class_preds', RESULTS_CSV))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7ca137f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 403 rows.\n"
     ]
    }
   ],
   "source": [
    "# Storing the CSV content into a BQ table:\n",
    "dataset_name = 'labeled_patents'\n",
    "table_name = 'image_classification_results'\n",
    "schema_lst = [('image_name', 'STRING', 'REQUIRED', 'Name of the image analyzed.'), \n",
    "              ('label',      'STRING', 'REQUIRED', 'Predicted class. It can be US or EU'),\n",
    "              ('confidence',  'FLOAT', 'REQUIRED', 'Confidence of the prediction.')]\n",
    "csv_blob_name = os.path.join('gs://', BUCKET, 'labeled_patents', 'img_class_preds', RESULTS_CSV)\n",
    "\n",
    "create_table_from_csv(dataset_name, table_name, schema_lst, csv_blob_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a76c000",
   "metadata": {},
   "source": [
    "### 3.8. Cleaning temporal folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd1d801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deleting the temporal folder:\n",
    "os.rmdir(TEMP_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6b0f89a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-5.m76",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-5:m76"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
